test optimize
set opt_level=speed
target x86_64

function %unsigned_shift_right_shift_left_i8(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 5
    v2 = ushr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v4 = iconst.i8 -32
    ; check: v5 = band v0, v4
    ; check: return v5
}

function %unsigned_shift_right_shift_left_i32(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i32 5
    v2 = ushr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v4 = iconst.i32 -32
    ; check: v5 = band v0, v4
    ; check: return v5
}

function %unsigned_shift_right_shift_left_i64(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 5
    v2 = ushr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v4 = iconst.i64 -32
    ; check: v5 = band v0, v4
    ; check: return v5
}

function %signed_shift_right_shift_left_i8(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 5
    v2 = sshr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v4 = iconst.i8 -32
    ; check: v5 = band v0, v4
    ; check: return v5
}

function %signed_shift_right_shift_left_i32(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i32 5
    v2 = sshr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v4 = iconst.i32 -32
    ; check: v5 = band v0, v4
    ; check: return v5
}

function %signed_shift_right_shift_left_i64(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 5
    v2 = sshr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v4 = iconst.i64 -32
    ; check: v5 = band v0, v4
    ; check: return v5
}

function %signed_shift_right_shift_left_i8_mask_rhs(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 0xf5
    v2 = sshr v0, v1
    v3 = ishl v2, v1
    return v3
    ; check: v7 = iconst.i8 -32
    ; check: v8 = band v0, v7
    ; check: return v8
}

function %sextend_shift_32_64_unsigned(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 32
    v2 = sextend.i64 v0
    v3 = ishl v2, v1
    v4 = ushr v3, v1
    return v4
    ; check: v7 = uextend.i64 v0
    ; check: return v7
}

function %sextend_shift_32_64_signed(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 32
    v2 = sextend.i64 v0
    v3 = ishl v2, v1
    v4 = sshr v3, v1
    return v4
    ; check: return v2
}

function %sextend_undershift_32_64_unsigned(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 31
    v2 = sextend.i64 v0
    v3 = ishl v2, v1
    v4 = ushr v3, v1
    return v4
    ; check: v5 = iconst.i64 0x0001_ffff_ffff
    ; check: v6 = band v2, v5
    ; check: return v6
}

function %sextend_undershift_32_64_signed(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 31
    v2 = sextend.i64 v0
    v3 = ishl v2, v1
    v4 = sshr v3, v1
    return v4
    ; check: return v2
}

function %sextend_shift_8_64_unsigned(i8) -> i64 {
block0(v0: i8):
    v1 = iconst.i8 56
    v2 = sextend.i64 v0
    v3 = ishl v2, v1
    v4 = ushr v3, v1
    return v4
    ; check: v7 = uextend.i64 v0
    ; check: return v7
}

function %sextend_shift_8_64_signed(i8) -> i64 {
block0(v0: i8):
    v1 = iconst.i8 56
    v2 = sextend.i64 v0
    v3 = ishl v2, v1
    v4 = sshr v3, v1
    return v4
    ; check: return v2
}

function %uextend_shift_32_64_unsigned(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 32
    v2 = uextend.i64 v0
    v3 = ishl v2, v1
    v4 = ushr v3, v1
    return v4
    ; check: return v2
}

function %uextend_shift_32_64_signed(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 32
    v2 = uextend.i64 v0
    v3 = ishl v2, v1
    v4 = sshr v3, v1
    return v4
    ; check: v5 = sextend.i64 v0
    ; check: return v5
}

function %uextend_undershift_32_64_unsigned(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 31
    v2 = uextend.i64 v0
    v3 = ishl v2, v1
    v4 = ushr v3, v1
    return v4
    ; check: return v2
}

function %uextend_undershift_32_64_signed(i32) -> i64 {
block0(v0: i32):
    v1 = iconst.i8 31
    v2 = uextend.i64 v0
    v3 = ishl v2, v1
    v4 = sshr v3, v1
    return v4
    ; check: return v2
}

function %uextend_shift_8_64_unsigned(i8) -> i64 {
block0(v0: i8):
    v1 = iconst.i8 56
    v2 = uextend.i64 v0
    v3 = ishl v2, v1
    v4 = ushr v3, v1
    return v4
    ; check: return v2
}

function %uextend_shift_8_64_signed(i8) -> i64 {
block0(v0: i8):
    v1 = iconst.i8 56
    v2 = uextend.i64 v0
    v3 = ishl v2, v1
    v4 = sshr v3, v1
    return v4
    ; check: v5 = sextend.i64 v0
    ; check: return v5
}


function %ineg_ushr_to_sshr(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 63
    v2 = ushr v0, v1
    v3 = ineg v2
    return v3
    ; check: v4 = sshr v0, v1
    ; check: return v4
}

function %i32_shl_sshr_8_to_ireduce(i32) -> i32 {
block0(v0: i32):
    v1 = ishl_imm v0, 24
    v2 = sshr_imm v1, 24
    return v2
    ; check: v5 = ireduce.i8 v0
    ; check: v6 = sextend.i32 v5
    ; check: return v6
}

function %i32_shl_sshr_16_to_ireduce(i32) -> i32 {
block0(v0: i32):
    v1 = ishl_imm v0, 16
    v2 = sshr_imm v1, 16
    return v2
    ; check: v5 = ireduce.i16 v0
    ; check: v6 = sextend.i32 v5
    ; check: return v6
}

function %i64_shl_sshr_8_to_ireduce(i64) -> i64 {
block0(v0: i64):
    v1 = ishl_imm v0, 56
    v2 = sshr_imm v1, 56
    return v2
    ; check: v5 = ireduce.i8 v0
    ; check: v6 = sextend.i64 v5
    ; check: return v6
}

function %i64_shl_sshr_16_to_ireduce(i64) -> i64 {
block0(v0: i64):
    v1 = ishl_imm v0, 48
    v2 = sshr_imm v1, 48
    return v2
    ; check: v5 = ireduce.i16 v0
    ; check: v6 = sextend.i64 v5
    ; check: return v6
}

function %i64_shl_sshr_32_to_ireduce(i64) -> i64 {
block0(v0: i64):
    v1 = ishl_imm v0, 32
    v2 = sshr_imm v1, 32
    return v2
    ; check: v5 = ireduce.i32 v0
    ; check: v6 = sextend.i64 v5
    ; check: return v6
}

function %i32_shl_ushr_8_to_ireduce(i32) -> i32 {
block0(v0: i32):
    v1 = ishl_imm v0, 24
    v2 = ushr_imm v1, 24
    return v2
    ; check: v7 = ireduce.i8 v0
    ; check: v8 = uextend.i32 v7
    ; check: return v8
}

function %i32_shl_ushr_16_to_ireduce(i32) -> i32 {
block0(v0: i32):
    v1 = ishl_imm v0, 16
    v2 = ushr_imm v1, 16
    return v2
    ; check: v7 = ireduce.i16 v0
    ; check: v8 = uextend.i32 v7
    ; check: return v8
}

function %i64_shl_ushr_8_to_ireduce(i64) -> i64 {
block0(v0: i64):
    v1 = ishl_imm v0, 56
    v2 = ushr_imm v1, 56
    return v2
    ; check: v7 = ireduce.i8 v0
    ; check: v8 = uextend.i64 v7
    ; check: return v8
}

function %i64_shl_ushr_16_to_ireduce(i64) -> i64 {
block0(v0: i64):
    v1 = ishl_imm v0, 48
    v2 = ushr_imm v1, 48
    return v2
    ; check: v7 = ireduce.i16 v0
    ; check: v8 = uextend.i64 v7
    ; check: return v8
}

function %i64_shl_ushr_32_to_ireduce(i64) -> i64 {
block0(v0: i64):
    v1 = ishl_imm v0, 32
    v2 = ushr_imm v1, 32
    return v2
    ; check: v7 = ireduce.i32 v0
    ; check: v8 = uextend.i64 v7
    ; check: return v8
}

function %ishl_amt_type_ireduce(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = ireduce.i8 v1
    v3 = ishl.i8 v0, v2
    return v3
}

; check: v4 = ishl v0, v1
; check: return v4

function %ishl_amt_type_sextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sextend.i32 v1
    v3 = ishl.i8 v0, v2
    return v3
}

; check: v4 = ishl v0, v1
; check: return v4

function %ishl_amt_type_uextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = uextend.i32 v1
    v3 = ishl.i8 v0, v2
    return v3
}

; check: v4 = ishl v0, v1
; check: return v4


function %ushr_amt_type_ireduce(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = ireduce.i8 v1
    v3 = ushr.i8 v0, v2
    return v3
}

; check: v4 = ushr v0, v1
; check: return v4

function %ushr_amt_type_sextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sextend.i32 v1
    v3 = ushr.i8 v0, v2
    return v3
}

; check: v4 = ushr v0, v1
; check: return v4

function %ushr_amt_type_uextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = uextend.i32 v1
    v3 = ushr.i8 v0, v2
    return v3
}

; check: v4 = ushr v0, v1
; check: return v4


function %sshr_amt_type_ireduce(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = ireduce.i8 v1
    v3 = sshr.i8 v0, v2
    return v3
}

; check: v4 = sshr v0, v1
; check: return v4

function %sshr_amt_type_sextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sextend.i32 v1
    v3 = sshr.i8 v0, v2
    return v3
}

; check: v4 = sshr v0, v1
; check: return v4

function %sshr_amt_type_uextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = uextend.i32 v1
    v3 = sshr.i8 v0, v2
    return v3
}

; check: v4 = sshr v0, v1
; check: return v4


function %rotr_amt_type_ireduce(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = ireduce.i8 v1
    v3 = rotr.i8 v0, v2
    return v3
}

; check: v4 = rotr v0, v1
; check: return v4

function %rotr_amt_type_sextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sextend.i32 v1
    v3 = rotr.i8 v0, v2
    return v3
}

; check: v4 = rotr v0, v1
; check: return v4

function %rotr_amt_type_uextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = uextend.i32 v1
    v3 = rotr.i8 v0, v2
    return v3
}

; check: v4 = rotr v0, v1
; check: return v4


function %rotl_amt_type_ireduce(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = ireduce.i8 v1
    v3 = rotl.i8 v0, v2
    return v3
}

; check: v4 = rotl v0, v1
; check: return v4

function %rotl_amt_type_sextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sextend.i32 v1
    v3 = rotl.i8 v0, v2
    return v3
}

; check: v4 = rotl v0, v1
; check: return v4

function %rotl_amt_type_uextend(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = uextend.i32 v1
    v3 = rotl.i8 v0, v2
    return v3
}

; check: v4 = rotl v0, v1
; check: return v4


function %ishl_amt_type_iconcat(i8, i16, i16) -> i8 {
block0(v0: i8, v1: i16, v2: i16):
    v3 = iconcat.i16 v1, v2
    v4 = ishl.i8 v0, v3
    return v4
}

; check: v5 = ishl v0, v1
; check: return v5


function %ushr_amt_type_iconcat(i8, i16, i16) -> i8 {
block0(v0: i8, v1: i16, v2: i16):
    v3 = iconcat.i16 v1, v2
    v4 = ushr.i8 v0, v3
    return v4
}

; check: v5 = ushr v0, v1
; check: return v5


function %sshr_amt_type_iconcat(i8, i16, i16) -> i8 {
block0(v0: i8, v1: i16, v2: i16):
    v3 = iconcat.i16 v1, v2
    v4 = sshr.i8 v0, v3
    return v4
}

; check: v5 = sshr v0, v1
; check: return v5


function %rotr_amt_type_iconcat(i8, i16, i16) -> i8 {
block0(v0: i8, v1: i16, v2: i16):
    v3 = iconcat.i16 v1, v2
    v4 = rotr.i8 v0, v3
    return v4
}

; check: v5 = rotr v0, v1
; check: return v5


function %rotl_amt_type_iconcat(i8, i16, i16) -> i8 {
block0(v0: i8, v1: i16, v2: i16):
    v3 = iconcat.i16 v1, v2
    v4 = rotl.i8 v0, v3
    return v4
}

; check: v5 = rotl v0, v1
; check: return v5

function %ishl_prop_overflow(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 7
    v2 = iconst.i8 7
    v3 = ishl.i8 v0, v1
    v4 = ishl.i8 v3, v2
    return v4
}

; check: v5 = iconst.i8 0
; check: return v5  ; v5 = 0

function %ishl_prop_type_diff(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i16 7
    v2 = iconst.i16 7
    v3 = ishl.i8 v0, v1
    v4 = ishl.i8 v3, v2
    return v4
}

; check: v5 = iconst.i8 0
; check: return v5  ; v5 = 0

function %ushr_prop(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 1
    v2 = iconst.i8 1
    v3 = ushr.i8 v0, v1
    v4 = ushr.i8 v3, v2
    return v4
}

; check: v5 = iconst.i8 2
; check: v6 = ushr v0, v5  ; v5 = 2
; check: return v6

function %ushr_prop_overflow(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 7
    v2 = iconst.i8 7
    v3 = ushr.i8 v0, v1
    v4 = ushr.i8 v3, v2
    return v4
}

; check: v5 = iconst.i8 0
; check: return v5  ; v5 = 0


function %sshr_prop(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 1
    v2 = iconst.i8 1
    v3 = sshr.i8 v0, v1
    v4 = sshr.i8 v3, v2
    return v4
}

; check: v5 = iconst.i8 2
; check: v6 = sshr v0, v5  ; v5 = 2
; check: return v6

function %i128_ushr_overflow_becomes_0(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i16 200
    v2 = ushr v0, v1
    v3 = ushr v2, v1
    return v3
}

; check: v7 = iconst.i64 0
; nextln: v8 = uextend.i128 v7
; nextln: return v8


function %i128_ishl_overflow_becomes_0(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i16 200
    v2 = ishl v0, v1
    v3 = ishl v2, v1
    return v3
}

; check: v7 = iconst.i64 0
; nextln: v8 = uextend.i128 v7
; nextln: return v8

function %simd_shift_does_not_panic(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iconst.i64 0
    v2 = ushr v0, v1
    v3 = ushr v2, v1
    return v3
}

; check: v1 = iconst.i64 0
; nextln: v2 = ushr v0, v1  ; v1 = 0
; check: return v2

function %merges_shift_amount_based_on_lane_type(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iconst.i16 30
    v2 = iconst.i16 3

    v3 = sshr v0, v1
    v4 = sshr v3, v2
    v5 = sshr v4, v2
    return v5
}

; check: v6 = iconst.i16 6
; check: v7 = sshr v0, v6
; check: v9 = sshr v7, v6
; check: return v9


function %rotr_rotr_prop(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 1
    v2 = iconst.i8 1
    v3 = rotr.i8 v0, v1
    v4 = rotr.i8 v3, v2
    return v4
}

; check: v6 = iconst.i8 2
; check: v12 = rotr v0, v6  ; v6 = 2
; check: return v12

function %rotr_rotr_add(i8, i8, i8) -> i8 {
block0(v0: i8, v1: i8, v2: i8):
    v3 = rotr.i8 v0, v1
    v4 = rotr.i8 v3, v2
    return v4
}

; check: v5 = iadd v1, v2
; check: v6 = rotr v0, v5
; check: return v6

function %rotl_rotl_prop(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 1
    v2 = iconst.i8 1
    v3 = rotl.i8 v0, v1
    v4 = rotl.i8 v3, v2
    return v4
}

; check: v6 = iconst.i8 2
; check: v12 = rotl v0, v6  ; v6 = 2
; check: return v12

function %rotl_rotl_add(i8, i8, i8) -> i8 {
block0(v0: i8, v1: i8, v2: i8):
    v3 = rotl.i8 v0, v1
    v4 = rotl.i8 v3, v2
    return v4
}

; check: v5 = iadd v1, v2
; check: v6 = rotl v0, v5
; check: return v6


function %rotl_rotr_prop(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 2
    v2 = iconst.i8 1
    v3 = rotr.i8 v0, v1
    v4 = rotl.i8 v3, v2
    return v4
}

; check: v2 = iconst.i8 1
; check: v15 = rotr v0, v2  ; v2 = 1
; check: return v15

function %rotl_rotr_add(i8, i8, i8) -> i8 {
block0(v0: i8, v1: i8, v2: i8):
    v3 = rotr.i8 v0, v1
    v4 = rotl.i8 v3, v2
    return v4
}

; check: v5 = isub v1, v2
; check: v6 = rotr v0, v5
; check: return v6

function %rotl_rotr_prop(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 2
    v2 = iconst.i8 1
    v3 = rotl.i8 v0, v1
    v4 = rotr.i8 v3, v2
    return v4
}

; check: v2 = iconst.i8 1
; check: v15 = rotl v0, v2  ; v2 = 1
; check: return v15

function %rotl_rotr_add(i8, i8, i8) -> i8 {
block0(v0: i8, v1: i8, v2: i8):
    v3 = rotl.i8 v0, v1
    v4 = rotr.i8 v3, v2
    return v4
}

; check: v5 = isub v1, v2
; check: v6 = rotl v0, v5
; check: return v6

function %rotl_rotr_subsume(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
    v2 = rotl.i8 v0, v1
    v3 = rotr.i8 v2, v1
    return v3
}

; check: return v0


function %rotr_rotl_subsume(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
    v2 = rotr.i8 v0, v1
    v3 = rotl.i8 v2, v1
    return v3
}

; check: return v0

function %shifts_to_rotl(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i16 3
    v2 = iconst.i32 61
    v3 = ishl v0, v1
    v4 = ushr v0, v2
    v5 = bor v3, v4
    return v5
}

; check: v1 = iconst.i16 3
; nextln: v6 = rotl v0, v1  ; v1 = 3
; check: return v6


function %shifts_to_rotl_reverse(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i16 32
    v2 = iconst.i32 32
    v3 = ishl v0, v1
    v4 = ushr v0, v2
    v5 = bor v4, v3
    return v5
}

; check: v1 = iconst.i16 32
; nextln: v6 = rotl v0, v1  ; v1 = 32
; check: return v6

function %shifts_to_rotr_equivalent(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i16 3
    v2 = iconst.i32 61
    v3 = ishl v0, v2
    v4 = ushr v0, v1
    v5 = bor v3, v4
    return v5
}

; check: v2 = iconst.i32 61
; nextln: v6 = rotl v0, v2  ; v2 = 61
; check: return v6

function %ishl_normalize_amount(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i32 99
    v2 = ishl v0, v1
    return v2
}

; check: v3 = iconst.i32 35
; check: v4 = ishl v0, v3
; check: return v4
