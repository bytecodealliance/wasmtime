test optimize
set opt_level=speed
target x86_64
target aarch64
target s390x
target riscv64

function %select_self(i8, i32) -> i32 {
block0(v0: i8, v1: i32):
    v2 = select v0, v1, v1
    return v2
    ; check: return v1
}

function %bitselect_self(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bitselect v0, v1, v1
    return v2
    ; check: return v1
}

function %select_sgt_to_smax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sgt v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smax v0, v1
; check:    return v4


; This tests an inverted select, where the operands are swapped.
function %select_sgt_to_smax_inverse(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sgt v0, v1
    v3 = select v2, v1, v0
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smin v0, v1
; check:    return v4


function %select_sge_to_smax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sge v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smax v0, v1
; check:    return v4


function %select_ugt_to_umax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ugt v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umax v0, v1
; check:    return v4


function %select_uge_to_umax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp uge v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umax v0, v1
; check:    return v4



function %select_slt_to_smin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smin v0, v1
; check:    return v4


function %select_sle_to_smin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sle v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smin v0, v1
; check:    return v4


function %select_ult_to_umin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ult v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umin v0, v1
; check:    return v4


function %select_ule_to_umin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ule v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umin v0, v1
; check:    return v4



function %select_with_different_regs_does_not_optimize(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = icmp ule v0, v1
    v5 = select v4, v2, v3
    return v5
}

; check: block0(v0: i32, v1: i32, v2: i32, v3: i32):
; check:    v4 = icmp ule v0, v1
; check:    v5 = select v4, v2, v3
; check:    return v5

function %then_true_else_false(i32, i32) -> i8 {
block0(v0: i32, v1: i32):
    v2 = icmp ule v0, v1
    v3 = iconst.i8 1
    v4 = iconst.i8 0
    v5 = select v2, v3, v4
    return v5
}
; check: return v2

function %then_false_else_true(i32, i32) -> i8 {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = iconst.i8 0
    v4 = iconst.i8 1
    v5 = select v2, v3, v4
    return v5
}
; check: v6 = icmp sge v0, v1
; check: return v6

function %then_one_else_zero(i32, i32) -> i64 {
block0(v0: i32, v1: i32):
    v2 = icmp ule v0, v1
    v3 = iconst.i64 1
    v4 = iconst.i64 0
    v5 = select v2, v3, v4
    return v5
}
; check: v6 = uextend.i64 v2
; check: return v6

function %then_zero_else_false(i32, i32) -> i64 {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = iconst.i64 0
    v4 = iconst.i64 1
    v5 = select v2, v3, v4
    return v5
}
; check: v6 = icmp sge v0, v1
; check: v8 = uextend.i64 v6
; check: return v8

function %then_negone_else_zero(i32, i32) -> i64 {
block0(v0: i32, v1: i32):
    v2 = icmp ule v0, v1
    v3 = iconst.i64 -1
    v4 = iconst.i64 0
    v5 = select v2, v3, v4
    return v5
}
; check: v6 = bmask.i64 v2
; check: return v6

function %then_zero_else_else_negone(i32, i32) -> i64 {
block0(v0: i32, v1: i32):
    v2 = icmp sle v0, v1
    v3 = iconst.i64 0
    v4 = iconst.i64 -1
    v5 = select v2, v3, v4
    return v5
}
; check: v6 = icmp sgt v0, v1
; check: v8 = bmask.i64 v6
; check: return v8

;; select(cond, unop(x), unop(y)) => unop(select(cond, x, y))
function %transpose_unop_gvn_1(i8, i32, i32) -> i32 {
block0(v0: i8, v1: i32, v2: i32):
    v3 = ineg v1
    v4 = ineg v2
    v5 = select v0, v3, v4
    return v5
    ; check: v6 = select v0, v1, v2
    ; check: v7 = ineg v6
    ; check: return v7
}

;; select(cond, binop(x, z), binop(y, z)) => binop(select(cond, x, y), z)
function %transpose_binop_gvn_1(i8, i32, i32, i32) -> i32 {
block0(v0: i8, v1: i32, v2: i32, v3: i32):
    v4 = iadd v1, v3
    v5 = iadd v2, v3
    v6 = select v0, v4, v5
    return v6
    ; check: v7 = select v0, v1, v2
    ; check: v8 = iadd v7, v3
    ; check: return v8
}

;; select(cond, binop(x, y), binop(x, z)) => binop(x, select(cond, y, z))
function %transpose_binop_gvn_2(i8, i32, i32, i32) -> i32 {
block0(v0: i8, v1: i32, v2: i32, v3: i32):
    v4 = iadd v1, v2
    v5 = iadd v1, v3
    v6 = select v0, v4, v5
    return v6
    ; check: v7 = select v0, v2, v3
    ; check: v8 = iadd v1, v7
    ; check: return v8
}

;; select(cond, ternop(w, y, z), ternop(x, y, z)) => ternop(select(cond, w, x), y, z)
function %transpose_ternop_gvn_1(i8, f32, f32, f32, f32) -> f32 {
block0(v0: i8, v1: f32, v2: f32, v3: f32, v4: f32):
    v5 = fma v1, v3, v4
    v6 = fma v2, v3, v4
    v7 = select v0, v5, v6
    return v6
    ; check: v6 = fma v2, v3, v4
    ; check: return v6
}

;; select(cond, ternop(w, x, z), ternop(w, y, z)) => ternop(w, select(cond, x, y), z)
function %transpose_ternop_gvn_2(i8, f32, f32, f32, f32) -> f32 {
block0(v0: i8, v1: f32, v2: f32, v3: f32, v4: f32):
    v5 = fma v1, v2, v4
    v6 = fma v1, v3, v4
    v7 = select v0, v5, v6
    return v6
    ; check: v6 = fma v1, v3, v4
    ; check: return v6
}

;; select(cond, ternop(w, x, y), ternop(w, x, z)) => ternop(w, x, select(cond, y, z))
function %transpose_ternop_gvn_3(i8, f32, f32, f32, f32) -> f32 {
block0(v0: i8, v1: f32, v2: f32, v3: f32, v4: f32):
    v5 = fma v1, v2, v3
    v6 = fma v1, v2, v4
    v7 = select v0, v5, v6
    return v6
    ; check: v6 = fma v1, v2, v4
    ; check: return v6
}

;; select(cond, icmp(cc, x, z), icmp(cc, y, z)) => icmp(cc, select(cond, x, y), z)
function %transpose_icmp_gvn_1(i8, i32, i32, i32) -> i8 {
block0(v0: i8, v1: i32, v2: i32, v3: i32):
    v5 = icmp eq v1, v3
    v6 = icmp eq v2, v3
    v7 = select v0, v5, v6
    return v7
    ; check: v8 = select v0, v1, v2
    ; check: v9 = icmp eq v8, v3
    ; check: return v9
}

;; select(cond, icmp(cc, x, y), icmp(cc, x, z)) => icmp(cc, x, select(cond, y, z))
function %transpose_icmp_gvn_2(i8, i32, i32, i32) -> i8 {
block0(v0: i8, v1: i32, v2: i32, v3: i32):
    v5 = icmp eq v1, v2
    v6 = icmp eq v1, v3
    v7 = select v0, v5, v6
    return v7
    ; check: v8 = select v0, v2, v3
    ; check: v9 = icmp eq v1, v8
    ; check: return v9
}

;; select(cond, fcmp(cc, x, z), fcmp(cc, y, z)) => fcmp(cc, select(cond, x, y), z)
function %transpose_fcmp_gvn_1(i8, f32, f32, f32) -> i8 {
block0(v0: i8, v1: f32, v2: f32, v3: f32):
    v5 = fcmp eq v1, v3
    v6 = fcmp eq v2, v3
    v7 = select v0, v5, v6
    return v7
    ; check: v8 = select v0, v1, v2
    ; check: v9 = fcmp eq v8, v3
    ; check: return v9
}

;; select(cond, fcmp(cc, x, y), fcmp(cc, x, z)) => fcmp(cc, x, select(cond, y, z))
function %transpose_fcmp_gvn_2(i8, f32, f32, f32) -> i8 {
block0(v0: i8, v1: f32, v2: f32, v3: f32):
    v5 = fcmp eq v1, v2
    v6 = fcmp eq v1, v3
    v7 = select v0, v5, v6
    return v7
    ; check: v8 = select v0, v2, v3
    ; check: v9 = fcmp eq v1, v8
    ; check: return v9
}
