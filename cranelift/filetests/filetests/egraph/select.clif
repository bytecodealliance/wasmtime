test optimize
set opt_level=speed
target x86_64
target aarch64
target s390x
target riscv64

function %select_self(i8, i32) -> i32 {
block0(v0: i8, v1: i32):
    v2 = select v0, v1, v1
    return v2
    ; check: return v1
}

function %bitselect_self(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bitselect v0, v1, v1
    return v2
    ; check: return v1
}

function %select_sgt_to_smax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sgt v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smax v0, v1
; check:    return v4


; This tests an inverted select, where the operands are swapped.
function %select_sgt_to_smax_inverse(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sgt v0, v1
    v3 = select v2, v1, v0
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smin v0, v1
; check:    return v4


function %select_sge_to_smax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sge v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smax v0, v1
; check:    return v4


function %select_ugt_to_umax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ugt v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umax v0, v1
; check:    return v4


function %select_uge_to_umax(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp uge v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umax v0, v1
; check:    return v4



function %select_slt_to_smin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smin v0, v1
; check:    return v4


function %select_sle_to_smin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp sle v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = smin v0, v1
; check:    return v4


function %select_ult_to_umin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ult v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umin v0, v1
; check:    return v4


function %select_ule_to_umin(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ule v0, v1
    v3 = select v2, v0, v1
    return v3
}

; check: block0(v0: i32, v1: i32):
; check:    v4 = umin v0, v1
; check:    return v4



function %select_with_different_regs_does_not_optimize(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = icmp ule v0, v1
    v5 = select v4, v2, v3
    return v5
}

; check: block0(v0: i32, v1: i32, v2: i32, v3: i32):
; check:    v4 = icmp ule v0, v1
; check:    v5 = select v4, v2, v3
; check:    return v5

function %select_uextend_fcmp(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
    v2 = fcmp uno v0, v1
    v3 = uextend.i64 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: f32, v1: f32):
; check:     v5 = fcmp uno v0, v1
; nextln:    v6 = select v5, v0, v1
; check:     return v6


function %select_sextend_fcmp(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
    v2 = fcmp uno v0, v1
    v3 = sextend.i64 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: f32, v1: f32):
; check:     v5 = fcmp uno v0, v1
; nextln:    v6 = select v5, v0, v1
; check:     return v6



function %select_uextend_icmp(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ne v0, v1
    v3 = uextend.i64 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: i32, v1: i32):
; check:     v6 = icmp ne v0, v1
; nextln:    v7 = select v6, v0, v1
; check:     return v7


function %select_sextend_icmp(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = icmp ne v0, v1
    v3 = sextend.i64 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: i32, v1: i32):
; check:     v5 = icmp ne v0, v1
; nextln:    v6 = select v5, v0, v1
; check:     return v6




function %select_uextend_bmask(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bmask.i32 v0
    v3 = uextend.i64 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: i32, v1: i32):
; check:     v2 = bmask.i32 v0
; nextln:    v5 = select v2, v0, v1
; check:     return v5

function %select_sextend_bmask(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bmask.i32 v0
    v3 = sextend.i64 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: i32, v1: i32):
; check:     v2 = bmask.i32 v0
; nextln:    v5 = select v2, v0, v1
; check:     return v5


function %select_ireduce_bmask(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bmask.i32 v0
    v3 = ireduce.i16 v2
    v4 = select v3, v0, v1
    return v4
}

; check: block0(v0: i32, v1: i32):
; check:     v2 = bmask.i32 v0
; nextln:    v5 = select v2, v0, v1
; check:     return v5



function %fcmp_select_neg_1_i8(f64, f64) -> i8 {
block0(v0: f64, v1: f64):
    v2 = fcmp.f64 lt v0, v1
    v3 = iconst.i8 0
    v4 = iconst.i8 -1
    v5 = select v2, v4, v3
    return v5
}
; check: v6 = fcmp lt v0, v1
; check: return v6


function %fcmp_select_neg_1_i64(f64, f64) -> i64 {
block0(v0: f64, v1: f64):
    v2 = fcmp.f64 lt v0, v1
    v3 = iconst.i64 0
    v4 = iconst.i64 -1
    v5 = select v2, v4, v3
    return v5
}
; check: v6 = fcmp lt v0, v1
; nextln: v7 = sextend.i64 v6
; check: return v7


function %icmp_select_neg_1_i8(i64, i64) -> i8 {
block0(v0: i64, v1: i64):
    v2 = icmp.i64 slt v0, v1
    v3 = iconst.i8 0
    v4 = iconst.i8 -1
    v5 = select v2, v4, v3
    return v5
}
; check: v6 = icmp slt v0, v1
; check: return v6


function %icmp_select_neg_1_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = icmp.i64 slt v0, v1
    v3 = iconst.i64 0
    v4 = iconst.i64 -1
    v5 = select v2, v4, v3
    return v5
}
; check: v6 = icmp slt v0, v1
; nextln: v7 = sextend.i64 v6
; check: return v7
