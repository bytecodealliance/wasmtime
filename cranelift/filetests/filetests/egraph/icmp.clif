test optimize precise-output
set opt_level=speed
target x86_64

;; Masking the result of a comparison with 1 always results in the comparison
;; itself (comparisons in wasm may sometimes be hidden behind extensions):
function %mask_icmp_result(i64, i64) -> i8 {
block0(v1: i64, v2: i64):
    v3 = icmp ult v1, v2
    v4 = iconst.i8 1
    v5 = band v3, v4
    return v5
}

; function %mask_icmp_result(i64, i64) -> i8 fast {
; block0(v1: i64, v2: i64):
;     v3 = icmp ult v1, v2
;     return v3
; }

function %mask_icmp_extend_result(i64, i64) -> i64 {
block0(v1: i64, v2: i64):
    v3 = icmp ult v1, v2
    v4 = uextend.i64 v3
    v5 = iconst.i64 1
    v6 = band v4, v5
    return v6
}

; function %mask_icmp_extend_result(i64, i64) -> i64 fast {
; block0(v1: i64, v2: i64):
;     v3 = icmp ult v1, v2
;     v4 = uextend.i64 v3
;     return v4
; }

function %icmp_simplify_does_not_build_iconst_i128() -> i8 {
block0:
    v0 = iconst.i64 0x6350_5050_5050_3750
    v1 = iconcat v0, v0
    v2 = bmask.i8 v1
    v3 = iconst.i8 0
    v4 = icmp ugt v2, v3
    return v4
}

; function %icmp_simplify_does_not_build_iconst_i128() -> i8 fast {
; block0:
;     v0 = iconst.i64 0x6350_5050_5050_3750
;     v1 = iconcat v0, v0  ; v0 = 0x6350_5050_5050_3750, v0 = 0x6350_5050_5050_3750
;     v2 = bmask.i8 v1
;     v3 = iconst.i8 0
;     v5 = icmp ne v2, v3  ; v3 = 0
;     return v5
; }

function %sge_one(i64) -> i8 {
block0(v0: i64):
    v1 = iconst.i64 1
    v2 = icmp sge v0, v1
    return v2
}

; function %sge_one(i64) -> i8 fast {
; block0(v0: i64):
;     v3 = iconst.i64 0
;     v4 = icmp sgt v0, v3  ; v3 = 0
;     return v4
; }

function %slt_one(i16) -> i8 {
block0(v0: i16):
    v1 = iconst.i16 1
    v2 = icmp slt v0, v1
    return v2
}

; function %slt_one(i16) -> i8 fast {
; block0(v0: i16):
;     v3 = iconst.i16 0
;     v4 = icmp sle v0, v3  ; v3 = 0
;     return v4
; }

function %uge_one(i32) -> i8 {
block0(v0: i32):
    v1 = iconst.i32 1
    v2 = icmp uge v0, v1
    return v2
}

; function %uge_one(i32) -> i8 fast {
; block0(v0: i32):
;     v3 = iconst.i32 0
;     v4 = icmp ne v0, v3  ; v3 = 0
;     return v4
; }

function %ult_one(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 1
    v2 = icmp ult v0, v1
    return v2
}

; function %ult_one(i8) -> i8 fast {
; block0(v0: i8):
;     v3 = iconst.i8 0
;     v4 = icmp eq v0, v3  ; v3 = 0
;     return v4
; }

function %byte_icmp_ucmp(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 0
    v2 = icmp sge v0, v1
    v3 = icmp eq v2, v1
    return v3
}

; function %byte_icmp_ucmp(i8) -> i8 fast {
; block0(v0: i8):
;     v1 = iconst.i8 0
;     v4 = icmp slt v0, v1  ; v1 = 0
;     return v4
; }

function %eq_one_of_icmp(i8) -> i8 fast {
block0(v0: i8):
    v1 = iconst.i8 5
    v2 = icmp eq v0, v1
    v3 = iconst.i8 1
    v4 = icmp eq v2, v3
    return v4
}

; function %eq_one_of_icmp(i8) -> i8 fast {
; block0(v0: i8):
;     v1 = iconst.i8 5
;     v2 = icmp eq v0, v1  ; v1 = 5
;     return v2
; }

function %ne_one_of_icmp(i8) -> i8 fast {
block0(v0: i8):
    v1 = iconst.i8 5
    v2 = icmp eq v0, v1
    v3 = iconst.i8 1
    v4 = icmp ne v2, v3
    return v4
}

; function %ne_one_of_icmp(i8) -> i8 fast {
; block0(v0: i8):
;     v1 = iconst.i8 5
;     v5 = icmp ne v0, v1  ; v1 = 5
;     return v5
; }

function %negate_lt(i8, i8) -> i8 fast {
block0(v0: i8, v1: i8):
    v2 = icmp ult v0, v1
    v3 = iconst.i8 1
    v4 = icmp ne v2, v3
    return v4
}

; function %negate_lt(i8, i8) -> i8 fast {
; block0(v0: i8, v1: i8):
;     v5 = icmp uge v0, v1
;     return v5
; }

;; `x == (x ^ y)` to `y == 0`.
function %simplify_icmp_eq_x_bxor_x_y(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = bxor v0, v1
    v3 = icmp eq v0, v2
    return v3
}

; function %simplify_icmp_eq_x_bxor_x_y(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v4 = iconst.i32 0
;     v5 = icmp eq v1, v4  ; v4 = 0
;     return v5
; }


;; `x != (x ^ y)` to `y != 0`.
function %simplify_icmp_ne_x_bxor_x_y(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = bxor v0, v1
    v3 = icmp ne v0, v2
    return v3
}

; function %simplify_icmp_ne_x_bxor_x_y(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v4 = iconst.i32 0
;     v5 = icmp ne v1, v4  ; v4 = 0
;     return v5
; }

function %issue_10929_no_crash_on_icmp_vectors() -> i32x4 {
  const0 = 0x40ad3fb47cb16076c8cb1fdd8189d40f

block0():
  v1 = vconst.i32x4 const0
  v2 = bxor_not v1, v1
  v3 = icmp.i32x4 ne v1, v2
  return v3
}

; function %issue_10929_no_crash_on_icmp_vectors() -> i32x4 fast {
;     const0 = 0x40ad3fb47cb16076c8cb1fdd8189d40f
;
; block0:
;     v1 = vconst.i32x4 const0
;     v4 = bnot v1  ; v1 = const0
;     v2 = bxor v1, v4  ; v1 = const0
;     v3 = icmp ne v1, v2  ; v1 = const0
;     return v3
; }

;; (x - y) u> x == y u> x
function %ugt_isub_x_y_x(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = isub v0, v1
    v3 = icmp ugt v2, v0
    return v3
}

; function %ugt_isub_x_y_x(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v4 = icmp ugt v1, v0
;     return v4
; }

;; (x - y) u<= x == y u<= x
function %ule_isub_x_y_x(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = isub v0, v1
    v3 = icmp ule v2, v0
    return v3
}

; function %ule_isub_x_y_x(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v4 = icmp ule v1, v0
;     return v4
; }

;; (ne cty (select ty (slt cty x y) x y) x) -> (sgt cty x y)
function %simplify_select_slt_ne_i32(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = select v2, v0, v1
    v4 = icmp ne v3, v0
    return v4
}

; function %simplify_select_slt_ne_i32(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v7 = icmp sgt v0, v1
;     return v7
; }

;; (ne cty (select ty (ult cty x y) x y) x) -> (ugt cty x y)
function %simplify_select_ult_ne_i64(i64, i64) -> i8 fast {
block0(v0: i64, v1: i64):
    v2 = icmp ult v0, v1
    v3 = select v2, v0, v1
    v4 = icmp ne v3, v0
    return v4
}

; function %simplify_select_ult_ne_i64(i64, i64) -> i8 fast {
; block0(v0: i64, v1: i64):
;     v7 = icmp ugt v0, v1
;     return v7
; }

;; (bxor ty (slt cty x y) (sgt cty x y)) -> (ne cty x y)
function %test_bxor_slt_sgt(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = icmp sgt v0, v1
    v4 = bxor v2, v3
    return v4
}

; function %test_bxor_slt_sgt(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v5 = icmp ne v0, v1
;     return v5
; }

;; (bxor ty (ult cty x y) (ugt cty x y)) -> (ne cty x y)
function %test_bxor_ult_ugt(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = icmp ult v0, v1
    v3 = icmp ugt v0, v1
    v4 = bxor v2, v3
    return v4
}

; function %test_bxor_ult_ugt(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v5 = icmp ne v0, v1
;     return v5
; }

;; Rule 1: (x <_s y) & (x >_s y) => false
function %test_slt_sgt(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = icmp slt v0, v1
    v3 = icmp sgt v0, v1
    v4 = band v2, v3
    return v4
}

; function %test_slt_sgt(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v5 = iconst.i8 0
;     return v5  ; v5 = 0
; }

;; Rule 2: (x <_u y) & (x >_u y) => false
function %test_ult_ugt(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = icmp ult v0, v1
    v3 = icmp ugt v0, v1
    v4 = band v2, v3
    return v4
}

; function %test_ult_ugt(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v5 = iconst.i8 0
;     return v5  ; v5 = 0
; }

;; Rule 3: (x <_s y_const) & (x >_u y_const) => false, if y_const < 0
function %test_slt_ugt_neg_const_y(i32) -> i8 fast {
block0(v0: i32):
    v1 = iconst.i32 -10
    v2 = icmp slt v0, v1
    v3 = icmp ugt v0, v1
    v4 = band v2, v3
    return v4
}

; function %test_slt_ugt_neg_const_y(i32) -> i8 fast {
; block0(v0: i32):
;     v5 = iconst.i8 0
;     return v5  ; v5 = 0
; }

;; Rule 4: (x_const <_s y) & (x_const >_u y) => false, if x_const >= 0
function %test_slt_ugt_pos_const_x(i32) -> i8 fast {
block0(v0: i32):
    v1 = iconst.i32 10
    v2 = icmp slt v1, v0
    v3 = icmp ugt v1, v0
    v4 = band v2, v3
    return v4
}

; function %test_slt_ugt_pos_const_x(i32) -> i8 fast {
; block0(v0: i32):
;     v9 = iconst.i8 0
;     return v9  ; v9 = 0
; }

;; Rule 5: (x <_u y_const) & (x >_s y_const) => false, if y_const >= 0
function %test_ult_sgt_pos_const_y(i32) -> i8 fast {
block0(v0: i32):
    v1 = iconst.i32 10
    v2 = icmp ult v0, v1
    v3 = icmp sgt v0, v1
    v4 = band v2, v3
    return v4
}

; function %test_ult_sgt_pos_const_y(i32) -> i8 fast {
; block0(v0: i32):
;     v5 = iconst.i8 0
;     return v5  ; v5 = 0
; }

;; Rule 6: (x_const <_u y) & (x_const >_s y) => false, if x_const < 0
function %test_ult_sgt_neg_const_x(i32) -> i8 fast {
block0(v0: i32):
    v1 = iconst.i32 -10
    v2 = icmp ult v1, v0
    v3 = icmp sgt v1, v0
    v4 = band v2, v3
    return v4
}

; function %test_ult_sgt_neg_const_x(i32) -> i8 fast {
; block0(v0: i32):
;     v9 = iconst.i8 0
;     return v9  ; v9 = 0
; }

;; (x <u y) & (x != -1) => (x <_u y)
function %test_ult_ne_minus_one(i32, i32) -> i8 fast {
block0(v0: i32, v1: i32):
    v2 = icmp ult v0, v1
    v3 = iconst.i32 -1
    v4 = icmp ne v0, v3
    v5 = band v2, v4
    return v5
}

; function %test_ult_ne_minus_one(i32, i32) -> i8 fast {
; block0(v0: i32, v1: i32):
;     v6 = icmp ult v0, v1
;     return v6
; }

