test optimize precise-output
set opt_level=speed
target x86_64

function %fold_and_xor_not1(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
    v2 = bnot v1
    v3 = bxor v0, v2
    v4 = band v3, v0
    return v4
}

; function %fold_and_xor_not1(i8, i8) -> i8 fast {
; block0(v0: i8, v1: i8):
;     v5 = band v0, v1
;     return v5
; }

function %fold_and_xor_not2(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
    v2 = bnot v1
    v3 = bxor v0, v2
    v4 = band v3, v0
    return v4
}

; function %fold_and_xor_not2(i16, i16) -> i16 fast {
; block0(v0: i16, v1: i16):
;     v5 = band v0, v1
;     return v5
; }

function %fold_and_xor_not3(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bnot v1
    v3 = bxor v0, v2
    v4 = band v3, v0
    return v4
}

; function %fold_and_xor_not3(i32, i32) -> i32 fast {
; block0(v0: i32, v1: i32):
;     v5 = band v0, v1
;     return v5
; }

function %fold_and_xor_not4_1(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bnot v1
    v3 = bxor v0, v2
    v4 = band v3, v0
    return v4
}

; function %fold_and_xor_not4_1(i64, i64) -> i64 fast {
; block0(v0: i64, v1: i64):
;     v5 = band v0, v1
;     return v5
; }

function %fold_and_xor_not4_2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bnot v1
    v3 = bxor v2, v0
    v4 = band v3, v0
    return v4
}

; function %fold_and_xor_not4_2(i64, i64) -> i64 fast {
; block0(v0: i64, v1: i64):
;     v5 = band v0, v1
;     return v5
; }


function %fold_and_xor_not4_3(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bnot v1
    v3 = bxor v0, v2
    v4 = band v0, v3
    return v4
}

; function %fold_and_xor_not4_3(i64, i64) -> i64 fast {
; block0(v0: i64, v1: i64):
;     v5 = band v0, v1
;     return v5
; }

function %fold_and_xor_not4_4(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bnot v1
    v3 = bxor v2, v0
    v4 = band v0, v3
    return v4
}

; function %fold_and_xor_not4_4(i64, i64) -> i64 fast {
; block0(v0: i64, v1: i64):
;     v5 = band v0, v1
;     return v5
; }

