test optimize precise-output
set opt_level=speed
target x86_64

function %fold_add_and_xor1(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
    v2 = band v0, v1
    v3 = bxor v0, v1
    v4 = iadd v2, v3
    return v4
}

; function %fold_add_and_xor1(i8, i8) -> i8 fast {
; block0(v0: i8, v1: i8):
;     v5 = bor v0, v1
;     return v5
; }



function %fold_add_and_xor2(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
    v2 = band v0, v1
    v3 = bxor v0, v1
    v4 = iadd v2, v3
    return v4
}

; function %fold_add_and_xor2(i16, i16) -> i16 fast {
; block0(v0: i16, v1: i16):
;     v5 = bor v0, v1
;     return v5
; }



function %fold_add_and_xor3(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v0, v1
    v3 = bxor v0, v1
    v4 = iadd v2, v3
    return v4
}

; function %fold_add_and_xor3(i32, i32) -> i32 fast {
; block0(v0: i32, v1: i32):
;     v5 = bor v0, v1
;     return v5
; }



function %fold_add_and_xor4(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band v0, v1
    v3 = bxor v0, v1
    v4 = iadd v2, v3
    return v4
}

; function %fold_add_and_xor4(i64, i64) -> i64 fast {
; block0(v0: i64, v1: i64):
;     v5 = bor v0, v1
;     return v5
; }


function %fold_add_and_xor4(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band v0, v1
    v3 = bxor v0, v1
    v4 = iadd v3, v2
    return v4
}

; function %fold_add_and_xor4(i64, i64) -> i64 fast {
; block0(v0: i64, v1: i64):
;     v5 = bor v0, v1
;     return v5
; }

