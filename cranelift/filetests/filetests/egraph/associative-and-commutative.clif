test optimize
set opt_level=speed
target x86_64

function %iadd_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v0, v1
    v5 = iadd v4, v2
    v6 = iadd v5, v3
    return v6
; check:  v4 = iadd v0, v1
; nextln: v7 = iadd v2, v3
; nextln: v8 = iadd v4, v7
; check: return v8
}

function %iadd_shallow_and_wide_twice(i32, i32, i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32, v4: i32, v5: i32):
    v6 = iadd v0, v1
    v7 = iadd v6, v2
    v8 = iadd v7, v3
    v9 = iadd v8, v4
    v10 = iadd v9, v5
    return v10
; check:  v6 = iadd v0, v1
; nextln: v11 = iadd v2, v3
; nextln: v12 = iadd v6, v11
; nextln: v16 = iadd v4, v5
; nextln: v23 = iadd v12, v16
; nextln: return v23
}

function %iadd_reassociate_then_cprop(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = iconst.i32 42
    v3 = iconst.i32 36
    v4 = iadd v0, v2
    v5 = iadd v1, v3
    v6 = iadd v4, v5
    return v6
; check:  v7 = iadd v0, v1
; nextln: v9 = iconst.i32 78
; nextln: v15 = iadd v7, v9  ; v9 = 78
; check:  return v15
}

function %imul_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = imul v0, v1
    v5 = imul v4, v2
    v6 = imul v5, v3
    return v6
; check:  v4 = imul v0, v1
; nextln: v7 = imul v2, v3
; nextln: v8 = imul v4, v7
; check: return v8
}

function %band_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = band v0, v1
    v5 = band v4, v2
    v6 = band v5, v3
    return v6
; check:  v4 = band v0, v1
; nextln: v7 = band v2, v3
; nextln: v8 = band v4, v7
; check: return v8
}

function %bxor_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = bxor v0, v1
    v5 = bxor v4, v2
    v6 = bxor v5, v3
    return v6
; check:  v4 = bxor v0, v1
; nextln: v7 = bxor v2, v3
; nextln: v8 = bxor v4, v7
; check: return v8
}

;; We don't have any assertions about the result of optimizing this function,
;; but it's a good canary for unbounded recursion in optimization rulesets. In
;; particular, because of the shared structure in the dag, it won't be obvious
;; to rules that are pattern matching on trees that this is actually a chain,
;; and they will exhibit exponential behavior as a result.
function %iadd_big_chain(i8) -> i16 {
block0(v0: i8):
    v1 = uextend.i32 v0
    v2 = iconst.i32 42
    v3 = iadd v1, v2
    v4 = iadd v3, v3
    v5 = iadd v4, v4
    v6 = iadd v5, v5
    v7 = iadd v6, v6
    v8 = iadd v7, v7
    v9 = iadd v8, v8
    v10 = iadd v9, v9
    v11 = iadd v10, v10
    v12 = iadd v11, v11
    v13 = iadd v12, v12
    v14 = iadd v13, v13
    v15 = iadd v14, v14
    v16 = iadd v15, v15
    v17 = iadd v16, v16
    v18 = iadd v17, v17
    v19 = iadd v18, v18
    v20 = iadd v19, v19
    v21 = iadd v20, v20
    v22 = iadd v21, v21
    v23 = iadd v22, v22
    v24 = iadd v23, v23
    v25 = iadd v24, v24
    v26 = iadd v25, v25
    v27 = iadd v26, v26
    v28 = ireduce.i16 v27
    return v28
}

function %iadd_sub_1_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v2, v3
    v5 = isub v1, v4
    v6 = isub v0, v5
    return v6
; check:  v7 = isub v0, v1
; nextln: v4 = isub v2, v3
; nextln: v8 = iadd v7, v4
; check: return v8
}

function %iadd_sub_2_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v2, v3
    v5 = isub v1, v4
    v6 = isub v0, v5
    return v6
; check:  v7 = isub v0, v1
; nextln: v4 = iadd v2, v3
; nextln: v8 = iadd v7, v4
; check: return v8
}

function %iadd_sub_3_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v2, v3
    v5 = iadd v1, v4
    v6 = isub v0, v5
    return v6
; check:  v7 = isub v0, v1
; nextln: v4 = isub v2, v3
; nextln: v8 = isub v7, v4
; check: return v8
}

function %iadd_sub_4_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v2, v3
    v5 = iadd v1, v4
    v6 = isub v0, v5
    return v6
; check:  v7 = isub v0, v1
; nextln: v4 = iadd v2, v3
; nextln: v8 = isub v7, v4
; check: return v8
}

function %iadd_sub_5_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v2, v3
    v5 = isub v1, v4
    v6 = iadd v0, v5
    return v6
; check:  v7 = iadd v0, v1
; nextln: v4 = isub v2, v3
; nextln: v8 = isub v7, v4
; check: return v8
}

function %iadd_sub_6_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v2, v3
    v5 = isub v1, v4
    v6 = iadd v0, v5
    return v6
; check:  v7 = iadd v0, v1
; nextln: v4 = iadd v2, v3
; nextln: v8 = isub v7, v4
; check: return v8
}

function %iadd_sub_7_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v2, v3
    v5 = iadd v1, v4
    v6 = iadd v0, v5
    return v6
; check:  v7 = iadd v0, v1
; nextln: v4 = isub v2, v3
; nextln: v8 = iadd v7, v4
; check: return v8
}

function %isub_add_1_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v0, v1
    v5 = isub v4, v2
    v6 = isub v5, v3
    return v6
; check:  v4 = isub v0, v1
; nextln: v7 = iadd v2, v3
; nextln: v8 = isub v4, v7
; check: return v8
}

function %isub_add_2_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v0, v1
    v5 = isub v4, v2
    v6 = iadd v5, v3
    return v6
; check:  v4 = isub v0, v1
; nextln: v7 = isub v2, v3
; nextln: v8 = isub v4, v7
; check: return v8
}

function %isub_add_3_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v0, v1
    v5 = iadd v4, v2
    v6 = isub v5, v3
    return v6
; check:  v4 = isub v0, v1
; nextln: v7 = isub v2, v3
; nextln: v8 = iadd v4, v7
; check: return v8
}

function %isub_add_4_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = isub v0, v1
    v5 = iadd v4, v2
    v6 = iadd v5, v3
    return v6
; check:  v4 = isub v0, v1
; nextln: v7 = iadd v2, v3
; nextln: v8 = iadd v4, v7
; check: return v8
}

function %isub_add_5_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v0, v1
    v5 = isub v4, v2
    v6 = isub v5, v3
    return v6
; check:  v4 = iadd v0, v1
; nextln: v7 = iadd v2, v3
; nextln: v8 = isub v4, v7
; check: return v8
}

function %isub_add_6_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v0, v1
    v5 = isub v4, v2
    v6 = iadd v5, v3
    return v6
; check:  v4 = iadd v0, v1
; nextln: v7 = isub v2, v3
; nextln: v8 = isub v4, v7
; check: return v8
}

function %isub_add_7_shallow_and_wide(i32, i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32, v3: i32):
    v4 = iadd v0, v1
    v5 = iadd v4, v2
    v6 = isub v5, v3
    return v6
; check:  v4 = iadd v0, v1
; nextln: v7 = isub v2, v3
; nextln: v8 = iadd v4, v7
; check: return v8
}