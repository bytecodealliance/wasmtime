test interpret
test run
target aarch64
target s390x
target x86_64
target x86_64 ssse3
target x86_64 sse41
target x86_64 sse42
target x86_64 sse42 has_avx
target x86_64 sse42 has_avx has_avx2
target x86_64 sse42 has_avx has_avx2 has_avx512f has_avx512vl
set enable_multi_ret_implicit_sret
target riscv64 has_v
target riscv64 has_v has_c has_zcb


function %sshr_i8x16(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i8x16([0 0xff 2 0xfd 4 0xfb 6 0xf9 8 0xf7 10 0xf5 12 0xf3 14 0xf1], 1) == [0 0xff 1 0xfe 2 0xfd 3 0xfc 4 0xfb 5 0xfa 6 0xf9 7 0xf8]
; run: %sshr_i8x16([0 0xff 2 0xfd 4 0xfb 6 0xf9 8 0xf7 10 0xf5 12 0xf3 14 0xf1], 9) == [0 0xff 1 0xfe 2 0xfd 3 0xfc 4 0xfb 5 0xfa 6 0xf9 7 0xf8]

function %sshr_i16x8(i16x8, i32) -> i16x8 {
block0(v0: i16x8, v1: i32):
    v2 = sshr v0, v1
    return v2
}
; note: because of the shifted-in sign-bit, lane 0 remains -1 == 0xffff, whereas lane 4 has been shifted to -8 == 0xfff8
; run: %sshr_i16x8([-1 2 4 8 -16 32 64 128], 1) == [-1 1 2 4 -8 16 32 64]
; run: %sshr_i16x8([-1 2 4 8 -16 32 64 128], 17) == [-1 1 2 4 -8 16 32 64]

function %sshr_i32x4(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i32x4([1 2 4 -8], 1) == [0 1 2 -4]
; run: %sshr_i32x4([1 2 4 -8], 33) == [0 1 2 -4]

function %sshr_i64x2(i64x2, i32) -> i64x2 {
block0(v0:i64x2, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i64x2([1 -1], 0) == [1 -1]
; run: %sshr_i64x2([1 -1], 1) == [0 -1] ; note the -1 shift result
; run: %sshr_i64x2([2 -2], 1) == [1 -1]
; run: %sshr_i64x2([0x80000000_00000000 0x7FFFFFFF_FFFFFFFF], 63) == [0xFFFFFFFF_FFFFFFFF 0]
; run: %sshr_i64x2([2 -2], 65) == [1 -1]



function %sshr_imm_i32x4(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = sshr_imm v0, 1
    return v1
}
; run: %sshr_imm_i32x4([1 2 4 -8]) == [0 1 2 -4]

function %sshr_imm_i16x8(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = sshr_imm v0, 1
    return v1
}
; run: %sshr_imm_i16x8([1 2 4 -8 0 0 0 0]) == [0 1 2 -4 0 0 0 0]


function %i8x16_sshr_const(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iconst.i32 2
    v2 = sshr v0, v1
    return v2
}
; run: %i8x16_sshr_const([0x01 0x02 0x04 0x08 0x10 0x20 0x40 0x80 0 0 0 0 0 0 0 0]) == [0 0 0x01 0x02 0x04 0x08 0x10 0xe0 0 0 0 0 0 0 0 0]

function %i16x8_sshr_const(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = iconst.i32 4
    v2 = sshr v0, v1
    return v2
}
; run: %i16x8_sshr_const([0x0001 0x0002 0x0004 0x0008 0x0010 0x0020 0x0040 0x0080]) == [0 0 0 0 0x1 0x2 0x4 0x8]
; run: %i16x8_sshr_const([-1 -2 -4 -8 -16 16 0x8000 0x80f3]) == [-1 -1 -1 -1 -1 1 0xf800 0xf80f]

function %i32x4_sshr_const(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = iconst.i32 4
    v2 = sshr v0, v1
    return v2
}
; run: %i32x4_sshr_const([1 0xfc 0x80000000 0xf83f3000]) == [0 0xf 0xf8000000 0xff83f300]

function %i64x2_sshr_const(i64x2) -> i64x2 {
block0(v0: i64x2):
    v1 = iconst.i32 32
    v2 = sshr v0, v1
    return v2
}
; run: %i64x2_sshr_const([0x1 0xf]) == [0 0]
; run: %i64x2_sshr_const([0x100000000 0]) == [1 0]
; run: %i64x2_sshr_const([-1 -1]) == [-1 -1]

function %i64x2_sshr_const2(i64x2) -> i64x2 {
block0(v0: i64x2):
    v1 = iconst.i32 8
    v2 = sshr v0, v1
    return v2
}
; run: %i64x2_sshr_const2([0x1 0xf]) == [0 0]
; run: %i64x2_sshr_const2([0x100000000 0]) == [0x1000000 0]
; run: %i64x2_sshr_const2([-1 -1]) == [-1 -1]

function %i64x2_sshr_const3(i64x2) -> i64x2 {
block0(v0: i64x2):
    v1 = iconst.i32 40
    v2 = sshr v0, v1
    return v2
}
; run: %i64x2_sshr_const3([0x1 0xf]) == [0 0]
; run: %i64x2_sshr_const3([0x10000000000 0]) == [1 0]
; run: %i64x2_sshr_const3([-1 -1]) == [-1 -1]
; run: %i64x2_sshr_const3([0x8000000080000000 0x8000000080000000]) == [0xffffffffff800000 0xffffffffff800000]
; run: %i64x2_sshr_const3([0x2424242424244424 0x8b1b1b1bffffff24]) == [0x242424 0xffffffffff8b1b1b]
; run: %i64x2_sshr_const3([0x2424242424244424 0x1b1b1b1bffffff24]) == [0x242424 0x1b1b1b]

