test interpret
test run
set opt_level=none
target aarch64
target s390x
target riscv64
target riscv64 has_c has_zcb
target s390x has_mie3
target x86_64
target pulley32
target pulley32be
target pulley64
target pulley64be

set opt_level=speed
target aarch64
target s390x
target riscv64
target riscv64 has_c has_zcb
target s390x has_mie3
target x86_64
target pulley32
target pulley32be
target pulley64
target pulley64be

function %bnot_band() -> i8 {
block0:
    v1 = iconst.i8 0
    v2 = iconst.i8 1
    v3 = bnot v1
    v4 = band v3, v2
    return v4
}
; run: %bnot_band() == 1

;; We have a optimization rule in the midend that turns this into a bmask
;; It's easier to have a runtest to ensure that it is correct than to inspect the output.
function %bitops_bmask(i16) -> i16 {
block0(v0: i16):
    v1 = bnot v0
    v2 = iconst.i16 1
    v3 = iadd.i16 v1, v2
    v4 = bor.i16 v0, v3
    v5 = iconst.i16 15
    v6 = ushr.i16 v4, v5
    v7 = iconst.i16 1
    v8 = isub.i16 v6, v7
    v9 = bnot.i16 v8
    return v9
}
; run: %bitops_bmask(0) == 0
; run: %bitops_bmask(1) == -1
; run: %bitops_bmask(0xFFFF) == -1
; run: %bitops_bmask(0x8000) == -1

function %a64_extr_i32_12(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = ushr_imm v0, 12
    v3 = ishl_imm v1, 20
    v4 = bor v2, v3
    return v4
}
; run: %a64_extr_i32_12(0x1234_5678, 0x1234_5678) == 0x678_1234_5
; run: %a64_extr_i32_12(0x1234_5678, 0x9abc_def0) == 0xef0_1234_5

function %a64_extr_i32_12_swap(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = ishl_imm v0, 20
    v3 = ushr_imm v1, 12
    v4 = bor v2, v3
    return v4
}
; run: %a64_extr_i32_12_swap(0x1234_5678, 0x1234_5678) == 0x678_1234_5
; run: %a64_extr_i32_12_swap(0x1234_5678, 0x9abc_def0) == 0x678_9abc_d

function %a64_extr_i32_28(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = ushr_imm v0, 4
    v3 = ishl_imm v1, 28
    v4 = bor v2, v3
    return v4
}
; run: %a64_extr_i32_28(0x1234_5678, 0x1234_5678) == 0x8_1234_567
; run: %a64_extr_i32_28(0x1234_5678, 0x9abc_def0) == 0x0_1234_567

function %a64_extr_i32_28_swap(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = ishl_imm v0, 4
    v3 = ushr_imm v1, 28
    v4 = bor v2, v3
    return v4
}
; run: %a64_extr_i32_28_swap(0x1234_5678, 0x1234_5678) == 0x234_5678_1
; run: %a64_extr_i32_28_swap(0x1234_5678, 0x9abc_def0) == 0x234_5678_9

function %a64_extr_i64_12(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = ushr_imm v0, 12
    v3 = ishl_imm v1, 52
    v4 = bor v2, v3
    return v4
}
; run: %a64_extr_i64_12(0x0102_0304_0506_0708, 0x090a_0b0c_0d0e_0f00) == 0xf00_0102_0304_0506_0

function %a64_extr_i64_12_swap(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = ishl_imm v0, 52
    v3 = ushr_imm v1, 12
    v4 = bor v2, v3
    return v4
}
; run: %a64_extr_i64_12_swap(0x0102_0304_0506_0708, 0x090a_0b0c_0d0e_0f00) == 0x708_090a_0b0c_0d0e_0

;; (x | y) ^ (x & y) is optimized to (x ^ y)
function %test_bxor_bor_band_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bor v0, v1
    v3 = band v0, v1
    v4 = bxor v2, v3
    return v4
}

; run: %test_bxor_bor_band_i64(0, 0) == 0
; run: %test_bxor_bor_band_i64(0, 1) == 1
; run: %test_bxor_bor_band_i64(1, 0) == 1
; run: %test_bxor_bor_band_i64(1, 1) == 0
; run: %test_bxor_bor_band_i64(7879, 7879) == 0


function %test_and_and1(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v0, v1
    v3 = band v2, v0
    return v3
}

; run: %test_and_and1(4, 3) == 0

function %test_and_and2(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v0, v1
    v3 = band v0, v2
    return v3
}

; run: %test_and_and2(4, 3) == 0

function %test_and_and3(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v1, v0
    v3 = band v0, v2
    return v3
}

; run: %test_and_and3(4, 3) == 0

function %test_and_and4(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v1, v0
    v3 = band v2, v0
    return v3
}

; run: %test_and_and4(4, 3) == 0

function %fold_and_xor_not(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bnot v1
    v3 = bxor v0, v2
    v4 = band v3, v0
    return v4
}

; run: %fold_and_xor_not(4, 5) == 4

function %fold_add_or_and3(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bor v0, v1
    v3 = band v0, v1
    v4 = iadd v2, v3
    return v4
}

; run: %fold_add_or_and3(4, 5) == 9

function %fold_add_and_xor4(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band v0, v1
    v3 = bxor v0, v1
    v4 = iadd v2, v3
    return v4
}

; run: %fold_add_and_xor4(1, 2) == 3

function %fold_or_and14(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band v0, v1
    v3 = bor v2, v0
    return v3
}

; run: %fold_or_and14(0xffff0000, 0x0000ffff) == 0xffff0000

function %fold_xor_xor15(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bxor v0, v1
    v3 = bxor v1, v2
    return v3
}

; run: %fold_xor_xor15(0xdead0000, 0x0000beef) == 0xdead0000

function %fold_or_and_not(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v0, v1
    v3 = bnot v0
    v4 = bor v2, v3
    return v4
}

; run: %fold_or_and_not(0, 0) == 0xffffffff

function %test_bxor_band_band(i32, i32, i32) -> i32 fast {
block0(v0: i32, v1: i32, v2: i32):
    v3 = band v0, v1  
    v4 = band v0, v2 
    v5 = bxor v3, v4
    return v5
}

; run: %test_bxor_band_band(2, 1, 1) == 0

