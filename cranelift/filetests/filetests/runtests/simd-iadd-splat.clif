test interpret
test run
target aarch64
target s390x
target x86_64
target x86_64 sse42
target x86_64 sse42 has_avx
set enable_multi_ret_implicit_sret
target riscv64 has_v
target riscv64 has_v has_c has_zcb
target pulley32
target pulley32be
target pulley64
target pulley64be

function %iadd_splat_const_i8x16(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = iconst.i8 5
    v2 = splat.i8x16 v1
    v3 = iadd v0, v2
    return v3
}
; run: %iadd_splat_const_i8x16([1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16]) == [6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21]

function %iadd_splat_const_i16x8(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = iconst.i16 -16
    v2 = splat.i16x8 v1
    v3 = iadd v0, v2
    return v3
}
; run: %iadd_splat_const_i16x8([1 2 3 4 5 6 7 8]) == [-15 -14 -13 -12 -11 -10 -9 -8]

function %iadd_splat_const_i32x4(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = iconst.i32 15
    v2 = splat.i32x4 v1
    v3 = iadd v0, v2
    return v3
}
; run: %iadd_splat_const_i32x4([1 2 3 4]) == [16 17 18 19]

function %iadd_splat_const_i64x2(i64x2) -> i64x2 {
block0(v0: i64x2):
    v1 = iconst.i64 -5
    v2 = splat.i64x2 v1
    v3 = iadd v2, v0
    return v3
}
; run: %iadd_splat_const_i64x2([1 2]) == [-4 -3]


function %iadd_splat_i8x16(i8x16,  i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
    v2 = splat.i8x16 v1
    v3 = iadd v0, v2
    return v3
}
; run: %iadd_splat_i8x16([1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16], -15) == [-14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1]

function %iadd_splat_i16x8(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
    v2 = splat.i16x8 v1
    v3 = iadd v0, v2
    return v3
}
; run: %iadd_splat_i16x8([1 2 3 4 5 6 7 8], -10) == [-9 -8 -7 -6 -5 -4 -3 -2]

function %iadd_splat_i32x4(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
    v2 = splat.i32x4 v1
    v3 = iadd v0, v2
    return v3
}
; run: %iadd_splat_i32x4([1 2 3 4], 22) == [23 24 25 26]

function %iadd_splat_i64x2(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
    v2 = splat.i64x2 v1
    v3 = iadd v2, v0
    return v3
}
; run: %iadd_splat_i64x2([1 2], 10) == [11 12]


function %iadd_splat_sextend_i16x8(i16x8, i8) -> i16x8 {
block0(v0: i16x8, v1: i8):
    v2 = sextend.i16 v1
    v3 = splat.i16x8 v2
    v4 = iadd v0, v3
    return v4
}
; run: %iadd_splat_sextend_i16x8([1 -2 3 4 5 6 7 8], -10) == [-9 -12 -7 -6 -5 -4 -3 -2]

function %iadd_splat_sextend_i32x4(i32x4, i16) -> i32x4 {
block0(v0: i32x4, v1: i16):
    v2 = sextend.i32 v1
    v3 = splat.i32x4 v2
    v4 = iadd v0, v3
    return v4
}
; run: %iadd_splat_sextend_i32x4([1 -2 3 4], -10) == [-9 -12 -7 -6]

function %iadd_splat_sextend_i64x2(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = sextend.i64 v1
    v3 = splat.i64x2 v2
    v4 = iadd v0, v3
    return v4
}
; run: %iadd_splat_sextend_i64x2([1 -2], 10) == [11 8]


function %iadd_splat_uextend_i16x8(i16x8, i8) -> i16x8 {
block0(v0: i16x8, v1: i8):
    v2 = uextend.i16 v1
    v3 = splat.i16x8 v2
    v4 = iadd v0, v3
    return v4
}
; run: %iadd_splat_uextend_i16x8([1 -2 3 4 5 6 7 8], 10) == [11 8 13 14 15 16 17 18]
; run: %iadd_splat_uextend_i16x8([1 -2 3 4 5 6 7 8], -10) == [0xf7 0xf4 0xf9 0xfa 0xfb 0xfc 0xfd 0xfe]

function %iadd_splat_uextend_i32x4(i32x4, i16) -> i32x4 {
block0(v0: i32x4, v1: i16):
    v2 = uextend.i32 v1
    v3 = splat.i32x4 v2
    v4 = iadd v0, v3
    return v4
}
; run: %iadd_splat_uextend_i32x4([1 -2 3 4], 10) == [11 8 13 14]
; run: %iadd_splat_uextend_i32x4([1 -2 3 4], -10) == [0xfff7 0xfff4 0xfff9 0xfffa]

function %iadd_splat_uextend_i64x2(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = uextend.i64 v1
    v3 = splat.i64x2 v2
    v4 = iadd v0, v3
    return v4
}
; run: %iadd_splat_uextend_i64x2([1 -2], 10) == [11 8]
; run: %iadd_splat_uextend_i64x2([1 -2], -10) == [0xfffffff7 0xfffffff4]
