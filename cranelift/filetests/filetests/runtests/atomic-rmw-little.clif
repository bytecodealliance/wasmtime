test interpret
test run
target s390x
target s390x has_mie2
target aarch64
target aarch64 has_lse
target x86_64
target riscv64 has_a
target riscv64 has_c has_zcb

; We can't test that these instructions are right regarding atomicity, but we can
; test if they perform their operation correctly

function %atomic_rmw_add_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little add v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_add_i64(0, 0) == [0, 0]
; run: %atomic_rmw_add_i64(1, 0) == [1, 1]
; run: %atomic_rmw_add_i64(0, 1) == [0, 1]
; run: %atomic_rmw_add_i64(1, 1) == [1, 2]
; run: %atomic_rmw_add_i64(0xC0FFEEEE_C0FFEEEE, 0x1DCB1111_1DCB1111) == [0xC0FFEEEE_C0FFEEEE, 0xDECAFFFF_DECAFFFF]

function %atomic_rmw_add_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little add v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_add_i64_no_res(0, 0) == 0
; run: %atomic_rmw_add_i64_no_res(1, 0) == 1
; run: %atomic_rmw_add_i64_no_res(0, 1) == 1
; run: %atomic_rmw_add_i64_no_res(1, 1) == 2
; run: %atomic_rmw_add_i64_no_res(0xC0FFEEEE_C0FFEEEE, 0x1DCB1111_1DCB1111) == 0xDECAFFFF_DECAFFFF

function %atomic_rmw_add_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little add v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_add_i32(0, 0) == [0, 0]
; run: %atomic_rmw_add_i32(1, 0) == [1, 1]
; run: %atomic_rmw_add_i32(0, 1) == [0, 1]
; run: %atomic_rmw_add_i32(1, 1) == [1, 2]
; run: %atomic_rmw_add_i32(0xC0FFEEEE, 0x1DCB1111) == [0xC0FFEEEE, 0xDECAFFFF]

function %atomic_rmw_add_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little add v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_add_i32_no_res(0, 0) == 0
; run: %atomic_rmw_add_i32_no_res(1, 0) == 1
; run: %atomic_rmw_add_i32_no_res(0, 1) == 1
; run: %atomic_rmw_add_i32_no_res(1, 1) == 2
; run: %atomic_rmw_add_i32_no_res(0xC0FFEEEE, 0x1DCB1111) == 0xDECAFFFF



function %atomic_rmw_sub_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little sub v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_sub_i64(0, 0) == [0, 0]
; run: %atomic_rmw_sub_i64(1, 0) == [1, 1]
; run: %atomic_rmw_sub_i64(0, 1) == [0, -1]
; run: %atomic_rmw_sub_i64(1, 1) == [1, 0]
; run: %atomic_rmw_sub_i64(0xDECAFFFF_DECAFFFF, 0x1DCB1111_1DCB1111) == [0xDECAFFFF_DECAFFFF, 0xC0FFEEEE_C0FFEEEE]

function %atomic_rmw_sub_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little sub v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_sub_i64_no_res(0, 0) == 0
; run: %atomic_rmw_sub_i64_no_res(1, 0) == 1
; run: %atomic_rmw_sub_i64_no_res(0, 1) == -1
; run: %atomic_rmw_sub_i64_no_res(1, 1) == 0
; run: %atomic_rmw_sub_i64_no_res(0xDECAFFFF_DECAFFFF, 0x1DCB1111_1DCB1111) == 0xC0FFEEEE_C0FFEEEE

function %atomic_rmw_sub_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little sub v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_sub_i32(0, 0) == [0, 0]
; run: %atomic_rmw_sub_i32(1, 0) == [1, 1]
; run: %atomic_rmw_sub_i32(0, 1) == [0, -1]
; run: %atomic_rmw_sub_i32(1, 1) == [1, 0]
; run: %atomic_rmw_sub_i32(0xDECAFFFF, 0x1DCB1111) == [0xDECAFFFF, 0xC0FFEEEE]

function %atomic_rmw_sub_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little sub v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_sub_i32_no_res(0, 0) == 0
; run: %atomic_rmw_sub_i32_no_res(1, 0) == 1
; run: %atomic_rmw_sub_i32_no_res(0, 1) == -1
; run: %atomic_rmw_sub_i32_no_res(1, 1) == 0
; run: %atomic_rmw_sub_i32_no_res(0xDECAFFFF, 0x1DCB1111) == 0xC0FFEEEE



function %atomic_rmw_and_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little and v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_and_i64(0, 0) == [0, 0]
; run: %atomic_rmw_and_i64(1, 0) == [1, 0]
; run: %atomic_rmw_and_i64(0, 1) == [0, 0]
; run: %atomic_rmw_and_i64(1, 1) == [1, 1]
; run: %atomic_rmw_and_i64(0xF1FFFEFE_FEEEFFFF, 0xCEFFEFEF_DFDBFFFF) == [0xF1FFFEFE_FEEEFFFF, 0xC0FFEEEE_DECAFFFF]

function %atomic_rmw_and_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little and v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_and_i64_no_res(0, 0) == 0
; run: %atomic_rmw_and_i64_no_res(1, 0) == 0
; run: %atomic_rmw_and_i64_no_res(0, 1) == 0
; run: %atomic_rmw_and_i64_no_res(1, 1) == 1
; run: %atomic_rmw_and_i64_no_res(0xF1FFFEFE_FEEEFFFF, 0xCEFFEFEF_DFDBFFFF) == 0xC0FFEEEE_DECAFFFF

function %atomic_rmw_and_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little and v2, v1

    v4 = load.i32 little v2
    return v3, v4
}

; run: %atomic_rmw_and_i32(0, 0) == [0, 0]
; run: %atomic_rmw_and_i32(1, 0) == [1, 0]
; run: %atomic_rmw_and_i32(0, 1) == [0, 0]
; run: %atomic_rmw_and_i32(1, 1) == [1, 1]
; run: %atomic_rmw_and_i32(0xF1FFFEFE, 0xCEFFEFEF) == [0xF1FFFEFE, 0xC0FFEEEE]

function %atomic_rmw_and_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little and v2, v1

    v4 = load.i32 little v2
    return v4
}

; run: %atomic_rmw_and_i32_no_res(0, 0) == 0
; run: %atomic_rmw_and_i32_no_res(1, 0) == 0
; run: %atomic_rmw_and_i32_no_res(0, 1) == 0
; run: %atomic_rmw_and_i32_no_res(1, 1) == 1
; run: %atomic_rmw_and_i32_no_res(0xF1FFFEFE, 0xCEFFEFEF) == 0xC0FFEEEE



function %atomic_rmw_or_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little or v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_or_i64(0, 0) == [0, 0]
; run: %atomic_rmw_or_i64(1, 0) == [1, 1]
; run: %atomic_rmw_or_i64(0, 1) == [0, 1]
; run: %atomic_rmw_or_i64(1, 1) == [1, 1]
; run: %atomic_rmw_or_i64(0x80AAAAAA_8A8AAAAA, 0x40554444_54405555) == [0x80AAAAAA_8A8AAAAA, 0xC0FFEEEE_DECAFFFF]

function %atomic_rmw_or_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little or v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_or_i64_no_res(0, 0) == 0
; run: %atomic_rmw_or_i64_no_res(1, 0) == 1
; run: %atomic_rmw_or_i64_no_res(0, 1) == 1
; run: %atomic_rmw_or_i64_no_res(1, 1) == 1
; run: %atomic_rmw_or_i64_no_res(0x80AAAAAA_8A8AAAAA, 0x40554444_54405555) == 0xC0FFEEEE_DECAFFFF

function %atomic_rmw_or_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little or v2, v1

    v4 = load.i32 little v2
    return v3, v4
}

; run: %atomic_rmw_or_i32(0, 0) == [0, 0]
; run: %atomic_rmw_or_i32(1, 0) == [1, 1]
; run: %atomic_rmw_or_i32(0, 1) == [0, 1]
; run: %atomic_rmw_or_i32(1, 1) == [1, 1]
; run: %atomic_rmw_or_i32(0x80AAAAAA, 0x40554444) == [0x80AAAAAA, 0xC0FFEEEE]

function %atomic_rmw_or_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little or v2, v1

    v4 = load.i32 little v2
    return v4
}

; run: %atomic_rmw_or_i32_no_res(0, 0) == 0
; run: %atomic_rmw_or_i32_no_res(1, 0) == 1
; run: %atomic_rmw_or_i32_no_res(0, 1) == 1
; run: %atomic_rmw_or_i32_no_res(1, 1) == 1
; run: %atomic_rmw_or_i32_no_res(0x80AAAAAA, 0x40554444) == 0xC0FFEEEE



function %atomic_rmw_xor_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little xor v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_xor_i64(0, 0) == [0, 0]
; run: %atomic_rmw_xor_i64(1, 0) == [1, 1]
; run: %atomic_rmw_xor_i64(0, 1) == [0, 1]
; run: %atomic_rmw_xor_i64(1, 1) == [1, 0]
; run: %atomic_rmw_xor_i64(0x8FA50A64_9440A07D, 0x4F5AE48A_4A8A5F82) == [0x8FA50A64_9440A07D, 0xC0FFEEEE_DECAFFFF]

function %atomic_rmw_xor_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little xor v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_xor_i64_no_res(0, 0) == 0
; run: %atomic_rmw_xor_i64_no_res(1, 0) == 1
; run: %atomic_rmw_xor_i64_no_res(0, 1) == 1
; run: %atomic_rmw_xor_i64_no_res(1, 1) == 0
; run: %atomic_rmw_xor_i64_no_res(0x8FA50A64_9440A07D, 0x4F5AE48A_4A8A5F82) == 0xC0FFEEEE_DECAFFFF

function %atomic_rmw_xor_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little xor v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_xor_i32(0, 0) == [0, 0]
; run: %atomic_rmw_xor_i32(1, 0) == [1, 1]
; run: %atomic_rmw_xor_i32(0, 1) == [0, 1]
; run: %atomic_rmw_xor_i32(1, 1) == [1, 0]
; run: %atomic_rmw_xor_i32(0x8FA50A64, 0x4F5AE48A) == [0x8FA50A64, 0xC0FFEEEE]

function %atomic_rmw_xor_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little xor v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_xor_i32_no_res(0, 0) == 0
; run: %atomic_rmw_xor_i32_no_res(1, 0) == 1
; run: %atomic_rmw_xor_i32_no_res(0, 1) == 1
; run: %atomic_rmw_xor_i32_no_res(1, 1) == 0
; run: %atomic_rmw_xor_i32_no_res(0x8FA50A64, 0x4F5AE48A) == 0xC0FFEEEE



function %atomic_rmw_nand_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little nand v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_nand_i64(0, 0) == [0, -1]
; run: %atomic_rmw_nand_i64(1, 0) == [1, -1]
; run: %atomic_rmw_nand_i64(0, 1) == [0, -1]
; run: %atomic_rmw_nand_i64(1, 1) == [1, -2]
; run: %atomic_rmw_nand_i64(0xC0FFEEEE_DECAFFFF, 0x7DCB5691_7DCB5691) == [0xC0FFEEEE_DECAFFFF, 0xBF34B97F_A335A96E]

function %atomic_rmw_nand_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little nand v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_nand_i64_no_res(0, 0) == -1
; run: %atomic_rmw_nand_i64_no_res(1, 0) == -1
; run: %atomic_rmw_nand_i64_no_res(0, 1) == -1
; run: %atomic_rmw_nand_i64_no_res(1, 1) == -2
; run: %atomic_rmw_nand_i64_no_res(0xC0FFEEEE_DECAFFFF, 0x7DCB5691_7DCB5691) == 0xBF34B97F_A335A96E

function %atomic_rmw_nand_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little nand v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_nand_i32(0, 0) == [0, -1]
; run: %atomic_rmw_nand_i32(1, 0) == [1, -1]
; run: %atomic_rmw_nand_i32(0, 1) == [0, -1]
; run: %atomic_rmw_nand_i32(1, 1) == [1, -2]
; run: %atomic_rmw_nand_i32(0xC0FFEEEE, 0x7DCB5691) == [0xC0FFEEEE, 0xBF34B97F]

function %atomic_rmw_nand_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little nand v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_nand_i32_no_res(0, 0) == -1
; run: %atomic_rmw_nand_i32_no_res(1, 0) == -1
; run: %atomic_rmw_nand_i32_no_res(0, 1) == -1
; run: %atomic_rmw_nand_i32_no_res(1, 1) == -2
; run: %atomic_rmw_nand_i32_no_res(0xC0FFEEEE, 0x7DCB5691) == 0xBF34B97F



function %atomic_rmw_umin_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little umin v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_umin_i64(0, 0) == [0, 0]
; run: %atomic_rmw_umin_i64(1, 0) == [1, 0]
; run: %atomic_rmw_umin_i64(0, 1) == [0, 0]
; run: %atomic_rmw_umin_i64(1, 1) == [1, 1]
; run: %atomic_rmw_umin_i64(-1, 1) == [-1, 1]
; run: %atomic_rmw_umin_i64(-1, -3) == [-1, -3]

function %atomic_rmw_umin_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little umin v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_umin_i64_no_res(0, 0) == 0
; run: %atomic_rmw_umin_i64_no_res(1, 0) == 0
; run: %atomic_rmw_umin_i64_no_res(0, 1) == 0
; run: %atomic_rmw_umin_i64_no_res(1, 1) == 1
; run: %atomic_rmw_umin_i64_no_res(-1, 1) == 1
; run: %atomic_rmw_umin_i64_no_res(-1, -3) == -3

function %atomic_rmw_umin_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little umin v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_umin_i32(0, 0) == [0, 0]
; run: %atomic_rmw_umin_i32(1, 0) == [1, 0]
; run: %atomic_rmw_umin_i32(0, 1) == [0, 0]
; run: %atomic_rmw_umin_i32(1, 1) == [1, 1]
; run: %atomic_rmw_umin_i32(-1, 1) == [-1, 1]
; run: %atomic_rmw_umin_i32(-1, -3) == [-1, -3]

function %atomic_rmw_umin_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little umin v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_umin_i32_no_res(0, 0) == 0
; run: %atomic_rmw_umin_i32_no_res(1, 0) == 0
; run: %atomic_rmw_umin_i32_no_res(0, 1) == 0
; run: %atomic_rmw_umin_i32_no_res(1, 1) == 1
; run: %atomic_rmw_umin_i32_no_res(-1, 1) == 1
; run: %atomic_rmw_umin_i32_no_res(-1, -3) == -3



function %atomic_rmw_umax_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little umax v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_umax_i64(0, 0) == [0, 0]
; run: %atomic_rmw_umax_i64(1, 0) == [1, 1]
; run: %atomic_rmw_umax_i64(0, 1) == [0, 1]
; run: %atomic_rmw_umax_i64(1, 1) == [1, 1]
; run: %atomic_rmw_umax_i64(-1, 1) == [-1, -1]
; run: %atomic_rmw_umax_i64(-1, -3) == [-1, -1]

function %atomic_rmw_umax_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little umax v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_umax_i64_no_res(0, 0) == 0
; run: %atomic_rmw_umax_i64_no_res(1, 0) == 1
; run: %atomic_rmw_umax_i64_no_res(0, 1) == 1
; run: %atomic_rmw_umax_i64_no_res(1, 1) == 1
; run: %atomic_rmw_umax_i64_no_res(-1, 1) == -1
; run: %atomic_rmw_umax_i64_no_res(-1, -3) == -1

function %atomic_rmw_umax_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little umax v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_umax_i32(0, 0) == [0, 0]
; run: %atomic_rmw_umax_i32(1, 0) == [1, 1]
; run: %atomic_rmw_umax_i32(0, 1) == [0, 1]
; run: %atomic_rmw_umax_i32(1, 1) == [1, 1]
; run: %atomic_rmw_umax_i32(-1, 1) == [-1, -1]
; run: %atomic_rmw_umax_i32(-1, -3) == [-1, -1]

function %atomic_rmw_umax_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little umax v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_umax_i32_no_res(0, 0) == 0
; run: %atomic_rmw_umax_i32_no_res(1, 0) == 1
; run: %atomic_rmw_umax_i32_no_res(0, 1) == 1
; run: %atomic_rmw_umax_i32_no_res(1, 1) == 1
; run: %atomic_rmw_umax_i32_no_res(-1, 1) == -1
; run: %atomic_rmw_umax_i32_no_res(-1, -3) == -1



function %atomic_rmw_smin_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little smin v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_smin_i64(0, 0) == [0, 0]
; run: %atomic_rmw_smin_i64(1, 0) == [1, 0]
; run: %atomic_rmw_smin_i64(0, 1) == [0, 0]
; run: %atomic_rmw_smin_i64(1, 1) == [1, 1]
; run: %atomic_rmw_smin_i64(-1, 1) == [-1, -1]
; run: %atomic_rmw_smin_i64(-1, -3) == [-1, -3]

function %atomic_rmw_smin_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little smin v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_smin_i64_no_res(0, 0) == 0
; run: %atomic_rmw_smin_i64_no_res(1, 0) == 0
; run: %atomic_rmw_smin_i64_no_res(0, 1) == 0
; run: %atomic_rmw_smin_i64_no_res(1, 1) == 1
; run: %atomic_rmw_smin_i64_no_res(-1, 1) == -1
; run: %atomic_rmw_smin_i64_no_res(-1, -3) == -3

function %atomic_rmw_smin_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little smin v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_smin_i32(0, 0) == [0, 0]
; run: %atomic_rmw_smin_i32(1, 0) == [1, 0]
; run: %atomic_rmw_smin_i32(0, 1) == [0, 0]
; run: %atomic_rmw_smin_i32(1, 1) == [1, 1]
; run: %atomic_rmw_smin_i32(-1, -1) == [-1, -1]
; run: %atomic_rmw_smin_i32(-1, -3) == [-1, -3]

function %atomic_rmw_smin_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little smin v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_smin_i32_no_res(0, 0) == 0
; run: %atomic_rmw_smin_i32_no_res(1, 0) == 0
; run: %atomic_rmw_smin_i32_no_res(0, 1) == 0
; run: %atomic_rmw_smin_i32_no_res(1, 1) == 1
; run: %atomic_rmw_smin_i32_no_res(-1, -1) == -1
; run: %atomic_rmw_smin_i32_no_res(-1, -3) == -3



function %atomic_rmw_smax_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little smax v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_smax_i64(0, 0) == [0, 0]
; run: %atomic_rmw_smax_i64(1, 0) == [1, 1]
; run: %atomic_rmw_smax_i64(0, 1) == [0, 1]
; run: %atomic_rmw_smax_i64(1, 1) == [1, 1]
; run: %atomic_rmw_smax_i64(-1, 1) == [-1, 1]
; run: %atomic_rmw_smax_i64(-1, -3) == [-1, -1]

function %atomic_rmw_smax_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little smax v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_smax_i64_no_res(0, 0) == 0
; run: %atomic_rmw_smax_i64_no_res(1, 0) == 1
; run: %atomic_rmw_smax_i64_no_res(0, 1) == 1
; run: %atomic_rmw_smax_i64_no_res(1, 1) == 1
; run: %atomic_rmw_smax_i64_no_res(-1, 1) == 1
; run: %atomic_rmw_smax_i64_no_res(-1, -3) == -1

function %atomic_rmw_smax_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little smax v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_smax_i32(0, 0) == [0, 0]
; run: %atomic_rmw_smax_i32(1, 0) == [1, 1]
; run: %atomic_rmw_smax_i32(0, 1) == [0, 1]
; run: %atomic_rmw_smax_i32(1, 1) == [1, 1]
; run: %atomic_rmw_smax_i32(-1, 1) == [-1, 1]
; run: %atomic_rmw_smax_i32(-1, -3) == [-1, -1]

function %atomic_rmw_smax_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little smax v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_smax_i32_no_res(0, 0) == 0
; run: %atomic_rmw_smax_i32_no_res(1, 0) == 1
; run: %atomic_rmw_smax_i32_no_res(0, 1) == 1
; run: %atomic_rmw_smax_i32_no_res(1, 1) == 1
; run: %atomic_rmw_smax_i32_no_res(-1, 1) == 1
; run: %atomic_rmw_smax_i32_no_res(-1, -3) == -1



function %atomic_rmw_xchg_i64(i64, i64) -> i64, i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little xchg v2, v1

    v4 = load.i64 little v2
    return v3, v4
}
; run: %atomic_rmw_xchg_i64(0, 0) == [0, 0]
; run: %atomic_rmw_xchg_i64(1, 0) == [1, 0]
; run: %atomic_rmw_xchg_i64(0, 1) == [0, 1]
; run: %atomic_rmw_xchg_i64(0, 0xC0FFEEEE_DECAFFFF) == [0, 0xC0FFEEEE_DECAFFFF]

function %atomic_rmw_xchg_i64_no_res(i64, i64) -> i64 {
    ss0 = explicit_slot 8

block0(v0: i64, v1: i64):
    v2 = stack_addr.i64 ss0
    store.i64 little v0, v2

    v3 = atomic_rmw.i64 little xchg v2, v1

    v4 = load.i64 little v2
    return v4
}
; run: %atomic_rmw_xchg_i64_no_res(0, 0) == 0
; run: %atomic_rmw_xchg_i64_no_res(1, 0) == 0
; run: %atomic_rmw_xchg_i64_no_res(0, 1) == 1
; run: %atomic_rmw_xchg_i64_no_res(0, 0xC0FFEEEE_DECAFFFF) == 0xC0FFEEEE_DECAFFFF

function %atomic_rmw_xchg_i32(i32, i32) -> i32, i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little xchg v2, v1

    v4 = load.i32 little v2
    return v3, v4
}
; run: %atomic_rmw_xchg_i32(0, 0) == [0, 0]
; run: %atomic_rmw_xchg_i32(1, 0) == [1, 0]
; run: %atomic_rmw_xchg_i32(0, 1) == [0, 1]
; run: %atomic_rmw_xchg_i32(0, 0xC0FFEEEE) == [0, 0xC0FFEEEE]

function %atomic_rmw_xchg_i32_no_res(i32, i32) -> i32 {
    ss0 = explicit_slot 4

block0(v0: i32, v1: i32):
    v2 = stack_addr.i64 ss0
    store.i32 little v0, v2

    v3 = atomic_rmw.i32 little xchg v2, v1

    v4 = load.i32 little v2
    return v4
}
; run: %atomic_rmw_xchg_i32_no_res(0, 0) == 0
; run: %atomic_rmw_xchg_i32_no_res(1, 0) == 0
; run: %atomic_rmw_xchg_i32_no_res(0, 1) == 1
; run: %atomic_rmw_xchg_i32_no_res(0, 0xC0FFEEEE) == 0xC0FFEEEE
