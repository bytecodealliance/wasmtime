test compile precise-output
set unwind_info=false
target riscv64

function %bswap_i16(i16) -> i16 {
block0(v0: i16):
    v1 = bswap v0
    return v1
}

; VCode:
; block0:
;   slli a2,a0,8
;   srli a4,a0,8
;   slli a0,a4,56
;   srli a3,a0,56
;   or a0,a2,a3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 8
;   srli a4, a0, 8
;   slli a0, a4, 0x38
;   srli a3, a0, 0x38
;   or a0, a2, a3
;   ret

function %bswap_i32(i32) -> i32 {
block0(v0: i32):
    v1 = bswap v0
    return v1
}

; VCode:
; block0:
;   slli a2,a0,8
;   srli a4,a0,8
;   slli a1,a4,56
;   srli a3,a1,56
;   or a4,a2,a3
;   slli a1,a4,16
;   srli a2,a0,16
;   slli a4,a2,8
;   srli a0,a2,8
;   slli a2,a0,56
;   srli a5,a2,56
;   or a0,a4,a5
;   slli a2,a0,48
;   srli a4,a2,48
;   or a0,a1,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 8
;   srli a4, a0, 8
;   slli a1, a4, 0x38
;   srli a3, a1, 0x38
;   or a4, a2, a3
;   slli a1, a4, 0x10
;   srli a2, a0, 0x10
;   slli a4, a2, 8
;   srli a0, a2, 8
;   slli a2, a0, 0x38
;   srli a5, a2, 0x38
;   or a0, a4, a5
;   slli a2, a0, 0x30
;   srli a4, a2, 0x30
;   or a0, a1, a4
;   ret

function %bswap_i64(i64) -> i64 {
block0(v0: i64):
    v1 = bswap v0
    return v1
}

; VCode:
; block0:
;   slli a2,a0,8
;   srli a4,a0,8
;   slli a1,a4,56
;   srli a3,a1,56
;   or a4,a2,a3
;   slli a1,a4,16
;   srli a2,a0,16
;   slli a4,a2,8
;   srli a2,a2,8
;   slli a2,a2,56
;   srli a5,a2,56
;   or a2,a4,a5
;   slli a2,a2,48
;   srli a4,a2,48
;   or a1,a1,a4
;   slli a2,a1,32
;   srli a5,a0,32
;   slli a0,a5,8
;   srli a3,a5,8
;   slli a4,a3,56
;   srli a1,a4,56
;   or a3,a0,a1
;   slli a4,a3,16
;   srli a0,a5,16
;   slli a3,a0,8
;   srli a5,a0,8
;   slli a0,a5,56
;   srli a5,a0,56
;   or a5,a3,a5
;   slli a0,a5,48
;   srli a3,a0,48
;   or a4,a4,a3
;   slli a0,a4,32
;   srli a3,a0,32
;   or a0,a2,a3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 8
;   srli a4, a0, 8
;   slli a1, a4, 0x38
;   srli a3, a1, 0x38
;   or a4, a2, a3
;   slli a1, a4, 0x10
;   srli a2, a0, 0x10
;   slli a4, a2, 8
;   srli a2, a2, 8
;   slli a2, a2, 0x38
;   srli a5, a2, 0x38
;   or a2, a4, a5
;   slli a2, a2, 0x30
;   srli a4, a2, 0x30
;   or a1, a1, a4
;   slli a2, a1, 0x20
;   srli a5, a0, 0x20
;   slli a0, a5, 8
;   srli a3, a5, 8
;   slli a4, a3, 0x38
;   srli a1, a4, 0x38
;   or a3, a0, a1
;   slli a4, a3, 0x10
;   srli a0, a5, 0x10
;   slli a3, a0, 8
;   srli a5, a0, 8
;   slli a0, a5, 0x38
;   srli a5, a0, 0x38
;   or a5, a3, a5
;   slli a0, a5, 0x30
;   srli a3, a0, 0x30
;   or a4, a4, a3
;   slli a0, a4, 0x20
;   srli a3, a0, 0x20
;   or a0, a2, a3
;   ret

function %bswap_i128(i128) -> i128 {
block0(v0: i128):
    v1 = bswap v0
    return v1
}

; VCode:
; block0:
;   slli a3,a1,8
;   srli a5,a1,8
;   slli a2,a5,56
;   srli a4,a2,56
;   or a5,a3,a4
;   slli a2,a5,16
;   srli a3,a1,16
;   slli a5,a3,8
;   srli a3,a3,8
;   slli a3,a3,56
;   srli a3,a3,56
;   or a3,a5,a3
;   slli a3,a3,48
;   srli a5,a3,48
;   or a2,a2,a5
;   slli a3,a2,32
;   srli a1,a1,32
;   slli a2,a1,8
;   srli a4,a1,8
;   slli a5,a4,56
;   srli a4,a5,56
;   or a4,a2,a4
;   slli a5,a4,16
;   srli a1,a1,16
;   slli a4,a1,8
;   srli a1,a1,8
;   slli a1,a1,56
;   srli a1,a1,56
;   or a1,a4,a1
;   slli a1,a1,48
;   srli a4,a1,48
;   or a5,a5,a4
;   slli a1,a5,32
;   srli a4,a1,32
;   or a5,a3,a4
;   mv t1,a5
;   slli a1,a0,8
;   srli a3,a0,8
;   slli a5,a3,56
;   srli a2,a5,56
;   or a3,a1,a2
;   slli a5,a3,16
;   srli a1,a0,16
;   slli a3,a1,8
;   srli a1,a1,8
;   slli a1,a1,56
;   srli a4,a1,56
;   or a1,a3,a4
;   slli a1,a1,48
;   srli a3,a1,48
;   or a5,a5,a3
;   slli a1,a5,32
;   srli a4,a0,32
;   slli a5,a4,8
;   srli a2,a4,8
;   slli a3,a2,56
;   srli a0,a3,56
;   or a2,a5,a0
;   slli a3,a2,16
;   srli a5,a4,16
;   slli a2,a5,8
;   srli a4,a5,8
;   slli a5,a4,56
;   srli a4,a5,56
;   or a4,a2,a4
;   slli a5,a4,48
;   srli a2,a5,48
;   or a3,a3,a2
;   slli a5,a3,32
;   srli a2,a5,32
;   or a1,a1,a2
;   mv a0,t1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a3, a1, 8
;   srli a5, a1, 8
;   slli a2, a5, 0x38
;   srli a4, a2, 0x38
;   or a5, a3, a4
;   slli a2, a5, 0x10
;   srli a3, a1, 0x10
;   slli a5, a3, 8
;   srli a3, a3, 8
;   slli a3, a3, 0x38
;   srli a3, a3, 0x38
;   or a3, a5, a3
;   slli a3, a3, 0x30
;   srli a5, a3, 0x30
;   or a2, a2, a5
;   slli a3, a2, 0x20
;   srli a1, a1, 0x20
;   slli a2, a1, 8
;   srli a4, a1, 8
;   slli a5, a4, 0x38
;   srli a4, a5, 0x38
;   or a4, a2, a4
;   slli a5, a4, 0x10
;   srli a1, a1, 0x10
;   slli a4, a1, 8
;   srli a1, a1, 8
;   slli a1, a1, 0x38
;   srli a1, a1, 0x38
;   or a1, a4, a1
;   slli a1, a1, 0x30
;   srli a4, a1, 0x30
;   or a5, a5, a4
;   slli a1, a5, 0x20
;   srli a4, a1, 0x20
;   or a5, a3, a4
;   mv t1, a5
;   slli a1, a0, 8
;   srli a3, a0, 8
;   slli a5, a3, 0x38
;   srli a2, a5, 0x38
;   or a3, a1, a2
;   slli a5, a3, 0x10
;   srli a1, a0, 0x10
;   slli a3, a1, 8
;   srli a1, a1, 8
;   slli a1, a1, 0x38
;   srli a4, a1, 0x38
;   or a1, a3, a4
;   slli a1, a1, 0x30
;   srli a3, a1, 0x30
;   or a5, a5, a3
;   slli a1, a5, 0x20
;   srli a4, a0, 0x20
;   slli a5, a4, 8
;   srli a2, a4, 8
;   slli a3, a2, 0x38
;   srli a0, a3, 0x38
;   or a2, a5, a0
;   slli a3, a2, 0x10
;   srli a5, a4, 0x10
;   slli a2, a5, 8
;   srli a4, a5, 8
;   slli a5, a4, 0x38
;   srli a4, a5, 0x38
;   or a4, a2, a4
;   slli a5, a4, 0x30
;   srli a2, a5, 0x30
;   or a3, a3, a2
;   slli a5, a3, 0x20
;   srli a2, a5, 0x20
;   or a1, a1, a2
;   mv a0, t1
;   ret

