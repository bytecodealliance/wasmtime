test compile precise-output
set unwind_info=false
target riscv64

function %bitrev_i8(i8) -> i8 {
block0(v0: i8):
    v1 = bitrev v0
    return v1
}

; VCode:
; block0:
;   mv a1,a0
;   brev8 a0,a1##tmp=a2 tmp2=a3 step=a4 ty=i8
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a1, a0
;   mv a0, zero
;   addi a4, zero, 8
;   addi a2, zero, 1
;   slli a2, a2, 7
;   addi a3, zero, 1
;   slli a3, a3, 0
;   blez a4, 0x34
;   and t5, a2, a1
;   beq zero, t5, 8
;   or a0, a0, a3
;   addi a4, a4, -1
;   srli a2, a2, 1
;   addi t5, zero, 8
;   rem t5, a4, t5
;   bnez t5, 0xc
;   srli a3, a3, 0xf
;   j -0x28
;   slli a3, a3, 1
;   j -0x30
;   ret

function %bitrev_i16(i16) -> i16 {
block0(v0: i16):
    v1 = bitrev v0
    return v1
}

; VCode:
; block0:
;   slli a2,a0,8
;   srli a4,a0,8
;   slli a1,a4,56
;   srli a3,a1,56
;   or a4,a2,a3
;   slli a1,a4,16
;   srli a2,a0,16
;   slli a4,a2,8
;   srli a2,a2,8
;   slli a2,a2,56
;   srli a5,a2,56
;   or a2,a4,a5
;   slli a2,a2,48
;   srli a4,a2,48
;   or a1,a1,a4
;   slli a2,a1,32
;   srli a5,a0,32
;   slli a0,a5,8
;   srli a3,a5,8
;   slli a4,a3,56
;   srli a1,a4,56
;   or a3,a0,a1
;   slli a4,a3,16
;   srli a0,a5,16
;   slli a3,a0,8
;   srli a5,a0,8
;   slli a0,a5,56
;   srli a5,a0,56
;   or a5,a3,a5
;   slli a0,a5,48
;   srli a3,a0,48
;   or a4,a4,a3
;   slli a0,a4,32
;   srli a3,a0,32
;   or a4,a2,a3
;   brev8 a3,a4##tmp=a0 tmp2=a1 step=a2 ty=i64
;   srli a0,a3,48
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 8
;   srli a4, a0, 8
;   slli a1, a4, 0x38
;   srli a3, a1, 0x38
;   or a4, a2, a3
;   slli a1, a4, 0x10
;   srli a2, a0, 0x10
;   slli a4, a2, 8
;   srli a2, a2, 8
;   slli a2, a2, 0x38
;   srli a5, a2, 0x38
;   or a2, a4, a5
;   slli a2, a2, 0x30
;   srli a4, a2, 0x30
;   or a1, a1, a4
;   slli a2, a1, 0x20
;   srli a5, a0, 0x20
;   slli a0, a5, 8
;   srli a3, a5, 8
;   slli a4, a3, 0x38
;   srli a1, a4, 0x38
;   or a3, a0, a1
;   slli a4, a3, 0x10
;   srli a0, a5, 0x10
;   slli a3, a0, 8
;   srli a5, a0, 8
;   slli a0, a5, 0x38
;   srli a5, a0, 0x38
;   or a5, a3, a5
;   slli a0, a5, 0x30
;   srli a3, a0, 0x30
;   or a4, a4, a3
;   slli a0, a4, 0x20
;   srli a3, a0, 0x20
;   or a4, a2, a3
;   mv a3, zero
;   addi a2, zero, 0x40
;   addi a0, zero, 1
;   slli a0, a0, 0x3f
;   addi a1, zero, 1
;   slli a1, a1, 0x38
;   blez a2, 0x34
;   and t5, a0, a4
;   beq zero, t5, 8
;   or a3, a3, a1
;   addi a2, a2, -1
;   srli a0, a0, 1
;   addi t5, zero, 8
;   rem t5, a2, t5
;   bnez t5, 0xc
;   srli a1, a1, 0xf
;   j -0x28
;   slli a1, a1, 1
;   j -0x30
;   srli a0, a3, 0x30
;   ret

function %bitrev_i32(i32) -> i32 {
block0(v0: i32):
    v1 = bitrev v0
    return v1
}

; VCode:
; block0:
;   slli a2,a0,8
;   srli a4,a0,8
;   slli a1,a4,56
;   srli a3,a1,56
;   or a4,a2,a3
;   slli a1,a4,16
;   srli a2,a0,16
;   slli a4,a2,8
;   srli a2,a2,8
;   slli a2,a2,56
;   srli a5,a2,56
;   or a2,a4,a5
;   slli a2,a2,48
;   srli a4,a2,48
;   or a1,a1,a4
;   slli a2,a1,32
;   srli a5,a0,32
;   slli a0,a5,8
;   srli a3,a5,8
;   slli a4,a3,56
;   srli a1,a4,56
;   or a3,a0,a1
;   slli a4,a3,16
;   srli a0,a5,16
;   slli a3,a0,8
;   srli a5,a0,8
;   slli a0,a5,56
;   srli a5,a0,56
;   or a5,a3,a5
;   slli a0,a5,48
;   srli a3,a0,48
;   or a4,a4,a3
;   slli a0,a4,32
;   srli a3,a0,32
;   or a4,a2,a3
;   brev8 a3,a4##tmp=a0 tmp2=a1 step=a2 ty=i64
;   srli a0,a3,32
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 8
;   srli a4, a0, 8
;   slli a1, a4, 0x38
;   srli a3, a1, 0x38
;   or a4, a2, a3
;   slli a1, a4, 0x10
;   srli a2, a0, 0x10
;   slli a4, a2, 8
;   srli a2, a2, 8
;   slli a2, a2, 0x38
;   srli a5, a2, 0x38
;   or a2, a4, a5
;   slli a2, a2, 0x30
;   srli a4, a2, 0x30
;   or a1, a1, a4
;   slli a2, a1, 0x20
;   srli a5, a0, 0x20
;   slli a0, a5, 8
;   srli a3, a5, 8
;   slli a4, a3, 0x38
;   srli a1, a4, 0x38
;   or a3, a0, a1
;   slli a4, a3, 0x10
;   srli a0, a5, 0x10
;   slli a3, a0, 8
;   srli a5, a0, 8
;   slli a0, a5, 0x38
;   srli a5, a0, 0x38
;   or a5, a3, a5
;   slli a0, a5, 0x30
;   srli a3, a0, 0x30
;   or a4, a4, a3
;   slli a0, a4, 0x20
;   srli a3, a0, 0x20
;   or a4, a2, a3
;   mv a3, zero
;   addi a2, zero, 0x40
;   addi a0, zero, 1
;   slli a0, a0, 0x3f
;   addi a1, zero, 1
;   slli a1, a1, 0x38
;   blez a2, 0x34
;   and t5, a0, a4
;   beq zero, t5, 8
;   or a3, a3, a1
;   addi a2, a2, -1
;   srli a0, a0, 1
;   addi t5, zero, 8
;   rem t5, a2, t5
;   bnez t5, 0xc
;   srli a1, a1, 0xf
;   j -0x28
;   slli a1, a1, 1
;   j -0x30
;   srli a0, a3, 0x20
;   ret

function %bitrev_i64(i64) -> i64 {
block0(v0: i64):
    v1 = bitrev v0
    return v1
}

; VCode:
; block0:
;   slli a2,a0,8
;   srli a4,a0,8
;   slli a1,a4,56
;   srli a3,a1,56
;   or a4,a2,a3
;   slli a1,a4,16
;   srli a2,a0,16
;   slli a4,a2,8
;   srli a2,a2,8
;   slli a2,a2,56
;   srli a5,a2,56
;   or a2,a4,a5
;   slli a2,a2,48
;   srli a4,a2,48
;   or a1,a1,a4
;   slli a2,a1,32
;   srli a5,a0,32
;   slli a0,a5,8
;   srli a3,a5,8
;   slli a4,a3,56
;   srli a1,a4,56
;   or a3,a0,a1
;   slli a4,a3,16
;   srli a0,a5,16
;   slli a3,a0,8
;   srli a5,a0,8
;   slli a0,a5,56
;   srli a5,a0,56
;   or a5,a3,a5
;   slli a0,a5,48
;   srli a3,a0,48
;   or a4,a4,a3
;   slli a0,a4,32
;   srli a3,a0,32
;   or a4,a2,a3
;   brev8 a0,a4##tmp=a3 tmp2=a1 step=a2 ty=i64
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 8
;   srli a4, a0, 8
;   slli a1, a4, 0x38
;   srli a3, a1, 0x38
;   or a4, a2, a3
;   slli a1, a4, 0x10
;   srli a2, a0, 0x10
;   slli a4, a2, 8
;   srli a2, a2, 8
;   slli a2, a2, 0x38
;   srli a5, a2, 0x38
;   or a2, a4, a5
;   slli a2, a2, 0x30
;   srli a4, a2, 0x30
;   or a1, a1, a4
;   slli a2, a1, 0x20
;   srli a5, a0, 0x20
;   slli a0, a5, 8
;   srli a3, a5, 8
;   slli a4, a3, 0x38
;   srli a1, a4, 0x38
;   or a3, a0, a1
;   slli a4, a3, 0x10
;   srli a0, a5, 0x10
;   slli a3, a0, 8
;   srli a5, a0, 8
;   slli a0, a5, 0x38
;   srli a5, a0, 0x38
;   or a5, a3, a5
;   slli a0, a5, 0x30
;   srli a3, a0, 0x30
;   or a4, a4, a3
;   slli a0, a4, 0x20
;   srli a3, a0, 0x20
;   or a4, a2, a3
;   mv a0, zero
;   addi a2, zero, 0x40
;   addi a3, zero, 1
;   slli a3, a3, 0x3f
;   addi a1, zero, 1
;   slli a1, a1, 0x38
;   blez a2, 0x34
;   and t5, a3, a4
;   beq zero, t5, 8
;   or a0, a0, a1
;   addi a2, a2, -1
;   srli a3, a3, 1
;   addi t5, zero, 8
;   rem t5, a2, t5
;   bnez t5, 0xc
;   srli a1, a1, 0xf
;   j -0x28
;   slli a1, a1, 1
;   j -0x30
;   ret

function %bitrev_i128(i128) -> i128 {
block0(v0: i128):
    v1 = bitrev v0
    return v1
}

; VCode:
; block0:
;   slli a3,a1,8
;   srli a5,a1,8
;   slli a2,a5,56
;   srli a4,a2,56
;   or a5,a3,a4
;   slli a2,a5,16
;   srli a3,a1,16
;   slli a5,a3,8
;   srli a3,a3,8
;   slli a3,a3,56
;   srli a3,a3,56
;   or a3,a5,a3
;   slli a3,a3,48
;   srli a5,a3,48
;   or a2,a2,a5
;   slli a3,a2,32
;   srli a1,a1,32
;   slli a2,a1,8
;   srli a4,a1,8
;   slli a5,a4,56
;   srli a4,a5,56
;   or a4,a2,a4
;   slli a5,a4,16
;   srli a1,a1,16
;   slli a4,a1,8
;   srli a1,a1,8
;   slli a1,a1,56
;   srli a1,a1,56
;   or a1,a4,a1
;   slli a1,a1,48
;   srli a4,a1,48
;   or a5,a5,a4
;   slli a1,a5,32
;   srli a4,a1,32
;   or a5,a3,a4
;   brev8 a4,a5##tmp=a1 tmp2=a2 step=a3 ty=i64
;   slli a1,a0,8
;   srli a2,a0,8
;   slli a5,a2,56
;   srli a2,a5,56
;   or a2,a1,a2
;   slli a5,a2,16
;   srli a1,a0,16
;   slli a2,a1,8
;   srli a1,a1,8
;   slli a1,a1,56
;   srli a3,a1,56
;   or a1,a2,a3
;   slli a1,a1,48
;   srli a2,a1,48
;   or a5,a5,a2
;   slli a1,a5,32
;   srli a3,a0,32
;   slli a5,a3,8
;   srli a0,a3,8
;   slli a2,a0,56
;   srli a0,a2,56
;   or a0,a5,a0
;   slli a2,a0,16
;   srli a5,a3,16
;   slli a0,a5,8
;   srli a3,a5,8
;   slli a5,a3,56
;   srli a3,a5,56
;   or a3,a0,a3
;   slli a5,a3,48
;   srli a0,a5,48
;   or a2,a2,a0
;   slli a5,a2,32
;   srli a0,a5,32
;   or a3,a1,a0
;   brev8 a1,a3##tmp=a2 tmp2=a5 step=a0 ty=i64
;   mv a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a3, a1, 8
;   srli a5, a1, 8
;   slli a2, a5, 0x38
;   srli a4, a2, 0x38
;   or a5, a3, a4
;   slli a2, a5, 0x10
;   srli a3, a1, 0x10
;   slli a5, a3, 8
;   srli a3, a3, 8
;   slli a3, a3, 0x38
;   srli a3, a3, 0x38
;   or a3, a5, a3
;   slli a3, a3, 0x30
;   srli a5, a3, 0x30
;   or a2, a2, a5
;   slli a3, a2, 0x20
;   srli a1, a1, 0x20
;   slli a2, a1, 8
;   srli a4, a1, 8
;   slli a5, a4, 0x38
;   srli a4, a5, 0x38
;   or a4, a2, a4
;   slli a5, a4, 0x10
;   srli a1, a1, 0x10
;   slli a4, a1, 8
;   srli a1, a1, 8
;   slli a1, a1, 0x38
;   srli a1, a1, 0x38
;   or a1, a4, a1
;   slli a1, a1, 0x30
;   srli a4, a1, 0x30
;   or a5, a5, a4
;   slli a1, a5, 0x20
;   srli a4, a1, 0x20
;   or a5, a3, a4
;   mv a4, zero
;   addi a3, zero, 0x40
;   addi a1, zero, 1
;   slli a1, a1, 0x3f
;   addi a2, zero, 1
;   slli a2, a2, 0x38
;   blez a3, 0x34
;   and t5, a1, a5
;   beq zero, t5, 8
;   or a4, a4, a2
;   addi a3, a3, -1
;   srli a1, a1, 1
;   addi t5, zero, 8
;   rem t5, a3, t5
;   bnez t5, 0xc
;   srli a2, a2, 0xf
;   j -0x28
;   slli a2, a2, 1
;   j -0x30
;   slli a1, a0, 8
;   srli a2, a0, 8
;   slli a5, a2, 0x38
;   srli a2, a5, 0x38
;   or a2, a1, a2
;   slli a5, a2, 0x10
;   srli a1, a0, 0x10
;   slli a2, a1, 8
;   srli a1, a1, 8
;   slli a1, a1, 0x38
;   srli a3, a1, 0x38
;   or a1, a2, a3
;   slli a1, a1, 0x30
;   srli a2, a1, 0x30
;   or a5, a5, a2
;   slli a1, a5, 0x20
;   srli a3, a0, 0x20
;   slli a5, a3, 8
;   srli a0, a3, 8
;   slli a2, a0, 0x38
;   srli a0, a2, 0x38
;   or a0, a5, a0
;   slli a2, a0, 0x10
;   srli a5, a3, 0x10
;   slli a0, a5, 8
;   srli a3, a5, 8
;   slli a5, a3, 0x38
;   srli a3, a5, 0x38
;   or a3, a0, a3
;   slli a5, a3, 0x30
;   srli a0, a5, 0x30
;   or a2, a2, a0
;   slli a5, a2, 0x20
;   srli a0, a5, 0x20
;   or a3, a1, a0
;   mv a1, zero
;   addi a0, zero, 0x40
;   addi a2, zero, 1
;   slli a2, a2, 0x3f
;   addi a5, zero, 1
;   slli a5, a5, 0x38
;   blez a0, 0x34
;   and t5, a2, a3
;   beq zero, t5, 8
;   or a1, a1, a5
;   addi a0, a0, -1
;   srli a2, a2, 1
;   addi t5, zero, 8
;   rem t5, a0, t5
;   bnez t5, 0xc
;   srli a5, a5, 0xf
;   j -0x28
;   slli a5, a5, 1
;   j -0x30
;   mv a0, a4
;   ret

