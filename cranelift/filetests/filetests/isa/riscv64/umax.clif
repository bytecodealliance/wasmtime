test compile precise-output
set unwind_info=false
target riscv64 has_v

function %umax_i8(i8, i8) -> i8{
block0(v0: i8, v1: i8):
    v2 = umax v0, v1
    return v2
}

; VCode:
; block0:
;   andi a2,a0,255
;   andi a1,a1,255
;   select a0,a2,a1##condition=(a2 ugt a1)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a2, a0, 0xff
;   andi a1, a1, 0xff
;   mv a0, a2
;   bltu a1, a2, 8
;   mv a0, a1
;   ret

function %umax_i16(i16, i16) -> i16{
block0(v0: i16, v1: i16):
    v2 = umax v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srli a2,a0,48
;   slli a0,a1,48
;   srli a1,a0,48
;   select a0,a2,a1##condition=(a2 ugt a1)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srli a2, a0, 0x30
;   slli a0, a1, 0x30
;   srli a1, a0, 0x30
;   mv a0, a2
;   bltu a1, a2, 8
;   mv a0, a1
;   ret

function %umax_i32(i32, i32) -> i32{
block0(v0: i32, v1: i32):
    v2 = umax v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,32
;   srli a2,a0,32
;   slli a0,a1,32
;   srli a1,a0,32
;   select a0,a2,a1##condition=(a2 ugt a1)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x20
;   srli a2, a0, 0x20
;   slli a0, a1, 0x20
;   srli a1, a0, 0x20
;   mv a0, a2
;   bltu a1, a2, 8
;   mv a0, a1
;   ret

function %umax_i64(i64, i64) -> i64{
block0(v0: i64, v1: i64):
    v2 = umax v0, v1
    return v2
}

; VCode:
; block0:
;   mv a2,a0
;   select a0,a2,a1##condition=(a2 ugt a1)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a2, a0
;   mv a0, a2
;   bltu a1, a2, 8
;   mv a0, a1
;   ret

function %umax_i128(i128, i128) -> i128{
block0(v0: i128, v1: i128):
    v2 = umax v0, v1
    return v2
}

; VCode:
; block0:
;   sltu a4,a3,a1
;   sltu t2,a2,a0
;   xor a5,a3,a1
;   mv a6,a1
;   select a4,t2,a4##condition=(a5 eq zero)
;   mv a5,a0
;   select [a0,a1],[a5,a6],[a2,a3]##condition=(a4 ne zero)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sltu a4, a3, a1
;   sltu t2, a2, a0
;   xor a5, a3, a1
;   mv a6, a1
;   bnez a5, 8
;   mv a4, t2
;   mv a5, a0
;   mv a0, a5
;   mv a1, a6
;   bnez a4, 0xc
;   mv a0, a2
;   mv a1, a3
;   ret

