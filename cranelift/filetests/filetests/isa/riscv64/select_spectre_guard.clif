test compile precise-output
set unwind_info=false
target riscv64

function %f(i8, i8, i8) -> i8 {
block0(v0: i8, v1: i8, v2: i8):
  v3 = iconst.i8 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i8 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   mv a3,a0
;   li a0,42
;   andi a4,a3,255
;   andi a0,a0,255
;   eq a3,a4,a0##ty=i8
;   sub a3,zero,a3
;   and a4,a1,a3
;   not a0,a3
;   and a2,a2,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a3, a0
;   addi a0, zero, 0x2a
;   andi a4, a3, 0xff
;   andi a0, a0, 0xff
;   bne a4, a0, 0xc
;   addi a3, zero, 1
;   j 8
;   mv a3, zero
;   neg a3, a3
;   and a4, a1, a3
;   not a0, a3
;   and a2, a2, a0
;   or a0, a4, a2
;   ret

function %f(i8, i16, i16) -> i16 {
block0(v0: i8, v1: i16, v2: i16):
  v3 = iconst.i8 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i16 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   mv a3,a0
;   li a0,42
;   andi a4,a3,255
;   andi a0,a0,255
;   eq a3,a4,a0##ty=i8
;   sub a3,zero,a3
;   and a4,a1,a3
;   not a0,a3
;   and a2,a2,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a3, a0
;   addi a0, zero, 0x2a
;   andi a4, a3, 0xff
;   andi a0, a0, 0xff
;   bne a4, a0, 0xc
;   addi a3, zero, 1
;   j 8
;   mv a3, zero
;   neg a3, a3
;   and a4, a1, a3
;   not a0, a3
;   and a2, a2, a0
;   or a0, a4, a2
;   ret

function %f(i8, i32, i32) -> i32 {
block0(v0: i8, v1: i32, v2: i32):
  v3 = iconst.i8 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i32 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   mv a3,a0
;   li a0,42
;   andi a4,a3,255
;   andi a0,a0,255
;   eq a3,a4,a0##ty=i8
;   sub a3,zero,a3
;   and a4,a1,a3
;   not a0,a3
;   and a2,a2,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a3, a0
;   addi a0, zero, 0x2a
;   andi a4, a3, 0xff
;   andi a0, a0, 0xff
;   bne a4, a0, 0xc
;   addi a3, zero, 1
;   j 8
;   mv a3, zero
;   neg a3, a3
;   and a4, a1, a3
;   not a0, a3
;   and a2, a2, a0
;   or a0, a4, a2
;   ret

function %f(i8, i64, i64) -> i64 {
block0(v0: i8, v1: i64, v2: i64):
  v3 = iconst.i8 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i64 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   mv a3,a0
;   li a0,42
;   andi a4,a3,255
;   andi a0,a0,255
;   eq a3,a4,a0##ty=i8
;   sub a3,zero,a3
;   and a4,a1,a3
;   not a0,a3
;   and a2,a2,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a3, a0
;   addi a0, zero, 0x2a
;   andi a4, a3, 0xff
;   andi a0, a0, 0xff
;   bne a4, a0, 0xc
;   addi a3, zero, 1
;   j 8
;   mv a3, zero
;   neg a3, a3
;   and a4, a1, a3
;   not a0, a3
;   and a2, a2, a0
;   or a0, a4, a2
;   ret

function %f(i8, i128, i128) -> i128 {
block0(v0: i8, v1: i128, v2: i128):
  v3 = iconst.i8 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i128 v4, v1, v2
  return v5
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   sd s10,-8(sp)
;   add sp,-16
; block0:
;   mv a5,a0
;   li a0,42
;   andi a5,a5,255
;   andi a0,a0,255
;   eq a5,a5,a0##ty=i8
;   sub a5,zero,a5
;   and a0,a1,a5
;   and a2,a2,a5
;   not s10,a5
;   not a1,a5
;   and a3,a3,s10
;   and a4,a4,a1
;   or a0,a0,a3
;   or a1,a2,a4
;   add sp,+16
;   ld s10,-8(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   sd s10, -8(sp)
;   addi sp, sp, -0x10
; block1: ; offset 0x18
;   mv a5, a0
;   addi a0, zero, 0x2a
;   andi a5, a5, 0xff
;   andi a0, a0, 0xff
;   bne a5, a0, 0xc
;   addi a5, zero, 1
;   j 8
;   mv a5, zero
;   neg a5, a5
;   and a0, a1, a5
;   and a2, a2, a5
;   not s10, a5
;   not a1, a5
;   and a3, a3, s10
;   and a4, a4, a1
;   or a0, a0, a3
;   or a1, a2, a4
;   addi sp, sp, 0x10
;   ld s10, -8(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %f(i16, i8, i8) -> i8 {
block0(v0: i16, v1: i8, v2: i8):
  v3 = iconst.i16 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i8 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,48
;   srli a0,a4,48
;   slli a3,a3,48
;   srli a4,a3,48
;   eq a0,a0,a4##ty=i16
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x30
;   srli a0, a4, 0x30
;   slli a3, a3, 0x30
;   srli a4, a3, 0x30
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i16, i16, i16) -> i16 {
block0(v0: i16, v1: i16, v2: i16):
  v3 = iconst.i16 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i16 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,48
;   srli a0,a4,48
;   slli a3,a3,48
;   srli a4,a3,48
;   eq a0,a0,a4##ty=i16
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x30
;   srli a0, a4, 0x30
;   slli a3, a3, 0x30
;   srli a4, a3, 0x30
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i16, i32, i32) -> i32 {
block0(v0: i16, v1: i32, v2: i32):
  v3 = iconst.i16 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i32 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,48
;   srli a0,a4,48
;   slli a3,a3,48
;   srli a4,a3,48
;   eq a0,a0,a4##ty=i16
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x30
;   srli a0, a4, 0x30
;   slli a3, a3, 0x30
;   srli a4, a3, 0x30
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i16, i64, i64) -> i64 {
block0(v0: i16, v1: i64, v2: i64):
  v3 = iconst.i16 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i64 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,48
;   srli a0,a4,48
;   slli a3,a3,48
;   srli a4,a3,48
;   eq a0,a0,a4##ty=i16
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x30
;   srli a0, a4, 0x30
;   slli a3, a3, 0x30
;   srli a4, a3, 0x30
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i16, i128, i128) -> i128 {
block0(v0: i16, v1: i128, v2: i128):
  v3 = iconst.i16 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i128 v4, v1, v2
  return v5
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   sd s6,-8(sp)
;   add sp,-16
; block0:
;   li a5,42
;   slli a0,a0,48
;   srli a0,a0,48
;   slli a5,a5,48
;   srli a5,a5,48
;   eq a0,a0,a5##ty=i16
;   sub s6,zero,a0
;   and a0,a1,s6
;   and a5,a2,s6
;   not a2,s6
;   not a1,s6
;   and a2,a3,a2
;   and a1,a4,a1
;   or a0,a0,a2
;   or a1,a5,a1
;   add sp,+16
;   ld s6,-8(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   sd s6, -8(sp)
;   addi sp, sp, -0x10
; block1: ; offset 0x18
;   addi a5, zero, 0x2a
;   slli a0, a0, 0x30
;   srli a0, a0, 0x30
;   slli a5, a5, 0x30
;   srli a5, a5, 0x30
;   bne a0, a5, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg s6, a0
;   and a0, a1, s6
;   and a5, a2, s6
;   not a2, s6
;   not a1, s6
;   and a2, a3, a2
;   and a1, a4, a1
;   or a0, a0, a2
;   or a1, a5, a1
;   addi sp, sp, 0x10
;   ld s6, -8(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %f(i32, i8, i8) -> i8 {
block0(v0: i32, v1: i8, v2: i8):
  v3 = iconst.i32 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i8 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,32
;   srli a0,a4,32
;   slli a3,a3,32
;   srli a4,a3,32
;   eq a0,a0,a4##ty=i32
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x20
;   srli a0, a4, 0x20
;   slli a3, a3, 0x20
;   srli a4, a3, 0x20
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i32, i16, i16) -> i16 {
block0(v0: i32, v1: i16, v2: i16):
  v3 = iconst.i32 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i16 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,32
;   srli a0,a4,32
;   slli a3,a3,32
;   srli a4,a3,32
;   eq a0,a0,a4##ty=i32
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x20
;   srli a0, a4, 0x20
;   slli a3, a3, 0x20
;   srli a4, a3, 0x20
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32):
  v3 = iconst.i32 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i32 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,32
;   srli a0,a4,32
;   slli a3,a3,32
;   srli a4,a3,32
;   eq a0,a0,a4##ty=i32
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x20
;   srli a0, a4, 0x20
;   slli a3, a3, 0x20
;   srli a4, a3, 0x20
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i32, i64, i64) -> i64 {
block0(v0: i32, v1: i64, v2: i64):
  v3 = iconst.i32 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i64 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a3,42
;   slli a4,a0,32
;   srli a0,a4,32
;   slli a3,a3,32
;   srli a4,a3,32
;   eq a0,a0,a4##ty=i32
;   sub a4,zero,a0
;   and a0,a1,a4
;   not a3,a4
;   and a4,a2,a3
;   or a0,a0,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 0x2a
;   slli a4, a0, 0x20
;   srli a0, a4, 0x20
;   slli a3, a3, 0x20
;   srli a4, a3, 0x20
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a4, a0
;   and a0, a1, a4
;   not a3, a4
;   and a4, a2, a3
;   or a0, a0, a4
;   ret

function %f(i32, i128, i128) -> i128 {
block0(v0: i32, v1: i128, v2: i128):
  v3 = iconst.i32 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i128 v4, v1, v2
  return v5
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   sd s6,-8(sp)
;   add sp,-16
; block0:
;   li a5,42
;   slli a0,a0,32
;   srli a0,a0,32
;   slli a5,a5,32
;   srli a5,a5,32
;   eq a0,a0,a5##ty=i32
;   sub s6,zero,a0
;   and a0,a1,s6
;   and a5,a2,s6
;   not a2,s6
;   not a1,s6
;   and a2,a3,a2
;   and a1,a4,a1
;   or a0,a0,a2
;   or a1,a5,a1
;   add sp,+16
;   ld s6,-8(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   sd s6, -8(sp)
;   addi sp, sp, -0x10
; block1: ; offset 0x18
;   addi a5, zero, 0x2a
;   slli a0, a0, 0x20
;   srli a0, a0, 0x20
;   slli a5, a5, 0x20
;   srli a5, a5, 0x20
;   bne a0, a5, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg s6, a0
;   and a0, a1, s6
;   and a5, a2, s6
;   not a2, s6
;   not a1, s6
;   and a2, a3, a2
;   and a1, a4, a1
;   or a0, a0, a2
;   or a1, a5, a1
;   addi sp, sp, 0x10
;   ld s6, -8(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %f(i64, i8, i8) -> i8 {
block0(v0: i64, v1: i8, v2: i8):
  v3 = iconst.i64 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i8 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a4,42
;   eq a4,a0,a4##ty=i64
;   sub a0,zero,a4
;   and a3,a1,a0
;   not a4,a0
;   and a0,a2,a4
;   or a0,a3,a0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   bne a0, a4, 0xc
;   addi a4, zero, 1
;   j 8
;   mv a4, zero
;   neg a0, a4
;   and a3, a1, a0
;   not a4, a0
;   and a0, a2, a4
;   or a0, a3, a0
;   ret

function %f(i64, i16, i16) -> i16 {
block0(v0: i64, v1: i16, v2: i16):
  v3 = iconst.i64 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i16 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a4,42
;   eq a4,a0,a4##ty=i64
;   sub a0,zero,a4
;   and a3,a1,a0
;   not a4,a0
;   and a0,a2,a4
;   or a0,a3,a0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   bne a0, a4, 0xc
;   addi a4, zero, 1
;   j 8
;   mv a4, zero
;   neg a0, a4
;   and a3, a1, a0
;   not a4, a0
;   and a0, a2, a4
;   or a0, a3, a0
;   ret

function %f(i64, i32, i32) -> i32 {
block0(v0: i64, v1: i32, v2: i32):
  v3 = iconst.i64 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i32 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a4,42
;   eq a4,a0,a4##ty=i64
;   sub a0,zero,a4
;   and a3,a1,a0
;   not a4,a0
;   and a0,a2,a4
;   or a0,a3,a0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   bne a0, a4, 0xc
;   addi a4, zero, 1
;   j 8
;   mv a4, zero
;   neg a0, a4
;   and a3, a1, a0
;   not a4, a0
;   and a0, a2, a4
;   or a0, a3, a0
;   ret

function %f(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i64 v4, v1, v2
  return v5
}

; VCode:
; block0:
;   li a4,42
;   eq a4,a0,a4##ty=i64
;   sub a0,zero,a4
;   and a3,a1,a0
;   not a4,a0
;   and a0,a2,a4
;   or a0,a3,a0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   bne a0, a4, 0xc
;   addi a4, zero, 1
;   j 8
;   mv a4, zero
;   neg a0, a4
;   and a3, a1, a0
;   not a4, a0
;   and a0, a2, a4
;   or a0, a3, a0
;   ret

function %f(i64, i128, i128) -> i128 {
block0(v0: i64, v1: i128, v2: i128):
  v3 = iconst.i64 42
  v4 = icmp eq v0, v3
  v5 = select_spectre_guard.i128 v4, v1, v2
  return v5
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   sd s11,-8(sp)
;   add sp,-16
; block0:
;   mv s11,a1
;   li a5,42
;   eq a5,a0,a5##ty=i64
;   sub a1,zero,a5
;   mv a5,s11
;   and a5,a5,a1
;   and a2,a2,a1
;   not a0,a1
;   not a1,a1
;   and a0,a3,a0
;   and a3,a4,a1
;   or a0,a5,a0
;   or a1,a2,a3
;   add sp,+16
;   ld s11,-8(sp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   sd s11, -8(sp)
;   addi sp, sp, -0x10
; block1: ; offset 0x18
;   mv s11, a1
;   addi a5, zero, 0x2a
;   bne a0, a5, 0xc
;   addi a5, zero, 1
;   j 8
;   mv a5, zero
;   neg a1, a5
;   mv a5, s11
;   and a5, a5, a1
;   and a2, a2, a1
;   not a0, a1
;   not a1, a1
;   and a0, a3, a0
;   and a3, a4, a1
;   or a0, a5, a0
;   or a1, a2, a3
;   addi sp, sp, 0x10
;   ld s11, -8(sp)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %f(i128, i8, i8) -> i8 {
block0(v0: i128, v1: i8, v2: i8):
  v3 = iconst.i64 42
  v4 = uextend.i128 v3
  v5 = icmp eq v0, v4
  v6 = select_spectre_guard.i8 v5, v1, v2
  return v6
}

; VCode:
; block0:
;   li a4,42
;   li a5,0
;   eq a0,[a0,a1],[a4,a5]##ty=i128
;   sub a5,zero,a0
;   and a4,a2,a5
;   not a0,a5
;   and a2,a3,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   mv a5, zero
;   bne a1, a5, 0x10
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a5, a0
;   and a4, a2, a5
;   not a0, a5
;   and a2, a3, a0
;   or a0, a4, a2
;   ret

function %f(i128, i16, i16) -> i16 {
block0(v0: i128, v1: i16, v2: i16):
  v3 = iconst.i64 42
  v4 = uextend.i128 v3
  v5 = icmp eq v0, v4
  v6 = select_spectre_guard.i16 v5, v1, v2
  return v6
}

; VCode:
; block0:
;   li a4,42
;   li a5,0
;   eq a0,[a0,a1],[a4,a5]##ty=i128
;   sub a5,zero,a0
;   and a4,a2,a5
;   not a0,a5
;   and a2,a3,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   mv a5, zero
;   bne a1, a5, 0x10
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a5, a0
;   and a4, a2, a5
;   not a0, a5
;   and a2, a3, a0
;   or a0, a4, a2
;   ret

function %f(i128, i32, i32) -> i32 {
block0(v0: i128, v1: i32, v2: i32):
  v3 = iconst.i64 42
  v4 = uextend.i128 v3
  v5 = icmp eq v0, v4
  v6 = select_spectre_guard.i32 v5, v1, v2
  return v6
}

; VCode:
; block0:
;   li a4,42
;   li a5,0
;   eq a0,[a0,a1],[a4,a5]##ty=i128
;   sub a5,zero,a0
;   and a4,a2,a5
;   not a0,a5
;   and a2,a3,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   mv a5, zero
;   bne a1, a5, 0x10
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a5, a0
;   and a4, a2, a5
;   not a0, a5
;   and a2, a3, a0
;   or a0, a4, a2
;   ret

function %f(i128, i64, i64) -> i64 {
block0(v0: i128, v1: i64, v2: i64):
  v3 = iconst.i64 42
  v4 = uextend.i128 v3
  v5 = icmp eq v0, v4
  v6 = select_spectre_guard.i64 v5, v1, v2
  return v6
}

; VCode:
; block0:
;   li a4,42
;   li a5,0
;   eq a0,[a0,a1],[a4,a5]##ty=i128
;   sub a5,zero,a0
;   and a4,a2,a5
;   not a0,a5
;   and a2,a3,a0
;   or a0,a4,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a4, zero, 0x2a
;   mv a5, zero
;   bne a1, a5, 0x10
;   bne a0, a4, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a5, a0
;   and a4, a2, a5
;   not a0, a5
;   and a2, a3, a0
;   or a0, a4, a2
;   ret

function %f(i128, i128, i128) -> i128 {
block0(v0: i128, v1: i128, v2: i128):
  v3 = iconst.i64 42
  v4 = uextend.i128 v3
  v5 = icmp eq v0, v4
  v6 = select_spectre_guard.i128 v5, v1, v2
  return v6
}

; VCode:
; block0:
;   li t0,42
;   li t1,0
;   eq a0,[a0,a1],[t0,t1]##ty=i128
;   sub a1,zero,a0
;   and a0,a2,a1
;   and a2,a3,a1
;   not a3,a1
;   not a1,a1
;   and a3,a4,a3
;   and a4,a5,a1
;   or a0,a0,a3
;   or a1,a2,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi t0, zero, 0x2a
;   mv t1, zero
;   bne a1, t1, 0x10
;   bne a0, t0, 0xc
;   addi a0, zero, 1
;   j 8
;   mv a0, zero
;   neg a1, a0
;   and a0, a2, a1
;   and a2, a3, a1
;   not a3, a1
;   not a1, a1
;   and a3, a4, a3
;   and a4, a5, a1
;   or a0, a0, a3
;   or a1, a2, a4
;   ret

