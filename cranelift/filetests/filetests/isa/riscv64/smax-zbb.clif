test compile precise-output
set unwind_info=false
target riscv64 has_zbb

function %smax_i8(i8, i8) -> i8{
block0(v0: i8, v1: i8):
    v2 = smax v0, v1
    return v2
}

; VCode:
; block0:
;   sext.b a3,a0
;   sext.b a5,a1
;   max a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x93, 0x16, 0x45, 0x60
;   .byte 0x93, 0x97, 0x45, 0x60
;   .byte 0x33, 0xe5, 0xf6, 0x0a
;   ret

function %smax_i16(i16, i16) -> i16{
block0(v0: i16, v1: i16):
    v2 = smax v0, v1
    return v2
}

; VCode:
; block0:
;   sext.h a3,a0
;   sext.h a5,a1
;   max a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x93, 0x16, 0x55, 0x60
;   .byte 0x93, 0x97, 0x55, 0x60
;   .byte 0x33, 0xe5, 0xf6, 0x0a
;   ret

function %smax_i32(i32, i32) -> i32{
block0(v0: i32, v1: i32):
    v2 = smax v0, v1
    return v2
}

; VCode:
; block0:
;   sext.w a3,a0
;   sext.w a5,a1
;   max a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sext.w a3, a0
;   sext.w a5, a1
;   .byte 0x33, 0xe5, 0xf6, 0x0a
;   ret

function %smax_i64(i64, i64) -> i64{
block0(v0: i64, v1: i64):
    v2 = smax v0, v1
    return v2
}

; VCode:
; block0:
;   max a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x33, 0x65, 0xb5, 0x0a
;   ret

function %smax_i128(i128, i128) -> i128{
block0(v0: i128, v1: i128):
    v2 = smax v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s7,8(sp)
;   sd s9,0(sp)
; block0:
;   slt a5,a3,a1
;   sltu a4,a2,a0
;   mv s7,a0
;   xor a0,a3,a1
;   mv s9,a1
;   select a5,a4,a5##condition=(a0 eq zero)
;   mv a4,s7
;   select [a0,a1],[a4,s9],[a2,a3]##condition=(a5 ne zero)
;   ld s7,8(sp)
;   ld s9,0(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s7, 8(sp)
;   sd s9, 0(sp)
; block1: ; offset 0x1c
;   slt a5, a3, a1
;   sltu a4, a2, a0
;   mv s7, a0
;   xor a0, a3, a1
;   mv s9, a1
;   bnez a0, 8
;   mv a5, a4
;   mv a4, s7
;   beqz a5, 0x10
;   mv a0, a4
;   mv a1, s9
;   j 0xc
;   mv a0, a2
;   mv a1, a3
;   ld s7, 8(sp)
;   ld s9, 0(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

