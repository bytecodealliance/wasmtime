test compile precise-output

target riscv64

;;;; Test passing `i64`s ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %callee_i64(i64) -> i64 tail {
block0(v0: i64):
    v1 = iadd_imm.i64 v0, 10
    return v1
}

; VCode:
; block0:
;   addi s1,s1,10
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi s1, s1, 0xa
;   ret

function %call_i64(i64) -> i64 tail {
    fn0 = %callee_i64(i64) -> i64 tail

block0(v0: i64):
    return_call fn0(v0)
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   load_sym t2,%callee_i64+0
;   return_call_ind t2 old_stack_arg_size:0 new_stack_arg_size:0 s1=s1
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   auipc t2, 0
;   ld t2, 0xc(t2)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %callee_i64 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   addi sp, s0, 0x10
;   ori s0, t6, 0
;   jr t2

;;;; Test colocated tail calls ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %colocated_i64(i64) -> i64 tail {
    fn0 = colocated %callee_i64(i64) -> i64 tail

block0(v0: i64):
    return_call fn0(v0)
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   return_call TestCase(%callee_i64) old_stack_arg_size:0 new_stack_arg_size:0 s1=s1
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   addi sp, s0, 0x10
;   ori s0, t6, 0
;   auipc t6, 0 ; reloc_external RiscvCall %callee_i64 0
;   jr t6

;;;; Test passing `f64`s ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %callee_f64(f64) -> f64 tail {
block0(v0: f64):
    v1 = f64const 0x10.0
    v2 = fadd.f64 v0, v1
    return v2
}

; VCode:
; block0:
;   auipc a1,0; ld a1,12(a1); j 12; .8byte 0x4030000000000000
;   fmv.d.x ft4,a1
;   fadd.d ft0,ft0,ft4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   auipc a1, 0
;   ld a1, 0xc(a1)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x30, 0x40
;   fmv.d.x ft4, a1
;   fadd.d ft0, ft0, ft4
;   ret

function %call_f64(f64) -> f64 tail {
    fn0 = %callee_f64(f64) -> f64 tail

block0(v0: f64):
    return_call fn0(v0)
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   load_sym t2,%callee_f64+0
;   return_call_ind t2 old_stack_arg_size:0 new_stack_arg_size:0 ft0=ft0
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   auipc t2, 0
;   ld t2, 0xc(t2)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %callee_f64 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   addi sp, s0, 0x10
;   ori s0, t6, 0
;   jr t2

;;;; Test passing `i8`s ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %callee_i8(i8) -> i8 tail {
block0(v0: i8):
    v1 = iconst.i8 0
    v2 = icmp eq v0, v1
    return v2
}

; VCode:
; block0:
;   li a2,0
;   andi a0,s1,255
;   andi a2,a2,255
;   eq s1,a0,a2##ty=i8
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a2, zero
;   andi a0, s1, 0xff
;   andi a2, a2, 0xff
;   bne a0, a2, 0xc
;   addi s1, zero, 1
;   j 8
;   mv s1, zero
;   ret

function %call_i8(i8) -> i8 tail {
    fn0 = %callee_i8(i8) -> i8 tail

block0(v0: i8):
    return_call fn0(v0)
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   load_sym t2,%callee_i8+0
;   return_call_ind t2 old_stack_arg_size:0 new_stack_arg_size:0 s1=s1
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   auipc t2, 0
;   ld t2, 0xc(t2)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %callee_i8 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   addi sp, s0, 0x10
;   ori s0, t6, 0
;   jr t2

;;;; Test passing many arguments on stack ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %tail_callee_stack_args(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64, v6: i64, v7: i64, v8: i64, v9: i64, v10: i64, v11: i64, v12: i64, v13: i64, v14: i64, v15: i64, v16: i64, v17: i64, v18: i64, v19: i64, v20: i64, v21: i64, v22: i64, v23: i64, v24: i64, v25: i64):
    return v25
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   ld a6,16(fp)
;   ld t3,24(fp)
;   ld t0,32(fp)
;   ld t2,40(fp)
;   ld s1,48(fp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   add sp, sp, #48 ; ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   ld a6, 0x10(s0)
;   ld t3, 0x18(s0)
;   ld t0, 0x20(s0)
;   ld t2, 0x28(s0)
;   ld s1, 0x30(s0)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   addi sp, sp, 0x30
;   ret

function %tail_caller_stack_args() -> i64 tail {
    fn0 = %tail_callee_stack_args(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail

block0:
    v0 = iconst.i64 10
    v1 = iconst.i64 15
    v2 = iconst.i64 20
    v3 = iconst.i64 25
    v4 = iconst.i64 30
    v5 = iconst.i64 35
    v6 = iconst.i64 40
    v7 = iconst.i64 45
    v8 = iconst.i64 50
    v9 = iconst.i64 55
    v10 = iconst.i64 60
    v11 = iconst.i64 65
    v12 = iconst.i64 70
    v13 = iconst.i64 75
    v14 = iconst.i64 80
    v15 = iconst.i64 85
    v16 = iconst.i64 90
    v17 = iconst.i64 95
    v18 = iconst.i64 100
    v19 = iconst.i64 105
    v20 = iconst.i64 110
    v21 = iconst.i64 115
    v22 = iconst.i64 120
    v23 = iconst.i64 125
    v24 = iconst.i64 130
    v25 = iconst.i64 135
    return_call fn0(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25)
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   add sp,-16
; block0:
;   li s1,10
;   sd s1,8(nominal_sp)
;   li a0,15
;   sd a0,0(nominal_sp)
;   li a1,20
;   li a2,25
;   li a3,30
;   li a4,35
;   li a5,40
;   li a6,45
;   li a7,50
;   li s2,55
;   li s3,60
;   li s4,65
;   li s5,70
;   li s6,75
;   li s7,80
;   li s8,85
;   li s9,90
;   li s10,95
;   li s11,100
;   li t3,105
;   li t4,110
;   li t0,115
;   li t1,120
;   li t2,125
;   li s1,130
;   li a0,135
;   add sp,-48
;   virtual_sp_offset_adj +48
;   sd t0,0(sp)
;   sd t1,8(sp)
;   sd t2,16(sp)
;   sd s1,24(sp)
;   sd a0,32(sp)
;   load_sym t0,%tail_callee_stack_args+0
;   ld a0,0(nominal_sp)
;   ld s1,8(nominal_sp)
;   return_call_ind t0 old_stack_arg_size:0 new_stack_arg_size:48 s1=s1 a0=a0 a1=a1 a2=a2 a3=a3 a4=a4 a5=a5 a6=a6 a7=a7 s2=s2 s3=s3 s4=s4 s5=s5 s6=s6 s7=s7 s8=s8 s9=s9 s10=s10 s11=s11 t3=t3 t4=t4
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
;   addi sp, sp, -0x10
; block1: ; offset 0x14
;   addi s1, zero, 0xa
;   sd s1, 8(sp)
;   addi a0, zero, 0xf
;   sd a0, 0(sp)
;   addi a1, zero, 0x14
;   addi a2, zero, 0x19
;   addi a3, zero, 0x1e
;   addi a4, zero, 0x23
;   addi a5, zero, 0x28
;   addi a6, zero, 0x2d
;   addi a7, zero, 0x32
;   addi s2, zero, 0x37
;   addi s3, zero, 0x3c
;   addi s4, zero, 0x41
;   addi s5, zero, 0x46
;   addi s6, zero, 0x4b
;   addi s7, zero, 0x50
;   addi s8, zero, 0x55
;   addi s9, zero, 0x5a
;   addi s10, zero, 0x5f
;   addi s11, zero, 0x64
;   addi t3, zero, 0x69
;   addi t4, zero, 0x6e
;   addi t0, zero, 0x73
;   addi t1, zero, 0x78
;   addi t2, zero, 0x7d
;   addi s1, zero, 0x82
;   addi a0, zero, 0x87
;   addi sp, sp, -0x30
;   sd t0, 0(sp)
;   sd t1, 8(sp)
;   sd t2, 0x10(sp)
;   sd s1, 0x18(sp)
;   sd a0, 0x20(sp)
;   auipc t0, 0
;   ld t0, 0xc(t0)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %tail_callee_stack_args 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   ld a0, 0x30(sp)
;   ld s1, 0x38(sp)
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   ld t5, 0x28(sp)
;   sd t5, 8(s0)
;   ld t5, 0x20(sp)
;   sd t5, 0(s0)
;   ld t5, 0x18(sp)
;   sd t5, -8(s0)
;   ld t5, 0x10(sp)
;   sd t5, -0x10(s0)
;   ld t5, 8(sp)
;   sd t5, -0x18(s0)
;   ld t5, 0(sp)
;   sd t5, -0x20(s0)
;   addi sp, s0, -0x20
;   ori s0, t6, 0
;   jr t0

;;;; Test diff blocks with diff return calls with diff # of stack args ;;;;;;;;;

function %different_callee1(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64, v6: i64, v7: i64, v8: i64, v9: i64, v10: i64, v11: i64, v12: i64, v13: i64, v14: i64, v15: i64, v16: i64, v17: i64, v18: i64, v19: i64, v20: i64, v21: i64, v22: i64, v23: i64, v24: i64, v25: i64):
    return v25
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   ld a6,16(fp)
;   ld t3,24(fp)
;   ld t0,32(fp)
;   ld t2,40(fp)
;   ld s1,48(fp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   add sp, sp, #48 ; ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   ld a6, 0x10(s0)
;   ld t3, 0x18(s0)
;   ld t0, 0x20(s0)
;   ld t2, 0x28(s0)
;   ld s1, 0x30(s0)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   addi sp, sp, 0x30
;   ret

function %different_callee2(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64, v6: i64, v7: i64, v8: i64, v9: i64, v10: i64, v11: i64, v12: i64, v13: i64, v14: i64, v15: i64, v16: i64, v17: i64, v18: i64, v19: i64, v20: i64, v21: i64, v22: i64, v23: i64, v24: i64, v25: i64, v26: i64):
    return v26
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
; block0:
;   ld a6,16(fp)
;   ld t3,24(fp)
;   ld t0,32(fp)
;   ld t2,40(fp)
;   ld a1,48(fp)
;   ld s1,56(fp)
;   ld ra,8(sp)
;   ld fp,0(sp)
;   add sp,+16
;   add sp, sp, #48 ; ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
; block1: ; offset 0x10
;   ld a6, 0x10(s0)
;   ld t3, 0x18(s0)
;   ld t0, 0x20(s0)
;   ld t2, 0x28(s0)
;   ld a1, 0x30(s0)
;   ld s1, 0x38(s0)
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   addi sp, sp, 0x30
;   ret

function %caller_of_different_callees(i64) -> i64 tail {
    fn0 = %different_callee1(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail
    fn1 = %different_callee2(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail

block0(v99: i64):
    v0 = iconst.i64 10
    v1 = iconst.i64 15
    v2 = iconst.i64 20
    v3 = iconst.i64 25
    v4 = iconst.i64 30
    v5 = iconst.i64 35
    v6 = iconst.i64 40
    v7 = iconst.i64 45
    v8 = iconst.i64 50
    v9 = iconst.i64 55
    v10 = iconst.i64 60
    v11 = iconst.i64 65
    v12 = iconst.i64 70
    v13 = iconst.i64 75
    v14 = iconst.i64 80
    v15 = iconst.i64 85
    v16 = iconst.i64 90
    v17 = iconst.i64 95
    v18 = iconst.i64 100
    v19 = iconst.i64 105
    v20 = iconst.i64 110
    v21 = iconst.i64 115
    v22 = iconst.i64 120
    v23 = iconst.i64 125
    v24 = iconst.i64 130
    v25 = iconst.i64 135
    brif v99, block1, block2

block1:
    return_call fn0(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25)

block2:
    v26 = iconst.i64 140
    return_call fn1(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25, v26)
}

; VCode:
;   add sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   add sp,-32
; block0:
;   li a1,10
;   sd a1,16(nominal_sp)
;   li a0,15
;   sd a0,8(nominal_sp)
;   li a1,20
;   sd a1,0(nominal_sp)
;   li a2,25
;   li a3,30
;   li a4,35
;   li a5,40
;   li a6,45
;   li a7,50
;   li s2,55
;   li s3,60
;   li s4,65
;   li s5,70
;   li s6,75
;   li s7,80
;   li s8,85
;   li s9,90
;   li s10,95
;   li s11,100
;   li t3,105
;   li t4,110
;   li a1,115
;   li a0,120
;   li t2,125
;   li t1,130
;   li t0,135
;   bne s1,zero,taken(label2),not_taken(label1)
; block1:
;   li s1,140
;   add sp,-48
;   virtual_sp_offset_adj +48
;   sd a1,0(sp)
;   sd a0,8(sp)
;   sd t2,16(sp)
;   sd t1,24(sp)
;   sd t0,32(sp)
;   sd s1,40(sp)
;   load_sym t0,%different_callee2+0
;   ld a1,0(nominal_sp)
;   ld a0,8(nominal_sp)
;   ld s1,16(nominal_sp)
;   return_call_ind t0 old_stack_arg_size:0 new_stack_arg_size:48 s1=s1 a0=a0 a1=a1 a2=a2 a3=a3 a4=a4 a5=a5 a6=a6 a7=a7 s2=s2 s3=s3 s4=s4 s5=s5 s6=s6 s7=s7 s8=s8 s9=s9 s10=s10 s11=s11 t3=t3 t4=t4
; block2:
;   ld s1,16(nominal_sp)
;   add sp,-48
;   virtual_sp_offset_adj +48
;   sd a1,0(sp)
;   sd a0,8(sp)
;   sd t2,16(sp)
;   sd t1,24(sp)
;   sd t0,32(sp)
;   load_sym t0,%different_callee1+0
;   ld a1,0(nominal_sp)
;   ld a0,8(nominal_sp)
;   return_call_ind t0 old_stack_arg_size:0 new_stack_arg_size:48 s1=s1 a0=a0 a1=a1 a2=a2 a3=a3 a4=a4 a5=a5 a6=a6 a7=a7 s2=s2 s3=s3 s4=s4 s5=s5 s6=s6 s7=s7 s8=s8 s9=s9 s10=s10 s11=s11 t3=t3 t4=t4
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   ori s0, sp, 0
;   addi sp, sp, -0x20
; block1: ; offset 0x14
;   addi a1, zero, 0xa
;   sd a1, 0x10(sp)
;   addi a0, zero, 0xf
;   sd a0, 8(sp)
;   addi a1, zero, 0x14
;   sd a1, 0(sp)
;   addi a2, zero, 0x19
;   addi a3, zero, 0x1e
;   addi a4, zero, 0x23
;   addi a5, zero, 0x28
;   addi a6, zero, 0x2d
;   addi a7, zero, 0x32
;   addi s2, zero, 0x37
;   addi s3, zero, 0x3c
;   addi s4, zero, 0x41
;   addi s5, zero, 0x46
;   addi s6, zero, 0x4b
;   addi s7, zero, 0x50
;   addi s8, zero, 0x55
;   addi s9, zero, 0x5a
;   addi s10, zero, 0x5f
;   addi s11, zero, 0x64
;   addi t3, zero, 0x69
;   addi t4, zero, 0x6e
;   addi a1, zero, 0x73
;   addi a0, zero, 0x78
;   addi t2, zero, 0x7d
;   addi t1, zero, 0x82
;   addi t0, zero, 0x87
;   bnez s1, 0x88
; block2: ; offset 0x8c
;   addi s1, zero, 0x8c
;   addi sp, sp, -0x30
;   sd a1, 0(sp)
;   sd a0, 8(sp)
;   sd t2, 0x10(sp)
;   sd t1, 0x18(sp)
;   sd t0, 0x20(sp)
;   sd s1, 0x28(sp)
;   auipc t0, 0
;   ld t0, 0xc(t0)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %different_callee2 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   ld a1, 0x30(sp)
;   ld a0, 0x38(sp)
;   ld s1, 0x40(sp)
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   ld t5, 0x28(sp)
;   sd t5, 8(s0)
;   ld t5, 0x20(sp)
;   sd t5, 0(s0)
;   ld t5, 0x18(sp)
;   sd t5, -8(s0)
;   ld t5, 0x10(sp)
;   sd t5, -0x10(s0)
;   ld t5, 8(sp)
;   sd t5, -0x18(s0)
;   ld t5, 0(sp)
;   sd t5, -0x20(s0)
;   addi sp, s0, -0x20
;   ori s0, t6, 0
;   jr t0
; block3: ; offset 0x110
;   ld s1, 0x10(sp)
;   addi sp, sp, -0x30
;   sd a1, 0(sp)
;   sd a0, 8(sp)
;   sd t2, 0x10(sp)
;   sd t1, 0x18(sp)
;   sd t0, 0x20(sp)
;   auipc t0, 0
;   ld t0, 0xc(t0)
;   j 0xc
;   .byte 0x00, 0x00, 0x00, 0x00 ; reloc_external Abs8 %different_callee1 0
;   .byte 0x00, 0x00, 0x00, 0x00
;   ld a1, 0x30(sp)
;   ld a0, 0x38(sp)
;   ld ra, 8(s0)
;   ld t6, 0(s0)
;   ld t5, 0x28(sp)
;   sd t5, 8(s0)
;   ld t5, 0x20(sp)
;   sd t5, 0(s0)
;   ld t5, 0x18(sp)
;   sd t5, -8(s0)
;   ld t5, 0x10(sp)
;   sd t5, -0x10(s0)
;   ld t5, 8(sp)
;   sd t5, -0x18(s0)
;   ld t5, 0(sp)
;   sd t5, -0x20(s0)
;   addi sp, s0, -0x20
;   ori s0, t6, 0
;   jr t0
