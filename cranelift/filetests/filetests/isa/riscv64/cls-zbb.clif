test compile precise-output
set unwind_info=false
target riscv64 has_zbb


function %cls_i8(i8) -> i8 {
block0(v0: i8):
    v1 = cls v0
    return v1
}

; VCode:
; block0:
;   sext.b a2,a0
;   not a4,a2
;   select a0,a4,a2##condition=(a2 slt zero)
;   clz a2,a0
;   addi a0,a2,-57
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x13, 0x16, 0x45, 0x60
;   not a4, a2
;   mv a0, a4
;   bltz a2, 8
;   mv a0, a2
;   .byte 0x13, 0x16, 0x05, 0x60
;   addi a0, a2, -0x39
;   ret

function %cls_i16(i16) -> i16 {
block0(v0: i16):
    v1 = cls v0
    return v1
}

; VCode:
; block0:
;   sext.h a2,a0
;   not a4,a2
;   select a0,a4,a2##condition=(a2 slt zero)
;   clz a2,a0
;   addi a0,a2,-49
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x13, 0x16, 0x55, 0x60
;   not a4, a2
;   mv a0, a4
;   bltz a2, 8
;   mv a0, a2
;   .byte 0x13, 0x16, 0x05, 0x60
;   addi a0, a2, -0x31
;   ret

function %cls_i32(i32) -> i32 {
block0(v0: i32):
    v1 = cls v0
    return v1
}

; VCode:
; block0:
;   sext.w a2,a0
;   not a4,a2
;   select a0,a4,a2##condition=(a2 slt zero)
;   clz a2,a0
;   addi a0,a2,-33
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sext.w a2, a0
;   not a4, a2
;   mv a0, a4
;   bltz a2, 8
;   mv a0, a2
;   .byte 0x13, 0x16, 0x05, 0x60
;   addi a0, a2, -0x21
;   ret

function %cls_i64(i64) -> i64 {
block0(v0: i64):
    v1 = cls v0
    return v1
}

; VCode:
; block0:
;   not a2,a0
;   select a4,a2,a0##condition=(a0 slt zero)
;   clz a0,a4
;   addi a0,a0,-1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   not a2, a0
;   mv a4, a2
;   bltz a0, 8
;   mv a4, a0
;   .byte 0x13, 0x15, 0x07, 0x60
;   addi a0, a0, -1
;   ret

function %cls_i128(i128) -> i128 {
block0(v0: i128):
    v1 = cls v0
    return v1
}

; VCode:
; block0:
;   not a3,a0
;   select a5,a3,a0##condition=(a1 slt zero)
;   not a2,a1
;   select a3,a2,a1##condition=(a1 slt zero)
;   clz a0,a3
;   clz a1,a5
;   select a4,a1,zero##condition=(a3 eq zero)
;   add a5,a0,a4
;   addi a0,a5,-1
;   li a1,0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   not a3, a0
;   mv a5, a3
;   bltz a1, 8
;   mv a5, a0
;   not a2, a1
;   mv a3, a2
;   bltz a1, 8
;   mv a3, a1
;   .byte 0x13, 0x95, 0x06, 0x60
;   .byte 0x93, 0x95, 0x07, 0x60
;   mv a4, a1
;   beqz a3, 8
;   mv a4, zero
;   add a5, a0, a4
;   addi a0, a5, -1
;   mv a1, zero
;   ret

