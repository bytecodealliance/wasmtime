test compile precise-output
set unwind_info=false
target riscv64

function %i128_rotr(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
  v2 = rotr.i128 v0, v1
  return v2
}

; VCode:
; block0:
;   mv a5,a0
;   andi a0,a2,63
;   li a3,64
;   sub a4,a3,a0
;   srl a3,a5,a0
;   sll a6,a1,a4
;   select a6,zero,a6##condition=(a0 eq zero)
;   or a3,a3,a6
;   srl a1,a1,a0
;   sll a4,a5,a4
;   select a4,zero,a4##condition=(a0 eq zero)
;   or a4,a1,a4
;   li a5,64
;   andi a2,a2,127
;   select [a0,a1],[a4,a3],[a3,a4]##condition=(a2 uge a5)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a5, a0
;   andi a0, a2, 0x3f
;   addi a3, zero, 0x40
;   sub a4, a3, a0
;   srl a3, a5, a0
;   sll a6, a1, a4
;   bnez a0, 8
;   mv a6, zero
;   or a3, a3, a6
;   srl a1, a1, a0
;   sll a4, a5, a4
;   bnez a0, 8
;   mv a4, zero
;   or a4, a1, a4
;   addi a5, zero, 0x40
;   andi a2, a2, 0x7f
;   mv a0, a4
;   mv a1, a3
;   bgeu a2, a5, 0xc
;   mv a0, a3
;   mv a1, a4
;   ret

function %f0(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = rotr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   mv a3,a0
;   andi a0,a1,63
;   li a1,64
;   sub a2,a1,a0
;   srl a1,a3,a0
;   sll a2,a3,a2
;   select a3,zero,a2##condition=(a0 eq zero)
;   or a0,a1,a3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a3, a0
;   andi a0, a1, 0x3f
;   addi a1, zero, 0x40
;   sub a2, a1, a0
;   srl a1, a3, a0
;   sll a2, a3, a2
;   mv a3, zero
;   beqz a0, 8
;   mv a3, a2
;   or a0, a1, a3
;   ret

function %f1(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = rotr.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,32
;   srli a2,a0,32
;   andi a0,a1,31
;   li a1,32
;   sub a1,a1,a0
;   srl a3,a2,a0
;   sll a5,a2,a1
;   select a1,zero,a5##condition=(a0 eq zero)
;   or a0,a3,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x20
;   srli a2, a0, 0x20
;   andi a0, a1, 0x1f
;   addi a1, zero, 0x20
;   sub a1, a1, a0
;   srl a3, a2, a0
;   sll a5, a2, a1
;   mv a1, zero
;   beqz a0, 8
;   mv a1, a5
;   or a0, a3, a1
;   ret

function %f2(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
  v2 = rotr.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srli a2,a0,48
;   andi a0,a1,15
;   li a1,16
;   sub a1,a1,a0
;   srl a3,a2,a0
;   sll a5,a2,a1
;   select a1,zero,a5##condition=(a0 eq zero)
;   or a0,a3,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srli a2, a0, 0x30
;   andi a0, a1, 0xf
;   addi a1, zero, 0x10
;   sub a1, a1, a0
;   srl a3, a2, a0
;   sll a5, a2, a1
;   mv a1, zero
;   beqz a0, 8
;   mv a1, a5
;   or a0, a3, a1
;   ret

function %f3(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
  v2 = rotr.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a4,a0,255
;   andi a0,a1,7
;   li a1,8
;   sub a3,a1,a0
;   srl a2,a4,a0
;   sll a3,a4,a3
;   select a5,zero,a3##condition=(a0 eq zero)
;   or a0,a2,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a4, a0, 0xff
;   andi a0, a1, 7
;   addi a1, zero, 8
;   sub a3, a1, a0
;   srl a2, a4, a0
;   sll a3, a4, a3
;   mv a5, zero
;   beqz a0, 8
;   mv a5, a3
;   or a0, a2, a5
;   ret

function %rotr_i64_const_i32(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i32 17
  v2 = rotr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   mv a3,a0
;   li a0,17
;   andi a0,a0,63
;   li a1,64
;   sub a2,a1,a0
;   mv a4,a3
;   srl a1,a4,a0
;   sll a2,a4,a2
;   select a3,zero,a2##condition=(a0 eq zero)
;   or a0,a1,a3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mv a3, a0
;   addi a0, zero, 0x11
;   andi a0, a0, 0x3f
;   addi a1, zero, 0x40
;   sub a2, a1, a0
;   mv a4, a3
;   srl a1, a4, a0
;   sll a2, a4, a2
;   mv a3, zero
;   beqz a0, 8
;   mv a3, a2
;   or a0, a1, a3
;   ret

