test compile precise-output
set unwind_info=false
target riscv64 has_zbb

function %umin_i8(i8, i8) -> i8{
block0(v0: i8, v1: i8):
    v2 = umin v0, v1
    return v2
}

; VCode:
; block0:
;   andi a3,a0,255
;   andi a5,a1,255
;   minu a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a3, a0, 0xff
;   andi a5, a1, 0xff
;   .byte 0x33, 0xd5, 0xf6, 0x0a
;   ret

function %umin_i16(i16, i16) -> i16{
block0(v0: i16, v1: i16):
    v2 = umin v0, v1
    return v2
}

; VCode:
; block0:
;   zext.h a3,a0
;   zext.h a5,a1
;   minu a0,a3,a5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0xbb, 0x46, 0x05, 0x08
;   .byte 0xbb, 0xc7, 0x05, 0x08
;   .byte 0x33, 0xd5, 0xf6, 0x0a
;   ret

function %umin_i32(i32, i32) -> i32{
block0(v0: i32, v1: i32):
    v2 = umin v0, v1
    return v2
}

; VCode:
; block0:
;   slli a3,a0,32
;   srli a5,a3,32
;   slli a1,a1,32
;   srli a3,a1,32
;   minu a0,a5,a3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a3, a0, 0x20
;   srli a5, a3, 0x20
;   slli a1, a1, 0x20
;   srli a3, a1, 0x20
;   .byte 0x33, 0xd5, 0xd7, 0x0a
;   ret

function %umin_i64(i64, i64) -> i64{
block0(v0: i64, v1: i64):
    v2 = umin v0, v1
    return v2
}

; VCode:
; block0:
;   minu a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   .byte 0x33, 0x55, 0xb5, 0x0a
;   ret

function %umin_i128(i128, i128) -> i128{
block0(v0: i128, v1: i128):
    v2 = umin v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s1,8(sp)
;   sd s9,0(sp)
; block0:
;   sltu a5,a1,a3
;   sltu s1,a0,a2
;   xor a4,a1,a3
;   mv s9,a1
;   select a5,s1,a5##condition=(a4 eq zero)
;   mv a4,a0
;   select [a0,a1],[a4,s9],[a2,a3]##condition=(a5 ne zero)
;   ld s1,8(sp)
;   ld s9,0(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s1, 8(sp)
;   sd s9, 0(sp)
; block1: ; offset 0x1c
;   sltu a5, a1, a3
;   sltu s1, a0, a2
;   xor a4, a1, a3
;   mv s9, a1
;   bnez a4, 8
;   mv a5, s1
;   mv a4, a0
;   mv a0, a4
;   mv a1, s9
;   bnez a5, 0xc
;   mv a0, a2
;   mv a1, a3
;   ld s1, 8(sp)
;   ld s9, 0(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

