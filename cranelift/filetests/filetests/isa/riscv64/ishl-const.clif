test compile precise-output
set unwind_info=false
target riscv64


function %ishl_i8_const_i8(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i8 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i8_const_i16(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i16 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i8_const_i32(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i32 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i8_const_i64(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i64 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i8_const_i128(i8) -> i8 {
block0(v0: i8):
    v1 = iconst.i64 5
    v2 = uextend.i128 v1
    v3 = ishl v0, v2
    return v3
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i16_const_i8(i16) -> i16 {
block0(v0: i16):
    v1 = iconst.i8 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i16_const_i16(i16) -> i16 {
block0(v0: i16):
    v1 = iconst.i16 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i16_const_i32(i16) -> i16 {
block0(v0: i16):
    v1 = iconst.i32 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i16_const_i64(i16) -> i16 {
block0(v0: i16):
    v1 = iconst.i64 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i16_const_i128(i16) -> i16 {
block0(v0: i16):
    v1 = iconst.i64 5
    v2 = uextend.i128 v1
    v3 = ishl v0, v2
    return v3
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i32_const_i8(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i8 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i32_const_i16(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i16 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i32_const_i32(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i32 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i32_const_i64(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i64 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i32_const_i128(i32) -> i32 {
block0(v0: i32):
    v1 = iconst.i64 5
    v2 = uextend.i128 v1
    v3 = ishl v0, v2
    return v3
}

; VCode:
; block0:
;   slliw a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slliw a0, a0, 5
;   ret

function %ishl_i64_const_i8(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i8 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 5
;   ret

function %ishl_i64_const_i16(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i16 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 5
;   ret

function %ishl_i64_const_i32(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i32 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 5
;   ret

function %ishl_i64_const_i64(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 5
;   ret

function %ishl_i64_const_i128(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 5
    v2 = uextend.i128 v1
    v3 = ishl v0, v2
    return v3
}

; VCode:
; block0:
;   slli a0,a0,5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 5
;   ret

function %ishl_i128_const_i8(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i8 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   li a5,5
;   andi a2,a5,63
;   li a3,64
;   sub a4,a3,a2
;   sll a3,a0,a2
;   srl a4,a0,a4
;   select a4,zero,a4##condition=(a2 eq zero)
;   sll a0,a1,a2
;   or a4,a4,a0
;   li s4,64
;   andi a2,a5,127
;   select [a0,a1],[zero,a3],[a3,a4]##condition=(a2 uge s4)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   addi a5, zero, 5
;   andi a2, a5, 0x3f
;   addi a3, zero, 0x40
;   sub a4, a3, a2
;   sll a3, a0, a2
;   srl a4, a0, a4
;   bnez a2, 8
;   mv a4, zero
;   sll a0, a1, a2
;   or a4, a4, a0
;   addi s4, zero, 0x40
;   andi a2, a5, 0x7f
;   mv a0, zero
;   mv a1, a3
;   bgeu a2, s4, 0xc
;   mv a0, a3
;   mv a1, a4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %ishl_i128_const_i16(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i16 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   li a5,5
;   andi a2,a5,63
;   li a3,64
;   sub a4,a3,a2
;   sll a3,a0,a2
;   srl a4,a0,a4
;   select a4,zero,a4##condition=(a2 eq zero)
;   sll a0,a1,a2
;   or a4,a4,a0
;   li s4,64
;   andi a2,a5,127
;   select [a0,a1],[zero,a3],[a3,a4]##condition=(a2 uge s4)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   addi a5, zero, 5
;   andi a2, a5, 0x3f
;   addi a3, zero, 0x40
;   sub a4, a3, a2
;   sll a3, a0, a2
;   srl a4, a0, a4
;   bnez a2, 8
;   mv a4, zero
;   sll a0, a1, a2
;   or a4, a4, a0
;   addi s4, zero, 0x40
;   andi a2, a5, 0x7f
;   mv a0, zero
;   mv a1, a3
;   bgeu a2, s4, 0xc
;   mv a0, a3
;   mv a1, a4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %ishl_i128_const_i32(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i32 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   li a5,5
;   andi a2,a5,63
;   li a3,64
;   sub a4,a3,a2
;   sll a3,a0,a2
;   srl a4,a0,a4
;   select a4,zero,a4##condition=(a2 eq zero)
;   sll a0,a1,a2
;   or a4,a4,a0
;   li s4,64
;   andi a2,a5,127
;   select [a0,a1],[zero,a3],[a3,a4]##condition=(a2 uge s4)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   addi a5, zero, 5
;   andi a2, a5, 0x3f
;   addi a3, zero, 0x40
;   sub a4, a3, a2
;   sll a3, a0, a2
;   srl a4, a0, a4
;   bnez a2, 8
;   mv a4, zero
;   sll a0, a1, a2
;   or a4, a4, a0
;   addi s4, zero, 0x40
;   andi a2, a5, 0x7f
;   mv a0, zero
;   mv a1, a3
;   bgeu a2, s4, 0xc
;   mv a0, a3
;   mv a1, a4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %ishl_i128_const_i64(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i64 5
    v2 = ishl v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   li a5,5
;   andi a2,a5,63
;   li a3,64
;   sub a4,a3,a2
;   sll a3,a0,a2
;   srl a4,a0,a4
;   select a4,zero,a4##condition=(a2 eq zero)
;   sll a0,a1,a2
;   or a4,a4,a0
;   li s4,64
;   andi a2,a5,127
;   select [a0,a1],[zero,a3],[a3,a4]##condition=(a2 uge s4)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   addi a5, zero, 5
;   andi a2, a5, 0x3f
;   addi a3, zero, 0x40
;   sub a4, a3, a2
;   sll a3, a0, a2
;   srl a4, a0, a4
;   bnez a2, 8
;   mv a4, zero
;   sll a0, a1, a2
;   or a4, a4, a0
;   addi s4, zero, 0x40
;   andi a2, a5, 0x7f
;   mv a0, zero
;   mv a1, a3
;   bgeu a2, s4, 0xc
;   mv a0, a3
;   mv a1, a4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %ishl_i128_const_i128(i128) -> i128 {
block0(v0: i128):
    v1 = iconst.i64 5
    v2 = uextend.i128 v1
    v3 = ishl v0, v2
    return v3
}

; VCode:
; block0:
;   li a3,5
;   li a2,0
;   andi a4,a3,63
;   li a2,64
;   sub a5,a2,a4
;   sll a2,a0,a4
;   srl a5,a0,a5
;   select a5,zero,a5##condition=(a4 eq zero)
;   sll a0,a1,a4
;   or a4,a5,a0
;   li a5,64
;   andi a3,a3,127
;   select [a0,a1],[zero,a2],[a2,a4]##condition=(a3 uge a5)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a3, zero, 5
;   mv a2, zero
;   andi a4, a3, 0x3f
;   addi a2, zero, 0x40
;   sub a5, a2, a4
;   sll a2, a0, a4
;   srl a5, a0, a5
;   bnez a4, 8
;   mv a5, zero
;   sll a0, a1, a4
;   or a4, a5, a0
;   addi a5, zero, 0x40
;   andi a3, a3, 0x7f
;   mv a0, zero
;   mv a1, a2
;   bgeu a3, a5, 0xc
;   mv a0, a2
;   mv a1, a4
;   ret

