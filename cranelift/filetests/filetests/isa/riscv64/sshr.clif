test compile precise-output
set unwind_info=false
target riscv64


function %sshr_i8_i8(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,56
;   srai a0,a0,56
;   andi a1,a1,7
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x38
;   srai a0, a0, 0x38
;   andi a1, a1, 7
;   sraw a0, a0, a1
;   ret

function %sshr_i8_i16(i8, i16) -> i8 {
block0(v0: i8, v1: i16):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,56
;   srai a0,a0,56
;   andi a1,a1,7
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x38
;   srai a0, a0, 0x38
;   andi a1, a1, 7
;   sraw a0, a0, a1
;   ret

function %sshr_i8_i32(i8, i32) -> i8 {
block0(v0: i8, v1: i32):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,56
;   srai a0,a0,56
;   andi a1,a1,7
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x38
;   srai a0, a0, 0x38
;   andi a1, a1, 7
;   sraw a0, a0, a1
;   ret

function %sshr_i8_i64(i8, i64) -> i8 {
block0(v0: i8, v1: i64):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,56
;   srai a0,a0,56
;   andi a1,a1,7
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x38
;   srai a0, a0, 0x38
;   andi a1, a1, 7
;   sraw a0, a0, a1
;   ret

function %sshr_i8_i128(i8, i128) -> i8 {
block0(v0: i8, v1: i128):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,56
;   srai a0,a0,56
;   andi a1,a1,7
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x38
;   srai a0, a0, 0x38
;   andi a1, a1, 7
;   sraw a0, a0, a1
;   ret

function %sshr_i16_i8(i16, i8) -> i16 {
block0(v0: i16, v1: i8):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srai a0,a0,48
;   andi a1,a1,15
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srai a0, a0, 0x30
;   andi a1, a1, 0xf
;   sraw a0, a0, a1
;   ret

function %sshr_i16_i16(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srai a0,a0,48
;   andi a1,a1,15
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srai a0, a0, 0x30
;   andi a1, a1, 0xf
;   sraw a0, a0, a1
;   ret

function %sshr_i16_i32(i16, i32) -> i16 {
block0(v0: i16, v1: i32):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srai a0,a0,48
;   andi a1,a1,15
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srai a0, a0, 0x30
;   andi a1, a1, 0xf
;   sraw a0, a0, a1
;   ret

function %sshr_i16_i64(i16, i64) -> i16 {
block0(v0: i16, v1: i64):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srai a0,a0,48
;   andi a1,a1,15
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srai a0, a0, 0x30
;   andi a1, a1, 0xf
;   sraw a0, a0, a1
;   ret

function %sshr_i16_i128(i16, i128) -> i16 {
block0(v0: i16, v1: i128):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srai a0,a0,48
;   andi a1,a1,15
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srai a0, a0, 0x30
;   andi a1, a1, 0xf
;   sraw a0, a0, a1
;   ret

function %sshr_i32_i8(i32, i8) -> i32 {
block0(v0: i32, v1: i8):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sraw a0, a0, a1
;   ret

function %sshr_i32_i16(i32, i16) -> i32 {
block0(v0: i32, v1: i16):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sraw a0, a0, a1
;   ret

function %sshr_i32_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sraw a0, a0, a1
;   ret

function %sshr_i32_i64(i32, i64) -> i32 {
block0(v0: i32, v1: i64):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sraw a0, a0, a1
;   ret

function %sshr_i32_i128(i32, i128) -> i32 {
block0(v0: i32, v1: i128):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sraw a0, a0, a1
;   ret

function %sshr_i64_i8(i64, i8) -> i64 {
block0(v0: i64, v1: i8):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sra a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sra a0, a0, a1
;   ret

function %sshr_i64_i16(i64, i16) -> i64 {
block0(v0: i64, v1: i16):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sra a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sra a0, a0, a1
;   ret

function %sshr_i64_i32(i64, i32) -> i64 {
block0(v0: i64, v1: i32):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sra a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sra a0, a0, a1
;   ret

function %sshr_i64_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sra a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sra a0, a0, a1
;   ret

function %sshr_i64_i128(i64, i128) -> i64 {
block0(v0: i64, v1: i128):
    v2 = sshr v0, v1
    return v2
}

; VCode:
; block0:
;   sra a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sra a0, a0, a1
;   ret

function %sshr_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = sshr v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   mv a4,a0
;   andi a0,a2,63
;   li a3,64
;   sub a3,a3,a0
;   sll a3,a1,a3
;   select a3,zero,a3##condition=(a0 eq zero)
;   srl a4,a4,a0
;   or a3,a3,a4
;   li a4,64
;   sra s4,a1,a0
;   li a0,-1
;   select a4,a0,zero##condition=(a1 slt zero)
;   li a5,64
;   andi a2,a2,127
;   select [a0,a1],[s4,a4],[a3,s4]##condition=(a2 uge a5)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   mv a4, a0
;   andi a0, a2, 0x3f
;   addi a3, zero, 0x40
;   sub a3, a3, a0
;   sll a3, a1, a3
;   bnez a0, 8
;   mv a3, zero
;   srl a4, a4, a0
;   or a3, a3, a4
;   addi a4, zero, 0x40
;   sra s4, a1, a0
;   addi a0, zero, -1
;   mv a4, a0
;   bltz a1, 8
;   mv a4, zero
;   addi a5, zero, 0x40
;   andi a2, a2, 0x7f
;   mv a0, s4
;   mv a1, a4
;   bgeu a2, a5, 0xc
;   mv a0, a3
;   mv a1, s4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %sshr_i128_i16(i128, i16) -> i128 {
block0(v0: i128, v1: i16):
    v2 = sshr v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   mv a4,a0
;   andi a0,a2,63
;   li a3,64
;   sub a3,a3,a0
;   sll a3,a1,a3
;   select a3,zero,a3##condition=(a0 eq zero)
;   srl a4,a4,a0
;   or a3,a3,a4
;   li a4,64
;   sra s4,a1,a0
;   li a0,-1
;   select a4,a0,zero##condition=(a1 slt zero)
;   li a5,64
;   andi a2,a2,127
;   select [a0,a1],[s4,a4],[a3,s4]##condition=(a2 uge a5)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   mv a4, a0
;   andi a0, a2, 0x3f
;   addi a3, zero, 0x40
;   sub a3, a3, a0
;   sll a3, a1, a3
;   bnez a0, 8
;   mv a3, zero
;   srl a4, a4, a0
;   or a3, a3, a4
;   addi a4, zero, 0x40
;   sra s4, a1, a0
;   addi a0, zero, -1
;   mv a4, a0
;   bltz a1, 8
;   mv a4, zero
;   addi a5, zero, 0x40
;   andi a2, a2, 0x7f
;   mv a0, s4
;   mv a1, a4
;   bgeu a2, a5, 0xc
;   mv a0, a3
;   mv a1, s4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %sshr_i128_i32(i128, i32) -> i128 {
block0(v0: i128, v1: i32):
    v2 = sshr v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   mv a4,a0
;   andi a0,a2,63
;   li a3,64
;   sub a3,a3,a0
;   sll a3,a1,a3
;   select a3,zero,a3##condition=(a0 eq zero)
;   srl a4,a4,a0
;   or a3,a3,a4
;   li a4,64
;   sra s4,a1,a0
;   li a0,-1
;   select a4,a0,zero##condition=(a1 slt zero)
;   li a5,64
;   andi a2,a2,127
;   select [a0,a1],[s4,a4],[a3,s4]##condition=(a2 uge a5)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   mv a4, a0
;   andi a0, a2, 0x3f
;   addi a3, zero, 0x40
;   sub a3, a3, a0
;   sll a3, a1, a3
;   bnez a0, 8
;   mv a3, zero
;   srl a4, a4, a0
;   or a3, a3, a4
;   addi a4, zero, 0x40
;   sra s4, a1, a0
;   addi a0, zero, -1
;   mv a4, a0
;   bltz a1, 8
;   mv a4, zero
;   addi a5, zero, 0x40
;   andi a2, a2, 0x7f
;   mv a0, s4
;   mv a1, a4
;   bgeu a2, a5, 0xc
;   mv a0, a3
;   mv a1, s4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %sshr_i128_i64(i128, i64) -> i128 {
block0(v0: i128, v1: i64):
    v2 = sshr v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s4,8(sp)
; block0:
;   mv a4,a0
;   andi a0,a2,63
;   li a3,64
;   sub a3,a3,a0
;   sll a3,a1,a3
;   select a3,zero,a3##condition=(a0 eq zero)
;   srl a4,a4,a0
;   or a3,a3,a4
;   li a4,64
;   sra s4,a1,a0
;   li a0,-1
;   select a4,a0,zero##condition=(a1 slt zero)
;   li a5,64
;   andi a2,a2,127
;   select [a0,a1],[s4,a4],[a3,s4]##condition=(a2 uge a5)
;   ld s4,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s4, 8(sp)
; block1: ; offset 0x18
;   mv a4, a0
;   andi a0, a2, 0x3f
;   addi a3, zero, 0x40
;   sub a3, a3, a0
;   sll a3, a1, a3
;   bnez a0, 8
;   mv a3, zero
;   srl a4, a4, a0
;   or a3, a3, a4
;   addi a4, zero, 0x40
;   sra s4, a1, a0
;   addi a0, zero, -1
;   mv a4, a0
;   bltz a1, 8
;   mv a4, zero
;   addi a5, zero, 0x40
;   andi a2, a2, 0x7f
;   mv a0, s4
;   mv a1, a4
;   bgeu a2, a5, 0xc
;   mv a0, a3
;   mv a1, s4
;   ld s4, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

function %sshr_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = sshr v0, v1
    return v2
}

; VCode:
;   addi sp,sp,-16
;   sd ra,8(sp)
;   sd fp,0(sp)
;   mv fp,sp
;   addi sp,sp,-16
;   sd s5,8(sp)
; block0:
;   mv a4,a0
;   andi a0,a2,63
;   li a3,64
;   sub a3,a3,a0
;   sll a3,a1,a3
;   select a3,zero,a3##condition=(a0 eq zero)
;   srl a5,a4,a0
;   or a3,a3,a5
;   li a4,64
;   sra s5,a1,a0
;   li a0,-1
;   select a4,a0,zero##condition=(a1 slt zero)
;   li a5,64
;   andi a2,a2,127
;   select [a0,a1],[s5,a4],[a3,s5]##condition=(a2 uge a5)
;   ld s5,8(sp)
;   addi sp,sp,16
;   ld ra,8(sp)
;   ld fp,0(sp)
;   addi sp,sp,16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi sp, sp, -0x10
;   sd ra, 8(sp)
;   sd s0, 0(sp)
;   mv s0, sp
;   addi sp, sp, -0x10
;   sd s5, 8(sp)
; block1: ; offset 0x18
;   mv a4, a0
;   andi a0, a2, 0x3f
;   addi a3, zero, 0x40
;   sub a3, a3, a0
;   sll a3, a1, a3
;   bnez a0, 8
;   mv a3, zero
;   srl a5, a4, a0
;   or a3, a3, a5
;   addi a4, zero, 0x40
;   sra s5, a1, a0
;   addi a0, zero, -1
;   mv a4, a0
;   bltz a1, 8
;   mv a4, zero
;   addi a5, zero, 0x40
;   andi a2, a2, 0x7f
;   mv a0, s5
;   mv a1, a4
;   bgeu a2, a5, 0xc
;   mv a0, a3
;   mv a1, s5
;   ld s5, 8(sp)
;   addi sp, sp, 0x10
;   ld ra, 8(sp)
;   ld s0, 0(sp)
;   addi sp, sp, 0x10
;   ret

