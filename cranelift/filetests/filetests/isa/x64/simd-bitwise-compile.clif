test compile precise-output
set enable_simd
target x86_64 skylake

function %band_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = band v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   andps   %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %band_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = band v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   andpd   %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %band_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = band v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pand    %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bor_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = bor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   orps    %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bor_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = bor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   orpd    %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bor_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   por     %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bxor_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
    v2 = bxor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   xorps   %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bxor_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
    v2 = bxor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   xorpd   %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bxor_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bxor v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pxor    %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %bitselect_i16x8() -> i16x8 {
block0:
    v0 = vconst.i16x8 [0 0 0 0 0 0 0 0]
    v1 = vconst.i16x8 [0 0 0 0 0 0 0 0]
    v2 = vconst.i16x8 [0 0 0 0 0 0 0 0]
    v3 = bitselect v0, v1, v2
    return v3
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqu  const(0), %xmm0
;   movdqu  const(0), %xmm2
;   movdqu  const(0), %xmm6
;   pand    %xmm2, %xmm0, %xmm2
;   pandn   %xmm0, %xmm6, %xmm0
;   por     %xmm0, %xmm2, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %vselect_i16x8(i16x8, i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8, v2: i16x8):
    v3 = vselect v0, v1, v2
    return v3
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm2, %xmm6
;   pblendvb %xmm6, %xmm1, %xmm6
;   movdqa  %xmm6, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %vselect_f32x4(i32x4, f32x4, f32x4) -> f32x4 {
block0(v0: i32x4, v1: f32x4, v2: f32x4):
    v3 = vselect v0, v1, v2
    return v3
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm2, %xmm6
;   blendvps %xmm6, %xmm1, %xmm6
;   movdqa  %xmm6, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %vselect_f64x2(i64x2, f64x2, f64x2) -> f64x2 {
block0(v0: i64x2, v1: f64x2, v2: f64x2):
    v3 = vselect v0, v1, v2
    return v3
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm2, %xmm6
;   blendvpd %xmm6, %xmm1, %xmm6
;   movdqa  %xmm6, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %ishl_i8x16(i32) -> i8x16 {
block0(v0: i32):
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = ishl v1, v0
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqu  const(1), %xmm0
;   movq    %rdi, %r11
;   andq    %r11, $7, %r11
;   movd    %r11d, %xmm6
;   psllw   %xmm0, %xmm6, %xmm0
;   lea     const(0), %rdi
;   shlq    $4, %r11, %r11
;   movdqu  0(%rdi,%r11,1), %xmm14
;   pand    %xmm0, %xmm14, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %ushr_i8x16_imm() -> i8x16 {
block0:
    v0 = iconst.i32 1
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = ushr v1, v0
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqu  const(1), %xmm0
;   movl    $1, %r10d
;   andq    %r10, $7, %r10
;   movd    %r10d, %xmm6
;   psrlw   %xmm0, %xmm6, %xmm0
;   lea     const(0), %rdi
;   shlq    $4, %r10, %r10
;   movdqu  0(%rdi,%r10,1), %xmm14
;   pand    %xmm0, %xmm14, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %sshr_i8x16(i32) -> i8x16 {
block0(v0: i32):
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = sshr v1, v0
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqu  const(0), %xmm9
;   movq    %rdi, %r10
;   andq    %r10, $7, %r10
;   movdqa  %xmm9, %xmm0
;   punpcklbw %xmm0, %xmm9, %xmm0
;   punpckhbw %xmm9, %xmm9, %xmm9
;   addl    %r10d, $8, %r10d
;   movd    %r10d, %xmm12
;   psraw   %xmm0, %xmm12, %xmm0
;   psraw   %xmm9, %xmm12, %xmm9
;   packsswb %xmm0, %xmm9, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %sshr_i8x16_imm(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = sshr_imm v0, 3
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movl    $3, %r11d
;   andq    %r11, $7, %r11
;   movdqa  %xmm0, %xmm14
;   punpcklbw %xmm14, %xmm0, %xmm14
;   movdqa  %xmm14, %xmm13
;   movdqa  %xmm0, %xmm14
;   punpckhbw %xmm14, %xmm0, %xmm14
;   addl    %r11d, $8, %r11d
;   movd    %r11d, %xmm15
;   movdqa  %xmm13, %xmm0
;   psraw   %xmm0, %xmm15, %xmm0
;   psraw   %xmm14, %xmm15, %xmm14
;   packsswb %xmm0, %xmm14, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %sshr_i64x2(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = sshr v0, v1
    return v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pextrd.w $0, %xmm0, %r9
;   pextrd.w $1, %xmm0, %r11
;   movq    %rdi, %rcx
;   sarq    %cl, %r9, %r9
;   sarq    %cl, %r11, %r11
;   uninit  %xmm0
;   pinsrd.w $0, %xmm0, %r9, %xmm0
;   pinsrd.w $1, %xmm0, %r11, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

