test compile precise-output
target x86_64 sse42 has_avx has_avx2 has_avx512f has_avx512vl

function %sshr(i64x2, i64) -> i64x2, i64x2 {
block0(v0: i64x2, v1: i64):
  ; Force register allocation to pick a different destination than
  ; source for at least one of these instructions.
  v2 = sshr v0, v1
  v3 = sshr v2, v1
  return v2, v3
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %r9
;   andq    %r9, $63, %r9
;   vmovd   %r9d, %xmm1
;   vpsraq  %xmm1, %xmm0, %xmm0
;   andq    %rdi, $63, %rdi
;   vmovd   %edi, %xmm1
;   vpsraq  %xmm1, %xmm0, %xmm1
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq %rdi, %r9
;   andq $0x3f, %r9
;   vmovd %r9d, %xmm1
;   vpsraq %xmm1, %xmm0, %xmm0
;   andq $0x3f, %rdi
;   vmovd %edi, %xmm1
;   vpsraq %xmm1, %xmm0, %xmm1
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %sshr_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
  v1 = sshr_imm v0, 31
  return v1
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   vpsraqimm $31, %xmm0, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpsraq $0x1f, %xmm0, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq


function %sshr_load_imm(i64) -> i64x2 {
block0(v0: i64):
  v1 = load.i64x2 v0
  v2 = sshr_imm v1, 31
  return v2
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   vpsraqimm $31, 0(%rdi), %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpsraq $0x1f, (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq


function %sshr_load2_imm(i64) -> i64x2 {
block0(v0: i64):
  v1 = load.i64x2 v0+7
  v2 = sshr_imm v1, 31
  return v2
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   vpsraqimm $31, 7(%rdi), %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpsraq $0x1f, 7(%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %sshr_load3_imm(i64) -> i64x2 {
block0(v0: i64):
  v1 = load.i64x2 v0+16
  v2 = sshr_imm v1, 31
  return v2
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   vpsraqimm $31, 16(%rdi), %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpsraq $0x1f, 0x10(%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

