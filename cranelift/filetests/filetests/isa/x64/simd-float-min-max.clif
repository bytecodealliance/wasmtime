test compile precise-output
target x86_64 sse41

function %fmax_f32x4(i64, f32x4) -> f32x4 {
block0(v0: i64, v1: f32x4):
    v2 = load.f32x4 v0
    v3 = fmax v1, v2
    return v3
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movups  0(%rdi), %xmm4
;   movdqa  %xmm0, %xmm6
;   maxps   %xmm0, %xmm4, %xmm0
;   maxps   %xmm4, %xmm6, %xmm4
;   movdqa  %xmm0, %xmm1
;   xorps   %xmm1, %xmm4, %xmm1
;   orps    %xmm0, %xmm1, %xmm0
;   movdqa  %xmm0, %xmm4
;   subps   %xmm4, %xmm1, %xmm4
;   cmpps   $3, %xmm0, %xmm0, %xmm0
;   psrld   %xmm0, $10, %xmm0
;   andnps  %xmm0, %xmm4, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movups (%rdi), %xmm4 ; trap: heap_oob
;   movdqa %xmm0, %xmm6
;   maxps %xmm4, %xmm0
;   maxps %xmm6, %xmm4
;   movdqa %xmm0, %xmm1
;   xorps %xmm4, %xmm1
;   orps %xmm1, %xmm0
;   movdqa %xmm0, %xmm4
;   subps %xmm1, %xmm4
;   cmpunordps %xmm0, %xmm0
;   psrld $0xa, %xmm0
;   andnps %xmm4, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %fmin_f32x4(i64, f32x4) -> f32x4 {
block0(v0: i64, v1: f32x4):
    v2 = load.f32x4 v0
    v3 = fmin v1, v2
    return v3
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movups  0(%rdi), %xmm4
;   movdqa  %xmm0, %xmm1
;   minps   %xmm1, %xmm4, %xmm1
;   minps   %xmm4, %xmm0, %xmm4
;   orps    %xmm1, %xmm4, %xmm1
;   movdqa  %xmm1, %xmm0
;   cmpps   $3, %xmm0, %xmm4, %xmm0
;   orps    %xmm1, %xmm0, %xmm1
;   psrld   %xmm0, $10, %xmm0
;   andnps  %xmm0, %xmm1, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movups (%rdi), %xmm4 ; trap: heap_oob
;   movdqa %xmm0, %xmm1
;   minps %xmm4, %xmm1
;   minps %xmm0, %xmm4
;   orps %xmm4, %xmm1
;   movdqa %xmm1, %xmm0
;   cmpunordps %xmm4, %xmm0
;   orps %xmm0, %xmm1
;   psrld $0xa, %xmm0
;   andnps %xmm1, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %fmax_f64x2(i64, f64x2) -> f64x2 {
block0(v0: i64, v1: f64x2):
    v2 = load.f64x2 v0
    v3 = fmax v1, v2
    return v3
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movupd  0(%rdi), %xmm4
;   movdqa  %xmm0, %xmm6
;   maxpd   %xmm0, %xmm4, %xmm0
;   maxpd   %xmm4, %xmm6, %xmm4
;   movdqa  %xmm0, %xmm1
;   xorpd   %xmm1, %xmm4, %xmm1
;   orpd %xmm1, %xmm0
;   movdqa  %xmm0, %xmm4
;   subpd   %xmm4, %xmm1, %xmm4
;   cmppd   $3, %xmm0, %xmm0, %xmm0
;   psrlq   %xmm0, $13, %xmm0
;   andnpd  %xmm0, %xmm4, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movupd (%rdi), %xmm4 ; trap: heap_oob
;   movdqa %xmm0, %xmm6
;   maxpd %xmm4, %xmm0
;   maxpd %xmm6, %xmm4
;   movdqa %xmm0, %xmm1
;   xorpd %xmm4, %xmm1
;   orpd %xmm1, %xmm0
;   movdqa %xmm0, %xmm4
;   subpd %xmm1, %xmm4
;   cmpunordpd %xmm0, %xmm0
;   psrlq $0xd, %xmm0
;   andnpd %xmm4, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %fmin_f64x2(i64, f64x2) -> f64x2 {
block0(v0: i64, v1: f64x2):
    v2 = load.f64x2 v0
    v3 = fmin v1, v2
    return v3
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movupd  0(%rdi), %xmm4
;   movdqa  %xmm0, %xmm5
;   minpd   %xmm0, %xmm4, %xmm0
;   minpd   %xmm4, %xmm5, %xmm4
;   movdqa  %xmm0, %xmm2
;   orpd %xmm4, %xmm2
;   cmppd   $3, %xmm0, %xmm4, %xmm0
;   orpd %xmm0, %xmm2
;   psrlq   %xmm0, $13, %xmm0
;   andnpd  %xmm0, %xmm2, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movupd (%rdi), %xmm4 ; trap: heap_oob
;   movdqa %xmm0, %xmm5
;   minpd %xmm4, %xmm0
;   minpd %xmm5, %xmm4
;   movdqa %xmm0, %xmm2
;   orpd %xmm4, %xmm2
;   cmpunordpd %xmm4, %xmm0
;   orpd %xmm0, %xmm2
;   psrlq $0xd, %xmm0
;   andnpd %xmm2, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

