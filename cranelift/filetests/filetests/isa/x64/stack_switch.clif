test compile precise-output
set opt_level=speed
set stack_switch_model=basic
target x86_64

;; Test code emitted for stack switch itself
function %switch(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = stack_switch v0, v1, v2
  return v3
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $48, %rsp
;   movq    %rbx, 0(%rsp)
;   movq    %r12, 8(%rsp)
;   movq    %r13, 16(%rsp)
;   movq    %r14, 24(%rsp)
;   movq    %r15, 32(%rsp)
; block0:
;   movq    %rdi, %r10
;   movq    %rdx, %rdi
;   %rdi = stack_switch_basic %r10, %rsi, %rdi
;   movq    %rdi, %rax
;   movq    0(%rsp), %rbx
;   movq    8(%rsp), %r12
;   movq    16(%rsp), %r13
;   movq    24(%rsp), %r14
;   movq    32(%rsp), %r15
;   addq    %rsp, $48, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x30, %rsp
;   movq %rbx, (%rsp)
;   movq %r12, 8(%rsp)
;   movq %r13, 0x10(%rsp)
;   movq %r14, 0x18(%rsp)
;   movq %r15, 0x20(%rsp)
; block1: ; offset 0x20
;   movq %rdi, %r10
;   movq %rdx, %rdi
;   movq (%rsi), %rax
;   movq %rsp, (%r10)
;   movq %rax, %rsp
;   movq 8(%rsi), %rax
;   movq %rbp, 8(%r10)
;   movq %rax, %rbp
;   movq 0x10(%rsi), %rax
;   leaq 6(%rip), %rcx
;   movq %rcx, 0x10(%r10)
;   jmpq *%rax
;   movq %rdi, %rax
;   movq (%rsp), %rbx
;   movq 8(%rsp), %r12
;   movq 0x10(%rsp), %r13
;   movq 0x18(%rsp), %r14
;   movq 0x20(%rsp), %r15
;   addq $0x30, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

;; Test clobbering of all 14 GPRs used by Cranelift
function %switch_int_clobber(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = iconst.i64 0

;; We create values v100 to v114 before the stack_switch and want to use them
;; afterwards. Thus, they must all be spilled on the stack.
;; Given that they are loads, they cannot be moved behind the stack_switch.

  v100 = load.i64 v0+0
  v101 = load.i64 v0+8
  v102 = load.i64 v0+16
  v103 = load.i64 v0+24
  v104 = load.i64 v0+32
  v105 = load.i64 v0+40
  v106 = load.i64 v0+48
  v107 = load.i64 v0+56
  v108 = load.i64 v0+64
  v109 = load.i64 v0+72
  v110 = load.i64 v0+80
  v111 = load.i64 v0+88
  v112 = load.i64 v0+96
  v113 = load.i64 v0+104
  v114 = load.i64 v0+112

  v299 = stack_switch v1, v1, v2

;; We use v100 - v114 after the switch: We load from each v(100+i) into
;; v(200+i), and use v(300+i) as an accumulator of all values so far.
;; These are loads and therefore they cannot be moved before the stack_switch.

  v200 = load.i64 v100
  v300 = iadd.i64 v299, v200
  v201 = load.i64 v101
  v301 = iadd.i64 v300, v201
  v202 = load.i64 v102
  v302 = iadd.i64 v301, v202
  v203 = load.i64 v103
  v303 = iadd.i64 v302, v203
  v204 = load.i64 v104
  v304 = iadd.i64 v303, v204
  v205 = load.i64 v105
  v305 = iadd.i64 v304, v205
  v206 = load.i64 v106
  v306 = iadd.i64 v305, v206
  v207 = load.i64 v107
  v307 = iadd.i64 v306, v207
  v208 = load.i64 v108
  v308 = iadd.i64 v307, v208
  v209 = load.i64 v109
  v309 = iadd.i64 v308, v209
  v210 = load.i64 v110
  v310 = iadd.i64 v309, v210
  v211 = load.i64 v111
  v311 = iadd.i64 v310, v211
  v212 = load.i64 v112
  v312 = iadd.i64 v311, v212
  v213 = load.i64 v113
  v313 = iadd.i64 v312, v213
  v214 = load.i64 v114
  v314 = iadd.i64 v313, v214

;; Let's also use v0 again
  v400 = iadd.i64 v314, v0

;; We cannot use v1 again: That causes TooManyLiveRegs, as this usage is an
;; instance of https://github.com/bytecodealliance/regalloc2/issues/145
;; v401 = iadd.i64 v400, v1

  return v400
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $176, %rsp
;   movq    %rbx, 128(%rsp)
;   movq    %r12, 136(%rsp)
;   movq    %r13, 144(%rsp)
;   movq    %r14, 152(%rsp)
;   movq    %r15, 160(%rsp)
; block0:
;   movq    0(%rdi), %r8
;   movq    %r8, rsp(16 + virtual offset)
;   movq    8(%rdi), %r8
;   movq    %r8, rsp(8 + virtual offset)
;   movq    16(%rdi), %r8
;   movq    %r8, rsp(120 + virtual offset)
;   movq    24(%rdi), %r8
;   movq    %r8, rsp(112 + virtual offset)
;   movq    32(%rdi), %r8
;   movq    %r8, rsp(104 + virtual offset)
;   movq    40(%rdi), %r8
;   movq    %r8, rsp(96 + virtual offset)
;   movq    48(%rdi), %r8
;   movq    %r8, rsp(88 + virtual offset)
;   movq    56(%rdi), %r8
;   movq    %r8, rsp(80 + virtual offset)
;   movq    64(%rdi), %r8
;   movq    %r8, rsp(72 + virtual offset)
;   movq    72(%rdi), %r8
;   movq    %r8, rsp(64 + virtual offset)
;   movq    80(%rdi), %r8
;   movq    %r8, rsp(56 + virtual offset)
;   movq    88(%rdi), %r8
;   movq    %r8, rsp(48 + virtual offset)
;   movq    96(%rdi), %r8
;   movq    %r8, rsp(40 + virtual offset)
;   movq    104(%rdi), %r8
;   movq    %r8, rsp(32 + virtual offset)
;   movq    112(%rdi), %r8
;   movq    %rdi, rsp(0 + virtual offset)
;   movq    %r8, rsp(24 + virtual offset)
;   uninit  %rdi
;   xorq %rdi, %rdi
;   %rdi = stack_switch_basic %rsi, %rsi, %rdi
;   movq    rsp(16 + virtual offset), %r8
;   movq    0(%r8), %r9
;   movq    rsp(8 + virtual offset), %r8
;   movq    %r9, rsp(16 + virtual offset)
;   movq    0(%r8), %r10
;   movq    rsp(120 + virtual offset), %r8
;   movq    %r10, rsp(8 + virtual offset)
;   movq    0(%r8), %r11
;   movq    rsp(112 + virtual offset), %r8
;   movq    0(%r8), %r10
;   movq    rsp(104 + virtual offset), %r8
;   movq    0(%r8), %rax
;   movq    rsp(96 + virtual offset), %r8
;   movq    0(%r8), %rcx
;   movq    rsp(88 + virtual offset), %r8
;   movq    0(%r8), %rdx
;   movq    rsp(80 + virtual offset), %r8
;   movq    0(%r8), %rbx
;   movq    rsp(72 + virtual offset), %r8
;   movq    0(%r8), %r14
;   movq    rsp(64 + virtual offset), %r8
;   movq    0(%r8), %r12
;   movq    rsp(56 + virtual offset), %r8
;   movq    0(%r8), %r15
;   movq    rsp(48 + virtual offset), %r8
;   movq    0(%r8), %r13
;   movq    rsp(40 + virtual offset), %r8
;   movq    0(%r8), %r8
;   movq    rsp(32 + virtual offset), %r9
;   movq    0(%r9), %rsi
;   movq    %rdi, %r9
;   movq    rsp(16 + virtual offset), %rdi
;   lea     0(%r9,%rdi,1), %rdi
;   movq    rsp(8 + virtual offset), %r9
;   lea     0(%r9,%r11,1), %r9
;   lea     0(%rdi,%r9,1), %r9
;   lea     0(%r10,%rax,1), %r10
;   lea     0(%rcx,%rdx,1), %r11
;   lea     0(%rbx,%r14,1), %rdi
;   lea     0(%r11,%rdi,1), %r11
;   lea     0(%r12,%r15,1), %rdi
;   lea     0(%r11,%rdi,1), %r11
;   lea     0(%r13,%r8,1), %r8
;   lea     0(%r11,%r8,1), %r8
;   lea     0(%r10,%r8,1), %r8
;   lea     0(%r9,%r8,1), %r8
;   movq    rsp(24 + virtual offset), %r9
;   addq (%r9), %rsi
;   movq    rsp(0 + virtual offset), %rdi
;   lea     0(%rsi,%rdi,1), %r9
;   lea     0(%r8,%r9,1), %rax
;   movq    128(%rsp), %rbx
;   movq    136(%rsp), %r12
;   movq    144(%rsp), %r13
;   movq    152(%rsp), %r14
;   movq    160(%rsp), %r15
;   addq    %rsp, $176, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0xb0, %rsp
;   movq %rbx, 0x80(%rsp)
;   movq %r12, 0x88(%rsp)
;   movq %r13, 0x90(%rsp)
;   movq %r14, 0x98(%rsp)
;   movq %r15, 0xa0(%rsp)
; block1: ; offset 0x33
;   movq (%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x10(%rsp)
;   movq 8(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 8(%rsp)
;   movq 0x10(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x78(%rsp)
;   movq 0x18(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x70(%rsp)
;   movq 0x20(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x68(%rsp)
;   movq 0x28(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x60(%rsp)
;   movq 0x30(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x58(%rsp)
;   movq 0x38(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x50(%rsp)
;   movq 0x40(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x48(%rsp)
;   movq 0x48(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x40(%rsp)
;   movq 0x50(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x38(%rsp)
;   movq 0x58(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x30(%rsp)
;   movq 0x60(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x28(%rsp)
;   movq 0x68(%rdi), %r8 ; trap: heap_oob
;   movq %r8, 0x20(%rsp)
;   movq 0x70(%rdi), %r8 ; trap: heap_oob
;   movq %rdi, (%rsp)
;   movq %r8, 0x18(%rsp)
;   xorq %rdi, %rdi
;   movq (%rsi), %rax
;   movq %rsp, (%rsi)
;   movq %rax, %rsp
;   movq 8(%rsi), %rax
;   movq %rbp, 8(%rsi)
;   movq %rax, %rbp
;   movq 0x10(%rsi), %rax
;   leaq 6(%rip), %rcx
;   movq %rcx, 0x10(%rsi)
;   jmpq *%rax
;   movq 0x10(%rsp), %r8
;   movq (%r8), %r9 ; trap: heap_oob
;   movq 8(%rsp), %r8
;   movq %r9, 0x10(%rsp)
;   movq (%r8), %r10 ; trap: heap_oob
;   movq 0x78(%rsp), %r8
;   movq %r10, 8(%rsp)
;   movq (%r8), %r11 ; trap: heap_oob
;   movq 0x70(%rsp), %r8
;   movq (%r8), %r10 ; trap: heap_oob
;   movq 0x68(%rsp), %r8
;   movq (%r8), %rax ; trap: heap_oob
;   movq 0x60(%rsp), %r8
;   movq (%r8), %rcx ; trap: heap_oob
;   movq 0x58(%rsp), %r8
;   movq (%r8), %rdx ; trap: heap_oob
;   movq 0x50(%rsp), %r8
;   movq (%r8), %rbx ; trap: heap_oob
;   movq 0x48(%rsp), %r8
;   movq (%r8), %r14 ; trap: heap_oob
;   movq 0x40(%rsp), %r8
;   movq (%r8), %r12 ; trap: heap_oob
;   movq 0x38(%rsp), %r8
;   movq (%r8), %r15 ; trap: heap_oob
;   movq 0x30(%rsp), %r8
;   movq (%r8), %r13 ; trap: heap_oob
;   movq 0x28(%rsp), %r8
;   movq (%r8), %r8 ; trap: heap_oob
;   movq 0x20(%rsp), %r9
;   movq (%r9), %rsi ; trap: heap_oob
;   movq %rdi, %r9
;   movq 0x10(%rsp), %rdi
;   addq %r9, %rdi
;   movq 8(%rsp), %r9
;   addq %r11, %r9
;   addq %rdi, %r9
;   addq %rax, %r10
;   leaq (%rcx, %rdx), %r11
;   leaq (%rbx, %r14), %rdi
;   addq %rdi, %r11
;   leaq (%r12, %r15), %rdi
;   addq %rdi, %r11
;   addq %r13, %r8
;   addq %r11, %r8
;   addq %r10, %r8
;   addq %r9, %r8
;   movq 0x18(%rsp), %r9
;   addq (%r9), %rsi ; trap: heap_oob
;   movq (%rsp), %rdi
;   leaq (%rsi, %rdi), %r9
;   leaq (%r8, %r9), %rax
;   movq 0x80(%rsp), %rbx
;   movq 0x88(%rsp), %r12
;   movq 0x90(%rsp), %r13
;   movq 0x98(%rsp), %r14
;   movq 0xa0(%rsp), %r15
;   addq $0xb0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

;; Test clobbering of all 16 float registers
function %switch_float_clobber(i64, i64) -> f64 {
block0(v0: i64, v1: i64):
  v2 = iconst.i64 0

;; We create values v100 to v115 before the stack_switch and want to use them
;; afterwards. Thus, they must all be spilled on the stack.
;; Given that they are loads, they cannot be moved behind the stack_switch.

  v100 = load.f64 v0+0
  v101 = load.f64 v0+8
  v102 = load.f64 v0+16
  v103 = load.f64 v0+24
  v104 = load.f64 v0+32
  v105 = load.f64 v0+40
  v106 = load.f64 v0+48
  v107 = load.f64 v0+56
  v108 = load.f64 v0+64
  v109 = load.f64 v0+72
  v110 = load.f64 v0+80
  v111 = load.f64 v0+88
  v112 = load.f64 v0+96
  v113 = load.f64 v0+104
  v114 = load.f64 v0+112
  v115 = load.f64 v0+120

  v198 = stack_switch v1, v1, v2
  v199 = fcvt_from_uint.f64 v198

;; We add v100 to v115, using v200 to v215 as an accumulator
;; We make v199 part of the result to prevent summation of v100 to v115 before
;; the switch.

  v200 = fadd v199, v100
  v201 = fadd v200, v101
  v202 = fadd v201, v102
  v203 = fadd v202, v103
  v204 = fadd v203, v104
  v205 = fadd v204, v105
  v206 = fadd v205, v106
  v207 = fadd v206, v107
  v208 = fadd v207, v108
  v209 = fadd v208, v109
  v210 = fadd v209, v110
  v211 = fadd v210, v111
  v212 = fadd v211, v112
  v213 = fadd v212, v113
  v214 = fadd v213, v114
  v215 = fadd v214, v115

  return v215
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $304, %rsp
;   movq    %rbx, 256(%rsp)
;   movq    %r12, 264(%rsp)
;   movq    %r13, 272(%rsp)
;   movq    %r14, 280(%rsp)
;   movq    %r15, 288(%rsp)
; block0:
;   movsd   0(%rdi), %xmm0
;   movdqu  %xmm0, rsp(240 + virtual offset)
;   movsd   8(%rdi), %xmm0
;   movdqu  %xmm0, rsp(224 + virtual offset)
;   movsd   16(%rdi), %xmm0
;   movdqu  %xmm0, rsp(208 + virtual offset)
;   movsd   24(%rdi), %xmm0
;   movdqu  %xmm0, rsp(192 + virtual offset)
;   movsd   32(%rdi), %xmm0
;   movdqu  %xmm0, rsp(176 + virtual offset)
;   movsd   40(%rdi), %xmm0
;   movdqu  %xmm0, rsp(160 + virtual offset)
;   movsd   48(%rdi), %xmm0
;   movdqu  %xmm0, rsp(144 + virtual offset)
;   movsd   56(%rdi), %xmm0
;   movdqu  %xmm0, rsp(128 + virtual offset)
;   movsd   64(%rdi), %xmm0
;   movdqu  %xmm0, rsp(112 + virtual offset)
;   movsd   72(%rdi), %xmm0
;   movdqu  %xmm0, rsp(96 + virtual offset)
;   movsd   80(%rdi), %xmm0
;   movdqu  %xmm0, rsp(80 + virtual offset)
;   movsd   88(%rdi), %xmm0
;   movdqu  %xmm0, rsp(64 + virtual offset)
;   movsd   96(%rdi), %xmm0
;   movdqu  %xmm0, rsp(48 + virtual offset)
;   movsd   104(%rdi), %xmm0
;   movdqu  %xmm0, rsp(32 + virtual offset)
;   movsd   112(%rdi), %xmm0
;   movdqu  %xmm0, rsp(16 + virtual offset)
;   movsd   120(%rdi), %xmm0
;   movdqu  %xmm0, rsp(0 + virtual offset)
;   uninit  %rdi
;   xorq %rdi, %rdi
;   %rdi = stack_switch_basic %rsi, %rsi, %rdi
;   u64_to_f64_seq %rdi, %xmm0, %rcx, %rdx
;   movdqu  rsp(240 + virtual offset), %xmm5
;   addsd   %xmm0, %xmm5, %xmm0
;   movdqu  rsp(224 + virtual offset), %xmm1
;   addsd   %xmm0, %xmm1, %xmm0
;   movdqu  rsp(208 + virtual offset), %xmm3
;   addsd   %xmm0, %xmm3, %xmm0
;   movdqu  rsp(192 + virtual offset), %xmm6
;   addsd   %xmm0, %xmm6, %xmm0
;   movdqu  rsp(176 + virtual offset), %xmm1
;   addsd   %xmm0, %xmm1, %xmm0
;   movdqu  rsp(160 + virtual offset), %xmm4
;   addsd   %xmm0, %xmm4, %xmm0
;   movdqu  rsp(144 + virtual offset), %xmm7
;   addsd   %xmm0, %xmm7, %xmm0
;   movdqu  rsp(128 + virtual offset), %xmm2
;   addsd   %xmm0, %xmm2, %xmm0
;   movdqu  rsp(112 + virtual offset), %xmm5
;   addsd   %xmm0, %xmm5, %xmm0
;   movdqu  rsp(96 + virtual offset), %xmm1
;   addsd   %xmm0, %xmm1, %xmm0
;   movdqu  rsp(80 + virtual offset), %xmm3
;   addsd   %xmm0, %xmm3, %xmm0
;   movdqu  rsp(64 + virtual offset), %xmm6
;   addsd   %xmm0, %xmm6, %xmm0
;   movdqu  rsp(48 + virtual offset), %xmm1
;   addsd   %xmm0, %xmm1, %xmm0
;   movdqu  rsp(32 + virtual offset), %xmm4
;   addsd   %xmm0, %xmm4, %xmm0
;   movdqu  rsp(16 + virtual offset), %xmm7
;   addsd   %xmm0, %xmm7, %xmm0
;   movdqu  rsp(0 + virtual offset), %xmm2
;   addsd   %xmm0, %xmm2, %xmm0
;   movq    256(%rsp), %rbx
;   movq    264(%rsp), %r12
;   movq    272(%rsp), %r13
;   movq    280(%rsp), %r14
;   movq    288(%rsp), %r15
;   addq    %rsp, $304, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x130, %rsp
;   movq %rbx, 0x100(%rsp)
;   movq %r12, 0x108(%rsp)
;   movq %r13, 0x110(%rsp)
;   movq %r14, 0x118(%rsp)
;   movq %r15, 0x120(%rsp)
; block1: ; offset 0x33
;   movsd (%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0xf0(%rsp)
;   movsd 8(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0xe0(%rsp)
;   movsd 0x10(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0xd0(%rsp)
;   movsd 0x18(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0xc0(%rsp)
;   movsd 0x20(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0xb0(%rsp)
;   movsd 0x28(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0xa0(%rsp)
;   movsd 0x30(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x90(%rsp)
;   movsd 0x38(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x80(%rsp)
;   movsd 0x40(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x70(%rsp)
;   movsd 0x48(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x60(%rsp)
;   movsd 0x50(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x50(%rsp)
;   movsd 0x58(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x40(%rsp)
;   movsd 0x60(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x30(%rsp)
;   movsd 0x68(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x20(%rsp)
;   movsd 0x70(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, 0x10(%rsp)
;   movsd 0x78(%rdi), %xmm0 ; trap: heap_oob
;   movdqu %xmm0, (%rsp)
;   xorq %rdi, %rdi
;   movq (%rsi), %rax
;   movq %rsp, (%rsi)
;   movq %rax, %rsp
;   movq 8(%rsi), %rax
;   movq %rbp, 8(%rsi)
;   movq %rax, %rbp
;   movq 0x10(%rsi), %rax
;   leaq 6(%rip), %rcx
;   movq %rcx, 0x10(%rsi)
;   jmpq *%rax
;   cmpq $0, %rdi
;   jl 0x135
;   cvtsi2sdq %rdi, %xmm0
;   jmp 0x14f
;   movq %rdi, %rcx
;   shrq $1, %rcx
;   movq %rdi, %rdx
;   andq $1, %rdx
;   orq %rcx, %rdx
;   cvtsi2sdq %rdx, %xmm0
;   addsd %xmm0, %xmm0
;   movdqu 0xf0(%rsp), %xmm5
;   addsd %xmm5, %xmm0
;   movdqu 0xe0(%rsp), %xmm1
;   addsd %xmm1, %xmm0
;   movdqu 0xd0(%rsp), %xmm3
;   addsd %xmm3, %xmm0
;   movdqu 0xc0(%rsp), %xmm6
;   addsd %xmm6, %xmm0
;   movdqu 0xb0(%rsp), %xmm1
;   addsd %xmm1, %xmm0
;   movdqu 0xa0(%rsp), %xmm4
;   addsd %xmm4, %xmm0
;   movdqu 0x90(%rsp), %xmm7
;   addsd %xmm7, %xmm0
;   movdqu 0x80(%rsp), %xmm2
;   addsd %xmm2, %xmm0
;   movdqu 0x70(%rsp), %xmm5
;   addsd %xmm5, %xmm0
;   movdqu 0x60(%rsp), %xmm1
;   addsd %xmm1, %xmm0
;   movdqu 0x50(%rsp), %xmm3
;   addsd %xmm3, %xmm0
;   movdqu 0x40(%rsp), %xmm6
;   addsd %xmm6, %xmm0
;   movdqu 0x30(%rsp), %xmm1
;   addsd %xmm1, %xmm0
;   movdqu 0x20(%rsp), %xmm4
;   addsd %xmm4, %xmm0
;   movdqu 0x10(%rsp), %xmm7
;   addsd %xmm7, %xmm0
;   movdqu (%rsp), %xmm2
;   addsd %xmm2, %xmm0
;   movq 0x100(%rsp), %rbx
;   movq 0x108(%rsp), %r12
;   movq 0x110(%rsp), %r13
;   movq 0x118(%rsp), %r14
;   movq 0x120(%rsp), %r15
;   addq $0x130, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

