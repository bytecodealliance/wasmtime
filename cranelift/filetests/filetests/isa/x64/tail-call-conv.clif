test compile precise-output
set enable_llvm_abi_extensions
set enable_multi_ret_implicit_sret
set preserve_frame_pointers
target x86_64

;; Test the `tail` calling convention with non-tail calls and stack arguments.

function %tail_callee_stack_args(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64, v6: i64, v7: i64, v8: i64, v9: i64, v10: i64, v11: i64, v12: i64, v13: i64, v14: i64):
    return v14
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   movq <offset:0>+-0x10(%rbp), %rax
;   movq %rbp, %rsp
;   popq %rbp
;   retq $0x50
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq 0x50(%rbp), %rax
;   movq %rbp, %rsp
;   popq %rbp
;   retq $0x50

function %tail_caller_stack_args() -> i64 {
    fn0 = %tail_callee_stack_args(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail

block0:
    v0 = iconst.i64 10
    v1 = iconst.i64 15
    v2 = iconst.i64 20
    v3 = iconst.i64 25
    v4 = iconst.i64 30
    v5 = iconst.i64 35
    v6 = iconst.i64 40
    v7 = iconst.i64 45
    v8 = iconst.i64 50
    v9 = iconst.i64 55
    v10 = iconst.i64 60
    v11 = iconst.i64 65
    v12 = iconst.i64 70
    v13 = iconst.i64 75
    v14 = iconst.i64 80
    v15 = call fn0(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14)
    return v15
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x90, %rsp
;   movq %rbx, 0x60(%rsp)
;   movq %r12, 0x68(%rsp)
;   movq %r13, 0x70(%rsp)
;   movq %r14, 0x78(%rsp)
;   movq %r15, 0x80(%rsp)
; block0:
;   movl $0xa, %esi
;   movq %rsi, <offset:1>+(%rsp)
;   movl $0xf, %esi
;   movl $0x14, %edx
;   movl $0x19, %ecx
;   movl $0x1e, %r8d
;   movl $0x23, %r9d
;   movl $0x28, %r10d
;   movl $0x2d, %r11d
;   movl $0x32, %eax
;   movl $0x37, %r12d
;   movl $0x3c, %r13d
;   movl $0x41, %r14d
;   movl $0x46, %r15d
;   movl $0x4b, %ebx
;   movl $0x50, %edi
;   movq %r10, (%rsp)
;   movq %r11, 8(%rsp)
;   movq %rax, 0x10(%rsp)
;   movq %r12, 0x18(%rsp)
;   movq %r13, 0x20(%rsp)
;   movq %r14, 0x28(%rsp)
;   movq %r15, 0x30(%rsp)
;   movq %rbx, 0x38(%rsp)
;   movq %rdi, 0x40(%rsp)
;   load_ext_name %tail_callee_stack_args+0, %r10
;   movq <offset:1>+(%rsp), %rdi
;   call    *%r10
;   movq 0x60(%rsp), %rbx
;   movq 0x68(%rsp), %r12
;   movq 0x70(%rsp), %r13
;   movq 0x78(%rsp), %r14
;   movq 0x80(%rsp), %r15
;   addq $0x90, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x90, %rsp
;   movq %rbx, 0x60(%rsp)
;   movq %r12, 0x68(%rsp)
;   movq %r13, 0x70(%rsp)
;   movq %r14, 0x78(%rsp)
;   movq %r15, 0x80(%rsp)
; block1: ; offset 0x27
;   movl $0xa, %esi
;   movq %rsi, 0x50(%rsp)
;   movl $0xf, %esi
;   movl $0x14, %edx
;   movl $0x19, %ecx
;   movl $0x1e, %r8d
;   movl $0x23, %r9d
;   movl $0x28, %r10d
;   movl $0x2d, %r11d
;   movl $0x32, %eax
;   movl $0x37, %r12d
;   movl $0x3c, %r13d
;   movl $0x41, %r14d
;   movl $0x46, %r15d
;   movl $0x4b, %ebx
;   movl $0x50, %edi
;   movq %r10, (%rsp)
;   movq %r11, 8(%rsp)
;   movq %rax, 0x10(%rsp)
;   movq %r12, 0x18(%rsp)
;   movq %r13, 0x20(%rsp)
;   movq %r14, 0x28(%rsp)
;   movq %r15, 0x30(%rsp)
;   movq %rbx, 0x38(%rsp)
;   movq %rdi, 0x40(%rsp)
;   movabsq $0, %r10 ; reloc_external Abs8 %tail_callee_stack_args 0
;   movq 0x50(%rsp), %rdi
;   callq *%r10
;   subq $0x50, %rsp
;   movq 0x60(%rsp), %rbx
;   movq 0x68(%rsp), %r12
;   movq 0x70(%rsp), %r13
;   movq 0x78(%rsp), %r14
;   movq 0x80(%rsp), %r15
;   addq $0x90, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

;; Test the `tail` calling convention with non-tail calls and stack returns.

function %tail_callee_stack_rets() -> i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 tail {
block0:
    v0 = iconst.i64 10
    v1 = iconst.i64 15
    v2 = iconst.i64 20
    v3 = iconst.i64 25
    v4 = iconst.i64 30
    v5 = iconst.i64 35
    v6 = iconst.i64 40
    v7 = iconst.i64 45
    v8 = iconst.i64 50
    v9 = iconst.i64 55
    v10 = iconst.i64 60
    v11 = iconst.i64 65
    v12 = iconst.i64 70
    v13 = iconst.i64 75
    v14 = iconst.i64 80
    v15 = iconst.i64 85
    v16 = iconst.i64 90
    v17 = iconst.i64 95
    v18 = iconst.i64 100
    v19 = iconst.i64 105
    v20 = iconst.i64 110
    v21 = iconst.i64 115
    v22 = iconst.i64 120
    v23 = iconst.i64 125
    v24 = iconst.i64 130
    v25 = iconst.i64 135
    return v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0xa0, %rsp
;   movq %rbx, 0x70(%rsp)
;   movq %r12, 0x78(%rsp)
;   movq %r13, 0x80(%rsp)
;   movq %r14, 0x88(%rsp)
;   movq %r15, 0x90(%rsp)
; block0:
;   movl $0xa, %ecx
;   movq %rcx, <offset:1>+0x68(%rsp)
;   movl $0xf, %edx
;   movq %rdx, <offset:1>+0x60(%rsp)
;   movl $0x14, %esi
;   movq %rsi, <offset:1>+0x58(%rsp)
;   movl $0x19, %r8d
;   movq %r8, <offset:1>+0x50(%rsp)
;   movl $0x1e, %eax
;   movq %rax, <offset:1>+0x48(%rsp)
;   movl $0x23, %r9d
;   movq %r9, <offset:1>+0x40(%rsp)
;   movl $0x28, %r10d
;   movq %r10, <offset:1>+0x38(%rsp)
;   movl $0x2d, %r10d
;   movq %r10, <offset:1>+0x30(%rsp)
;   movl $0x32, %r13d
;   movl $0x37, %r14d
;   movl $0x3c, %r15d
;   movl $0x41, %ebx
;   movl $0x46, %r12d
;   movl $0x4b, %r11d
;   movl $0x50, %eax
;   movl $0x55, %ecx
;   movl $0x5a, %edx
;   movl $0x5f, %esi
;   movq %rsi, <offset:1>+0x28(%rsp)
;   movl $0x64, %r8d
;   movl $0x69, %r9d
;   movl $0x6e, %r10d
;   movl $0x73, %esi
;   movq %rsi, <offset:1>+0x20(%rsp)
;   movl $0x78, %esi
;   movq %rsi, <offset:1>+0x18(%rsp)
;   movl $0x7d, %esi
;   movq %rsi, <offset:1>+0x10(%rsp)
;   movl $0x82, %esi
;   movq %rsi, <offset:1>+8(%rsp)
;   movl $0x87, %esi
;   movq %rsi, <offset:1>+(%rsp)
;   movq %r13, (%rdi)
;   movq %r14, 8(%rdi)
;   movq %r15, 0x10(%rdi)
;   movq %rbx, 0x18(%rdi)
;   movq %r12, 0x20(%rdi)
;   movq %r11, 0x28(%rdi)
;   movq %rax, 0x30(%rdi)
;   movq %rcx, 0x38(%rdi)
;   movq %rdx, 0x40(%rdi)
;   movq <offset:1>+0x28(%rsp), %rax
;   movq %rax, 0x48(%rdi)
;   movq %r8, 0x50(%rdi)
;   movq %r9, 0x58(%rdi)
;   movq %r10, 0x60(%rdi)
;   movq <offset:1>+0x20(%rsp), %rsi
;   movq %rsi, 0x68(%rdi)
;   movq <offset:1>+0x18(%rsp), %rsi
;   movq %rsi, 0x70(%rdi)
;   movq <offset:1>+0x10(%rsp), %rsi
;   movq %rsi, 0x78(%rdi)
;   movq <offset:1>+8(%rsp), %rsi
;   movq %rsi, 0x80(%rdi)
;   movq <offset:1>+(%rsp), %rsi
;   movq %rsi, 0x88(%rdi)
;   movq <offset:1>+0x68(%rsp), %rax
;   movq <offset:1>+0x60(%rsp), %rcx
;   movq <offset:1>+0x58(%rsp), %rdx
;   movq <offset:1>+0x50(%rsp), %rsi
;   movq <offset:1>+0x48(%rsp), %rdi
;   movq <offset:1>+0x40(%rsp), %r8
;   movq <offset:1>+0x38(%rsp), %r9
;   movq <offset:1>+0x30(%rsp), %r10
;   movq 0x70(%rsp), %rbx
;   movq 0x78(%rsp), %r12
;   movq 0x80(%rsp), %r13
;   movq 0x88(%rsp), %r14
;   movq 0x90(%rsp), %r15
;   addq $0xa0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0xa0, %rsp
;   movq %rbx, 0x70(%rsp)
;   movq %r12, 0x78(%rsp)
;   movq %r13, 0x80(%rsp)
;   movq %r14, 0x88(%rsp)
;   movq %r15, 0x90(%rsp)
; block1: ; offset 0x2d
;   movl $0xa, %ecx
;   movq %rcx, 0x68(%rsp)
;   movl $0xf, %edx
;   movq %rdx, 0x60(%rsp)
;   movl $0x14, %esi
;   movq %rsi, 0x58(%rsp)
;   movl $0x19, %r8d
;   movq %r8, 0x50(%rsp)
;   movl $0x1e, %eax
;   movq %rax, 0x48(%rsp)
;   movl $0x23, %r9d
;   movq %r9, 0x40(%rsp)
;   movl $0x28, %r10d
;   movq %r10, 0x38(%rsp)
;   movl $0x2d, %r10d
;   movq %r10, 0x30(%rsp)
;   movl $0x32, %r13d
;   movl $0x37, %r14d
;   movl $0x3c, %r15d
;   movl $0x41, %ebx
;   movl $0x46, %r12d
;   movl $0x4b, %r11d
;   movl $0x50, %eax
;   movl $0x55, %ecx
;   movl $0x5a, %edx
;   movl $0x5f, %esi
;   movq %rsi, 0x28(%rsp)
;   movl $0x64, %r8d
;   movl $0x69, %r9d
;   movl $0x6e, %r10d
;   movl $0x73, %esi
;   movq %rsi, 0x20(%rsp)
;   movl $0x78, %esi
;   movq %rsi, 0x18(%rsp)
;   movl $0x7d, %esi
;   movq %rsi, 0x10(%rsp)
;   movl $0x82, %esi
;   movq %rsi, 8(%rsp)
;   movl $0x87, %esi
;   movq %rsi, (%rsp)
;   movq %r13, (%rdi)
;   movq %r14, 8(%rdi)
;   movq %r15, 0x10(%rdi)
;   movq %rbx, 0x18(%rdi)
;   movq %r12, 0x20(%rdi)
;   movq %r11, 0x28(%rdi)
;   movq %rax, 0x30(%rdi)
;   movq %rcx, 0x38(%rdi)
;   movq %rdx, 0x40(%rdi)
;   movq 0x28(%rsp), %rax
;   movq %rax, 0x48(%rdi)
;   movq %r8, 0x50(%rdi)
;   movq %r9, 0x58(%rdi)
;   movq %r10, 0x60(%rdi)
;   movq 0x20(%rsp), %rsi
;   movq %rsi, 0x68(%rdi)
;   movq 0x18(%rsp), %rsi
;   movq %rsi, 0x70(%rdi)
;   movq 0x10(%rsp), %rsi
;   movq %rsi, 0x78(%rdi)
;   movq 8(%rsp), %rsi
;   movq %rsi, 0x80(%rdi)
;   movq (%rsp), %rsi
;   movq %rsi, 0x88(%rdi)
;   movq 0x68(%rsp), %rax
;   movq 0x60(%rsp), %rcx
;   movq 0x58(%rsp), %rdx
;   movq 0x50(%rsp), %rsi
;   movq 0x48(%rsp), %rdi
;   movq 0x40(%rsp), %r8
;   movq 0x38(%rsp), %r9
;   movq 0x30(%rsp), %r10
;   movq 0x70(%rsp), %rbx
;   movq 0x78(%rsp), %r12
;   movq 0x80(%rsp), %r13
;   movq 0x88(%rsp), %r14
;   movq 0x90(%rsp), %r15
;   addq $0xa0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %tail_caller_stack_rets() -> i64 tail {
    fn0 = colocated %tail_callee_stack_rets() -> i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 tail

block0:
    v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25 = call fn0()
    return v25
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x130, %rsp
;   movq %rbx, 0x100(%rsp)
;   movq %r12, 0x108(%rsp)
;   movq %r13, 0x110(%rsp)
;   movq %r14, 0x118(%rsp)
;   movq %r15, 0x120(%rsp)
; block0:
;   leaq (%rsp), %rdi
;   call    TestCase(%tail_callee_stack_rets)
;   movq <offset:1>+0x60(%rsp), %rax
;   movq 0x100(%rsp), %rbx
;   movq 0x108(%rsp), %r12
;   movq 0x110(%rsp), %r13
;   movq 0x118(%rsp), %r14
;   movq 0x120(%rsp), %r15
;   addq $0x130, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x130, %rsp
;   movq %rbx, 0x100(%rsp)
;   movq %r12, 0x108(%rsp)
;   movq %r13, 0x110(%rsp)
;   movq %r14, 0x118(%rsp)
;   movq %r15, 0x120(%rsp)
; block1: ; offset 0x33
;   leaq (%rsp), %rdi
;   callq 0x3c ; reloc_external CallPCRel4 %tail_callee_stack_rets -4
;   movq (%rsp), %r11
;   movq %r11, 0x90(%rsp)
;   movq 8(%rsp), %r11
;   movq %r11, 0x98(%rsp)
;   movq 0x10(%rsp), %r11
;   movq %r11, 0xa0(%rsp)
;   movq 0x18(%rsp), %r11
;   movq %r11, 0xa8(%rsp)
;   movq 0x20(%rsp), %r11
;   movq %r11, 0xb0(%rsp)
;   movq 0x28(%rsp), %r11
;   movq %r11, 0xb8(%rsp)
;   movq 0x30(%rsp), %r11
;   movq %r11, 0xc0(%rsp)
;   movq 0x38(%rsp), %r11
;   movq %r11, 0xc8(%rsp)
;   movq 0x40(%rsp), %r11
;   movq %r11, 0xd0(%rsp)
;   movq 0x48(%rsp), %r11
;   movq %r11, 0xd8(%rsp)
;   movq 0x50(%rsp), %r11
;   movq %r11, 0xe0(%rsp)
;   movq 0x58(%rsp), %r11
;   movq %r11, 0xe8(%rsp)
;   movq 0x60(%rsp), %rbx
;   movq 0x68(%rsp), %r12
;   movq 0x70(%rsp), %r13
;   movq 0x78(%rsp), %r14
;   movq 0x80(%rsp), %r15
;   movq 0x88(%rsp), %r11
;   movq %r11, 0xf0(%rsp)
;   movq 0xf0(%rsp), %rax
;   movq 0x100(%rsp), %rbx
;   movq 0x108(%rsp), %r12
;   movq 0x110(%rsp), %r13
;   movq 0x118(%rsp), %r14
;   movq 0x120(%rsp), %r15
;   addq $0x130, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

;; Test the `tail` calling convention with non-tail calls and both stack
;; arguments and stack returns.

function %tail_callee_stack_args_and_rets(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 tail {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64, v6: i64, v7: i64, v8: i64, v9: i64, v10: i64, v11: i64, v12: i64, v13: i64, v14: i64, v15: i64, v16: i64, v17: i64, v18: i64, v19: i64, v20: i64, v21: i64, v22: i64, v23: i64, v24: i64, v25: i64):
    return v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0xa0, %rsp
;   movq %rbx, 0x70(%rsp)
;   movq %r12, 0x78(%rsp)
;   movq %r13, 0x80(%rsp)
;   movq %r14, 0x88(%rsp)
;   movq %r15, 0x90(%rsp)
; block0:
;   movq %rsi, <offset:1>+(%rsp)
;   movq %rdx, <offset:1>+8(%rsp)
;   movq %rcx, <offset:1>+0x10(%rsp)
;   movq %r8, <offset:1>+0x18(%rsp)
;   movq %r9, <offset:1>+0x20(%rsp)
;   movq <offset:0>+-0xb0(%rbp), %r9
;   movq %r9, <offset:1>+0x28(%rsp)
;   movq <offset:0>+-0xa8(%rbp), %r10
;   movq %r10, <offset:1>+0x30(%rsp)
;   movq <offset:0>+-0xa0(%rbp), %r10
;   movq %r10, <offset:1>+0x38(%rsp)
;   movq <offset:0>+-0x98(%rbp), %rcx
;   movq %rcx, <offset:1>+0x40(%rsp)
;   movq <offset:0>+-0x90(%rbp), %r8
;   movq %r8, <offset:1>+0x48(%rsp)
;   movq <offset:0>+-0x88(%rbp), %r10
;   movq %r10, <offset:1>+0x50(%rsp)
;   movq <offset:0>+-0x80(%rbp), %rsi
;   movq %rsi, <offset:1>+0x58(%rsp)
;   movq <offset:0>+-0x78(%rbp), %rax
;   movq %rax, <offset:1>+0x60(%rsp)
;   movq <offset:0>+-0x70(%rbp), %r10
;   movq <offset:0>+-0x68(%rbp), %r9
;   movq <offset:0>+-0x60(%rbp), %r8
;   movq <offset:0>+-0x58(%rbp), %rdx
;   movq <offset:0>+-0x50(%rbp), %rcx
;   movq %rcx, <offset:1>+0x68(%rsp)
;   movq <offset:0>+-0x48(%rbp), %rsi
;   movq <offset:0>+-0x40(%rbp), %r15
;   movq <offset:0>+-0x38(%rbp), %r12
;   movq <offset:0>+-0x30(%rbp), %r14
;   movq <offset:0>+-0x28(%rbp), %rbx
;   movq <offset:0>+-0x20(%rbp), %r13
;   movq <offset:0>+-0x18(%rbp), %r11
;   movq <offset:0>+-0x10(%rbp), %rax
;   movq <offset:1>+0x40(%rsp), %rcx
;   movq %rcx, (%rdi)
;   movq <offset:1>+0x48(%rsp), %rcx
;   movq %rcx, 8(%rdi)
;   movq <offset:1>+0x50(%rsp), %rcx
;   movq %rcx, 0x10(%rdi)
;   movq <offset:1>+0x58(%rsp), %rcx
;   movq %rcx, 0x18(%rdi)
;   movq <offset:1>+0x60(%rsp), %rcx
;   movq %rcx, 0x20(%rdi)
;   movq %r10, 0x28(%rdi)
;   movq %r9, 0x30(%rdi)
;   movq %r8, 0x38(%rdi)
;   movq %rdx, 0x40(%rdi)
;   movq <offset:1>+0x68(%rsp), %rdx
;   movq %rdx, 0x48(%rdi)
;   movq %rsi, 0x50(%rdi)
;   movq %r15, 0x58(%rdi)
;   movq %r12, 0x60(%rdi)
;   movq %r14, 0x68(%rdi)
;   movq %rbx, 0x70(%rdi)
;   movq %r13, 0x78(%rdi)
;   movq %r11, 0x80(%rdi)
;   movq %rax, 0x88(%rdi)
;   movq <offset:1>+(%rsp), %rax
;   movq <offset:1>+8(%rsp), %rcx
;   movq <offset:1>+0x10(%rsp), %rdx
;   movq <offset:1>+0x18(%rsp), %rsi
;   movq <offset:1>+0x20(%rsp), %rdi
;   movq <offset:1>+0x28(%rsp), %r8
;   movq <offset:1>+0x30(%rsp), %r9
;   movq <offset:1>+0x38(%rsp), %r10
;   movq 0x70(%rsp), %rbx
;   movq 0x78(%rsp), %r12
;   movq 0x80(%rsp), %r13
;   movq 0x88(%rsp), %r14
;   movq 0x90(%rsp), %r15
;   addq $0xa0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq $0xb0
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0xa0, %rsp
;   movq %rbx, 0x70(%rsp)
;   movq %r12, 0x78(%rsp)
;   movq %r13, 0x80(%rsp)
;   movq %r14, 0x88(%rsp)
;   movq %r15, 0x90(%rsp)
; block1: ; offset 0x2d
;   movq %rsi, (%rsp)
;   movq %rdx, 8(%rsp)
;   movq %rcx, 0x10(%rsp)
;   movq %r8, 0x18(%rsp)
;   movq %r9, 0x20(%rsp)
;   movq 0x10(%rbp), %r9
;   movq %r9, 0x28(%rsp)
;   movq 0x18(%rbp), %r10
;   movq %r10, 0x30(%rsp)
;   movq 0x20(%rbp), %r10
;   movq %r10, 0x38(%rsp)
;   movq 0x28(%rbp), %rcx
;   movq %rcx, 0x40(%rsp)
;   movq 0x30(%rbp), %r8
;   movq %r8, 0x48(%rsp)
;   movq 0x38(%rbp), %r10
;   movq %r10, 0x50(%rsp)
;   movq 0x40(%rbp), %rsi
;   movq %rsi, 0x58(%rsp)
;   movq 0x48(%rbp), %rax
;   movq %rax, 0x60(%rsp)
;   movq 0x50(%rbp), %r10
;   movq 0x58(%rbp), %r9
;   movq 0x60(%rbp), %r8
;   movq 0x68(%rbp), %rdx
;   movq 0x70(%rbp), %rcx
;   movq %rcx, 0x68(%rsp)
;   movq 0x78(%rbp), %rsi
;   movq 0x80(%rbp), %r15
;   movq 0x88(%rbp), %r12
;   movq 0x90(%rbp), %r14
;   movq 0x98(%rbp), %rbx
;   movq 0xa0(%rbp), %r13
;   movq 0xa8(%rbp), %r11
;   movq 0xb0(%rbp), %rax
;   movq 0x40(%rsp), %rcx
;   movq %rcx, (%rdi)
;   movq 0x48(%rsp), %rcx
;   movq %rcx, 8(%rdi)
;   movq 0x50(%rsp), %rcx
;   movq %rcx, 0x10(%rdi)
;   movq 0x58(%rsp), %rcx
;   movq %rcx, 0x18(%rdi)
;   movq 0x60(%rsp), %rcx
;   movq %rcx, 0x20(%rdi)
;   movq %r10, 0x28(%rdi)
;   movq %r9, 0x30(%rdi)
;   movq %r8, 0x38(%rdi)
;   movq %rdx, 0x40(%rdi)
;   movq 0x68(%rsp), %rdx
;   movq %rdx, 0x48(%rdi)
;   movq %rsi, 0x50(%rdi)
;   movq %r15, 0x58(%rdi)
;   movq %r12, 0x60(%rdi)
;   movq %r14, 0x68(%rdi)
;   movq %rbx, 0x70(%rdi)
;   movq %r13, 0x78(%rdi)
;   movq %r11, 0x80(%rdi)
;   movq %rax, 0x88(%rdi)
;   movq (%rsp), %rax
;   movq 8(%rsp), %rcx
;   movq 0x10(%rsp), %rdx
;   movq 0x18(%rsp), %rsi
;   movq 0x20(%rsp), %rdi
;   movq 0x28(%rsp), %r8
;   movq 0x30(%rsp), %r9
;   movq 0x38(%rsp), %r10
;   movq 0x70(%rsp), %rbx
;   movq 0x78(%rsp), %r12
;   movq 0x80(%rsp), %r13
;   movq 0x88(%rsp), %r14
;   movq 0x90(%rsp), %r15
;   addq $0xa0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq $0xb0

function %tail_caller_stack_args_and_rets() -> i64 tail {
    fn0 = %tail_callee_stack_args_and_rets(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64 tail

block0:
    v0 = iconst.i64 10
    v1 = iconst.i64 15
    v2 = iconst.i64 20
    v3 = iconst.i64 25
    v4 = iconst.i64 30
    v5 = iconst.i64 35
    v6 = iconst.i64 40
    v7 = iconst.i64 45
    v8 = iconst.i64 50
    v9 = iconst.i64 55
    v10 = iconst.i64 60
    v11 = iconst.i64 65
    v12 = iconst.i64 70
    v13 = iconst.i64 75
    v14 = iconst.i64 80
    v15 = iconst.i64 85
    v16 = iconst.i64 90
    v17 = iconst.i64 95
    v18 = iconst.i64 100
    v19 = iconst.i64 105
    v20 = iconst.i64 110
    v21 = iconst.i64 115
    v22 = iconst.i64 120
    v23 = iconst.i64 125
    v24 = iconst.i64 130
    v25 = iconst.i64 135
    v26, v27, v28, v29, v30, v31, v32, v33, v34, v35, v36, v37, v38, v39, v40, v41, v42, v43, v44, v45, v46, v47, v48, v49, v50, v51 = call fn0(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25)
    return v51
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x1e0, %rsp
;   movq %rbx, 0x1b0(%rsp)
;   movq %r12, 0x1b8(%rsp)
;   movq %r13, 0x1c0(%rsp)
;   movq %r14, 0x1c8(%rsp)
;   movq %r15, 0x1d0(%rsp)
; block0:
;   movl $0xa, %edx
;   movq %rdx, <offset:1>+0x58(%rsp)
;   movl $0xf, %ecx
;   movq %rcx, <offset:1>+0x50(%rsp)
;   movl $0x14, %r8d
;   movq %r8, <offset:1>+0x48(%rsp)
;   movl $0x19, %r9d
;   movq %r9, <offset:1>+0x40(%rsp)
;   movl $0x1e, %r9d
;   movq %r9, <offset:1>+0x38(%rsp)
;   movl $0x23, %esi
;   movq %rsi, <offset:1>+0x30(%rsp)
;   movl $0x28, %edi
;   movl $0x2d, %eax
;   movl $0x32, %r10d
;   movl $0x37, %r14d
;   movl $0x3c, %r15d
;   movl $0x41, %ebx
;   movl $0x46, %r12d
;   movl $0x4b, %r13d
;   movl $0x50, %esi
;   movl $0x55, %edx
;   movl $0x5a, %ecx
;   movl $0x5f, %r8d
;   movl $0x64, %r9d
;   movl $0x69, %r11d
;   movq %r11, <offset:1>+0x28(%rsp)
;   movl $0x6e, %r11d
;   movq %r11, <offset:1>+0x20(%rsp)
;   movl $0x73, %r11d
;   movq %r11, <offset:1>+0x18(%rsp)
;   movl $0x78, %r11d
;   movq %r11, <offset:1>+0x10(%rsp)
;   movl $0x7d, %r11d
;   movq %r11, <offset:1>+8(%rsp)
;   movl $0x82, %r11d
;   movq %r11, <offset:1>+(%rsp)
;   movl $0x87, %r11d
;   movq %r11, <offset:1>+0x60(%rsp)
;   movq <offset:1>+0x30(%rsp), %r11
;   movq %r11, (%rsp)
;   movq %rdi, 8(%rsp)
;   movq %rax, 0x10(%rsp)
;   movq %r10, 0x18(%rsp)
;   movq %r14, 0x20(%rsp)
;   movq %r15, 0x28(%rsp)
;   movq %rbx, 0x30(%rsp)
;   movq %r12, 0x38(%rsp)
;   movq %r13, 0x40(%rsp)
;   movq %rsi, 0x48(%rsp)
;   movq %rdx, 0x50(%rsp)
;   movq %rcx, 0x58(%rsp)
;   movq %r8, 0x60(%rsp)
;   movq %r9, 0x68(%rsp)
;   movq <offset:1>+0x28(%rsp), %r11
;   movq %r11, 0x70(%rsp)
;   movq <offset:1>+0x20(%rsp), %r11
;   movq %r11, 0x78(%rsp)
;   movq <offset:1>+0x18(%rsp), %r11
;   movq %r11, 0x80(%rsp)
;   movq <offset:1>+0x10(%rsp), %r11
;   movq %r11, 0x88(%rsp)
;   movq <offset:1>+8(%rsp), %r11
;   movq %r11, 0x90(%rsp)
;   movq <offset:1>+(%rsp), %r11
;   movq %r11, 0x98(%rsp)
;   movq <offset:1>+0x60(%rsp), %r11
;   movq %r11, 0xa0(%rsp)
;   leaq 0xb0(%rsp), %rdi
;   load_ext_name %tail_callee_stack_args_and_rets+0, %r10
;   movq <offset:1>+0x48(%rsp), %rcx
;   movq <offset:1>+0x50(%rsp), %rdx
;   movq <offset:1>+0x58(%rsp), %rsi
;   movq <offset:1>+0x40(%rsp), %r8
;   movq <offset:1>+0x38(%rsp), %r9
;   call    *%r10
;   movq <offset:1>+0x60(%rsp), %rax
;   movq 0x1b0(%rsp), %rbx
;   movq 0x1b8(%rsp), %r12
;   movq 0x1c0(%rsp), %r13
;   movq 0x1c8(%rsp), %r14
;   movq 0x1d0(%rsp), %r15
;   addq $0x1e0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x1e0, %rsp
;   movq %rbx, 0x1b0(%rsp)
;   movq %r12, 0x1b8(%rsp)
;   movq %r13, 0x1c0(%rsp)
;   movq %r14, 0x1c8(%rsp)
;   movq %r15, 0x1d0(%rsp)
; block1: ; offset 0x33
;   movl $0xa, %edx
;   movq %rdx, 0x198(%rsp)
;   movl $0xf, %ecx
;   movq %rcx, 0x190(%rsp)
;   movl $0x14, %r8d
;   movq %r8, 0x188(%rsp)
;   movl $0x19, %r9d
;   movq %r9, 0x180(%rsp)
;   movl $0x1e, %r9d
;   movq %r9, 0x178(%rsp)
;   movl $0x23, %esi
;   movq %rsi, 0x170(%rsp)
;   movl $0x28, %edi
;   movl $0x2d, %eax
;   movl $0x32, %r10d
;   movl $0x37, %r14d
;   movl $0x3c, %r15d
;   movl $0x41, %ebx
;   movl $0x46, %r12d
;   movl $0x4b, %r13d
;   movl $0x50, %esi
;   movl $0x55, %edx
;   movl $0x5a, %ecx
;   movl $0x5f, %r8d
;   movl $0x64, %r9d
;   movl $0x69, %r11d
;   movq %r11, 0x168(%rsp)
;   movl $0x6e, %r11d
;   movq %r11, 0x160(%rsp)
;   movl $0x73, %r11d
;   movq %r11, 0x158(%rsp)
;   movl $0x78, %r11d
;   movq %r11, 0x150(%rsp)
;   movl $0x7d, %r11d
;   movq %r11, 0x148(%rsp)
;   movl $0x82, %r11d
;   movq %r11, 0x140(%rsp)
;   movl $0x87, %r11d
;   movq %r11, 0x1a0(%rsp)
;   movq 0x170(%rsp), %r11
;   movq %r11, (%rsp)
;   movq %rdi, 8(%rsp)
;   movq %rax, 0x10(%rsp)
;   movq %r10, 0x18(%rsp)
;   movq %r14, 0x20(%rsp)
;   movq %r15, 0x28(%rsp)
;   movq %rbx, 0x30(%rsp)
;   movq %r12, 0x38(%rsp)
;   movq %r13, 0x40(%rsp)
;   movq %rsi, 0x48(%rsp)
;   movq %rdx, 0x50(%rsp)
;   movq %rcx, 0x58(%rsp)
;   movq %r8, 0x60(%rsp)
;   movq %r9, 0x68(%rsp)
;   movq 0x168(%rsp), %r11
;   movq %r11, 0x70(%rsp)
;   movq 0x160(%rsp), %r11
;   movq %r11, 0x78(%rsp)
;   movq 0x158(%rsp), %r11
;   movq %r11, 0x80(%rsp)
;   movq 0x150(%rsp), %r11
;   movq %r11, 0x88(%rsp)
;   movq 0x148(%rsp), %r11
;   movq %r11, 0x90(%rsp)
;   movq 0x140(%rsp), %r11
;   movq %r11, 0x98(%rsp)
;   movq 0x1a0(%rsp), %r11
;   movq %r11, 0xa0(%rsp)
;   leaq 0xb0(%rsp), %rdi
;   movabsq $0, %r10 ; reloc_external Abs8 %tail_callee_stack_args_and_rets 0
;   movq 0x188(%rsp), %rcx
;   movq 0x190(%rsp), %rdx
;   movq 0x198(%rsp), %rsi
;   movq 0x180(%rsp), %r8
;   movq 0x178(%rsp), %r9
;   callq *%r10
;   subq $0xb0, %rsp
;   movq 0xb0(%rsp), %r11
;   movq %r11, 0x140(%rsp)
;   movq 0xb8(%rsp), %r11
;   movq %r11, 0x148(%rsp)
;   movq 0xc0(%rsp), %r11
;   movq %r11, 0x150(%rsp)
;   movq 0xc8(%rsp), %r11
;   movq %r11, 0x158(%rsp)
;   movq 0xd0(%rsp), %r11
;   movq %r11, 0x160(%rsp)
;   movq 0xd8(%rsp), %r11
;   movq %r11, 0x168(%rsp)
;   movq 0xe0(%rsp), %r11
;   movq %r11, 0x170(%rsp)
;   movq 0xe8(%rsp), %r11
;   movq %r11, 0x178(%rsp)
;   movq 0xf0(%rsp), %r11
;   movq %r11, 0x180(%rsp)
;   movq 0xf8(%rsp), %r11
;   movq %r11, 0x188(%rsp)
;   movq 0x100(%rsp), %r11
;   movq %r11, 0x190(%rsp)
;   movq 0x108(%rsp), %r11
;   movq %r11, 0x198(%rsp)
;   movq 0x110(%rsp), %rbx
;   movq 0x118(%rsp), %r12
;   movq 0x120(%rsp), %r13
;   movq 0x128(%rsp), %r14
;   movq 0x130(%rsp), %r15
;   movq 0x138(%rsp), %r11
;   movq %r11, 0x1a0(%rsp)
;   movq 0x1a0(%rsp), %rax
;   movq 0x1b0(%rsp), %rbx
;   movq 0x1b8(%rsp), %r12
;   movq 0x1c0(%rsp), %r13
;   movq 0x1c8(%rsp), %r14
;   movq 0x1d0(%rsp), %r15
;   addq $0x1e0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   retq

;; Test that tail calls that shrink the argument area don't clobber the location
;; of an indirect jump

function %tail_call_indirect_with_shrink(f64, f64, i8, i32 sext, i128, i32 sext, i128, i32, i128) -> i8 tail {
    sig0 = () -> i8 tail
    fn0 = %callee_simple sig0

block0(v0: f64, v1: f64, v2: i8, v3: i32, v4: i128, v5: i32, v6: i128, v7: i32, v8: i128):
    v14 = func_addr.i64 fn0
    return_call_indirect sig0, v14()
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   load_ext_name %callee_simple+0, %r10
;   return_call_unknown %r10 (0) tmp=%r11
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movabsq $0, %r10 ; reloc_external Abs8 %callee_simple 0
;   movq %rbp, %rsp
;   popq %rbp
;   movq (%rsp), %r11
;   movq %r11, 0x20(%rsp)
;   addq $0x20, %rsp
;   jmpq *%r10

