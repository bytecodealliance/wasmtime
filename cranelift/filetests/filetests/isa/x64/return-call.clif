test compile precise-output
set preserve_frame_pointers=true
target x86_64

;;;; Test passing `i64`s ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %callee_i64(i64) -> i64 tail {
block0(v0: i64):
    v1 = iadd_imm.i64 v0, 10
    return v1
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   lea     10(%rdi), %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   leaq 0xa(%rdi), %rax
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %call_i64(i64) -> i64 tail {
    fn0 = %callee_i64(i64) -> i64 tail

block0(v0: i64):
    return_call fn0(v0)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   load_ext_name %callee_i64+0, %rax
;   return_call_unknown %rax (0) %rdi=%rdi
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movabsq $0, %rax ; reloc_external Abs8 %callee_i64 0
;   movq %rbp, %rsp
;   popq %rbp
;   jmpq *%rax

;;;; Test colocated tail calls ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %colocated_i64(i64) -> i64 tail {
    fn0 = colocated %callee_i64(i64) -> i64 tail

block0(v0: i64):
    return_call fn0(v0)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   return_call_known TestCase(%callee_i64) (0) %rdi=%rdi
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq %rbp, %rsp
;   popq %rbp
;   jmp 0xd ; reloc_external CallPCRel4 %callee_i64 -4

;;;; Test passing `f64`s ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %callee_f64(f64) -> f64 tail {
block0(v0: f64):
    v1 = f64const 0x10.0
    v2 = fadd.f64 v0, v1
    return v2
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   addsd   %xmm0, const(0), %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   addsd 0x14(%rip), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %dh, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)

function %call_f64(f64) -> f64 tail {
    fn0 = %callee_f64(f64) -> f64 tail

block0(v0: f64):
    return_call fn0(v0)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   load_ext_name %callee_f64+0, %rax
;   return_call_unknown %rax (0) %xmm0=%xmm0
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movabsq $0, %rax ; reloc_external Abs8 %callee_f64 0
;   movq %rbp, %rsp
;   popq %rbp
;   jmpq *%rax

;;;; Test passing `i8`s ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %callee_i8(i8) -> i8 tail {
block0(v0: i8):
    v1 = iconst.i8 0
    v2 = icmp eq v0, v1
    return v2
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   testb   %dil, %dil
;   setz    %al
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   testb %dil, %dil
;   sete %al
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %call_i8(i8) -> i8 tail {
    fn0 = %callee_i8(i8) -> i8 tail

block0(v0: i8):
    return_call fn0(v0)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   load_ext_name %callee_i8+0, %rax
;   return_call_unknown %rax (0) %rdi=%rdi
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movabsq $0, %rax ; reloc_external Abs8 %callee_i8 0
;   movq %rbp, %rsp
;   popq %rbp
;   jmpq *%rax

;;;; Test passing fewer arguments on the stack ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %one_stack_arg(i32, i32, i32, i32, i32, i32, i32) tail {
block0(v0: i32, v1: i32, v2: i32, v3: i32, v4: i32, v5: i32, v6: i32):
    return
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    rbp(stack args max - 16), %r10
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret 16
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq 0x10(%rbp), %r10
;   movq %rbp, %rsp
;   popq %rbp
;   retq $0x10

function %call_one_stack_arg(i32, i32, i32, i32, i32, i32, i32, i32, i32) tail {
    fn0 = colocated %one_stack_arg(i32, i32, i32, i32, i32, i32, i32) tail

block0(v0: i32, v1: i32, v2: i32, v3: i32, v4: i32, v5: i32, v6: i32, v7: i32, v8: i32):
    return_call fn0(v2, v3, v4, v5, v6, v7, v8)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdx, %rdi
;   movq    %rcx, %rsi
;   movq    %r8, %rdx
;   movq    %r9, %rcx
;   movq    rbp(stack args max - 32), %r8
;   movq    rbp(stack args max - 24), %r9
;   movq    rbp(stack args max - 16), %rax
;   movl    %eax, rbp(stack args max - 16)
;   return_call_known TestCase(%one_stack_arg) (16) %rdi=%rdi %rsi=%rsi %rdx=%rdx %rcx=%rcx %r8=%r8 %r9=%r9
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq %rdx, %rdi
;   movq %rcx, %rsi
;   movq %r8, %rdx
;   movq %r9, %rcx
;   movq 0x10(%rbp), %r8
;   movq 0x18(%rbp), %r9
;   movq 0x20(%rbp), %rax
;   movl %eax, 0x20(%rbp)
;   movq %rbp, %rsp
;   popq %rbp
;   movq (%rsp), %r11
;   movq %r11, 0x10(%rsp)
;   addq $0x10, %rsp
;   jmp 0x35 ; reloc_external CallPCRel4 %one_stack_arg -4

function %call_zero_stack_args(i32, i32, i32, i32, i32, i32, i32, i32, i8) -> i8 tail {
    fn0 = colocated %callee_i8(i8) -> i8 tail

block0(v0: i32, v1: i32, v2: i32, v3: i32, v4: i32, v5: i32, v6: i32, v7: i32, v8: i8):
    return_call fn0(v8)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    rbp(stack args max - 32), %r10
;   movq    rbp(stack args max - 24), %rsi
;   movq    rbp(stack args max - 16), %rdi
;   return_call_known TestCase(%callee_i8) (0) %rdi=%rdi
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq 0x10(%rbp), %r10
;   movq 0x18(%rbp), %rsi
;   movq 0x20(%rbp), %rdi
;   movq %rbp, %rsp
;   popq %rbp
;   movq (%rsp), %r11
;   movq %r11, 0x20(%rsp)
;   addq $0x20, %rsp
;   jmp 0x26 ; reloc_external CallPCRel4 %callee_i8 -4

;;;; Test growing the argument area when it's non-empty ;;;;;;;;;;;;;;;;;;;;;;;;

function %call_from_one_stack_arg(i32, i32, i32, i32, i32, i32, i32) tail {
    fn0 = colocated %call_one_stack_arg(i32, i32, i32, i32, i32, i32, i32, i32, i32) tail

block0(v0: i32, v1: i32, v2: i32, v3: i32, v4: i32, v5: i32, v6: i32):
    return_call fn0(v1, v2, v3, v4, v5, v6, v0, v0, v1)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %rsp, %rbp
;   movq    16(%rsp), %r11
;   movq    %r11, 0(%rsp)
;   movq    24(%rsp), %r11
;   movq    %r11, 8(%rsp)
; block0:
;   movq    %rdx, %r10
;   movq    %rcx, %rdx
;   movq    %r8, %rcx
;   movq    %r9, %r8
;   movq    rbp(stack args max - 16), %r9
;   movl    %edi, rbp(stack args max - 32)
;   movl    %edi, rbp(stack args max - 24)
;   movl    %esi, rbp(stack args max - 16)
;   movq    %rsi, %rdi
;   movq    %r10, %rsi
;   return_call_known TestCase(%call_one_stack_arg) (32) %rdi=%rdi %rsi=%rsi %rdx=%rdx %rcx=%rcx %r8=%r8 %r9=%r9
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0x10, %rsp
;   movq %rsp, %rbp
;   movq 0x10(%rsp), %r11
;   movq %r11, (%rsp)
;   movq 0x18(%rsp), %r11
;   movq %r11, 8(%rsp)
; block1: ; offset 0x1e
;   movq %rdx, %r10
;   movq %rcx, %rdx
;   movq %r8, %rcx
;   movq %r9, %r8
;   movq 0x20(%rbp), %r9
;   movl %edi, 0x10(%rbp)
;   movl %edi, 0x18(%rbp)
;   movl %esi, 0x20(%rbp)
;   movq %rsi, %rdi
;   movq %r10, %rsi
;   movq %rbp, %rsp
;   popq %rbp
;   jmp 0x46 ; reloc_external CallPCRel4 %call_one_stack_arg -4

;;;; Test passing many arguments on stack ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %tail_callee_stack_args(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64, v6: i64, v7: i64, v8: i64, v9: i64, v10: i64, v11: i64, v12: i64, v13: i64, v14: i64, v15: i64, v16: i64, v17: i64, v18: i64, v19: i64, v20: i64, v21: i64, v22: i64, v23: i64, v24: i64, v25: i64):
    return v25
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    rbp(stack args max - 160), %r10
;   movq    rbp(stack args max - 152), %rsi
;   movq    rbp(stack args max - 144), %rax
;   movq    rbp(stack args max - 136), %rdx
;   movq    rbp(stack args max - 128), %r9
;   movq    rbp(stack args max - 120), %r11
;   movq    rbp(stack args max - 112), %rdi
;   movq    rbp(stack args max - 104), %rcx
;   movq    rbp(stack args max - 96), %r8
;   movq    rbp(stack args max - 88), %r10
;   movq    rbp(stack args max - 80), %rsi
;   movq    rbp(stack args max - 72), %rax
;   movq    rbp(stack args max - 64), %rdx
;   movq    rbp(stack args max - 56), %r9
;   movq    rbp(stack args max - 48), %r11
;   movq    rbp(stack args max - 40), %rdi
;   movq    rbp(stack args max - 32), %rcx
;   movq    rbp(stack args max - 24), %r8
;   movq    rbp(stack args max - 16), %r10
;   movq    rbp(stack args max - 8), %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret 160
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movq 0x10(%rbp), %r10
;   movq 0x18(%rbp), %rsi
;   movq 0x20(%rbp), %rax
;   movq 0x28(%rbp), %rdx
;   movq 0x30(%rbp), %r9
;   movq 0x38(%rbp), %r11
;   movq 0x40(%rbp), %rdi
;   movq 0x48(%rbp), %rcx
;   movq 0x50(%rbp), %r8
;   movq 0x58(%rbp), %r10
;   movq 0x60(%rbp), %rsi
;   movq 0x68(%rbp), %rax
;   movq 0x70(%rbp), %rdx
;   movq 0x78(%rbp), %r9
;   movq 0x80(%rbp), %r11
;   movq 0x88(%rbp), %rdi
;   movq 0x90(%rbp), %rcx
;   movq 0x98(%rbp), %r8
;   movq 0xa0(%rbp), %r10
;   movq 0xa8(%rbp), %rax
;   movq %rbp, %rsp
;   popq %rbp
;   retq $0xa0

function %tail_caller_stack_args() -> i64 tail {
    fn0 = %tail_callee_stack_args(i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64, i64) -> i64 tail

block0:
    v0 = iconst.i64 10
    v1 = iconst.i64 15
    v2 = iconst.i64 20
    v3 = iconst.i64 25
    v4 = iconst.i64 30
    v5 = iconst.i64 35
    v6 = iconst.i64 40
    v7 = iconst.i64 45
    v8 = iconst.i64 50
    v9 = iconst.i64 55
    v10 = iconst.i64 60
    v11 = iconst.i64 65
    v12 = iconst.i64 70
    v13 = iconst.i64 75
    v14 = iconst.i64 80
    v15 = iconst.i64 85
    v16 = iconst.i64 90
    v17 = iconst.i64 95
    v18 = iconst.i64 100
    v19 = iconst.i64 105
    v20 = iconst.i64 110
    v21 = iconst.i64 115
    v22 = iconst.i64 120
    v23 = iconst.i64 125
    v24 = iconst.i64 130
    v25 = iconst.i64 135
    return_call fn0(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, v23, v24, v25)
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $160, %rsp
;   movq    %rsp, %rbp
;   movq    160(%rsp), %r11
;   movq    %r11, 0(%rsp)
;   movq    168(%rsp), %r11
;   movq    %r11, 8(%rsp)
;   subq    %rsp, $160, %rsp
;   movq    %rbx, 112(%rsp)
;   movq    %r12, 120(%rsp)
;   movq    %r13, 128(%rsp)
;   movq    %r14, 136(%rsp)
;   movq    %r15, 144(%rsp)
; block0:
;   movl    $10, %edi
;   movq    %rdi, rsp(96 + virtual offset)
;   movl    $15, %esi
;   movq    %rsi, rsp(88 + virtual offset)
;   movl    $20, %edx
;   movq    %rdx, rsp(80 + virtual offset)
;   movl    $25, %ecx
;   movq    %rcx, rsp(72 + virtual offset)
;   movl    $30, %r8d
;   movq    %r8, rsp(64 + virtual offset)
;   movl    $35, %r9d
;   movq    %r9, rsp(56 + virtual offset)
;   movl    $40, %esi
;   movq    %rsi, rsp(48 + virtual offset)
;   movl    $45, %eax
;   movl    $50, %r10d
;   movl    $55, %r12d
;   movl    $60, %r13d
;   movl    $65, %r14d
;   movl    $70, %r15d
;   movl    $75, %ebx
;   movl    $80, %edi
;   movl    $85, %esi
;   movl    $90, %edx
;   movl    $95, %ecx
;   movl    $100, %r8d
;   movl    $105, %r9d
;   movl    $110, %r11d
;   movq    %r11, rsp(40 + virtual offset)
;   movl    $115, %r11d
;   movq    %r11, rsp(32 + virtual offset)
;   movl    $120, %r11d
;   movq    %r11, rsp(24 + virtual offset)
;   movl    $125, %r11d
;   movq    %r11, rsp(16 + virtual offset)
;   movl    $130, %r11d
;   movq    %r11, rsp(8 + virtual offset)
;   movl    $135, %r11d
;   movq    %r11, rsp(0 + virtual offset)
;   movq    rsp(48 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 160)
;   movq    %rax, rbp(stack args max - 152)
;   movq    %r10, rbp(stack args max - 144)
;   movq    %r12, rbp(stack args max - 136)
;   movq    %r13, rbp(stack args max - 128)
;   movq    %r14, rbp(stack args max - 120)
;   movq    %r15, rbp(stack args max - 112)
;   movq    %rbx, rbp(stack args max - 104)
;   movq    %rdi, rbp(stack args max - 96)
;   movq    %rsi, rbp(stack args max - 88)
;   movq    %rdx, rbp(stack args max - 80)
;   movq    %rcx, rbp(stack args max - 72)
;   movq    %r8, rbp(stack args max - 64)
;   movq    %r9, rbp(stack args max - 56)
;   movq    rsp(40 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 48)
;   movq    rsp(32 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 40)
;   movq    rsp(24 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 32)
;   movq    rsp(16 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 24)
;   movq    rsp(8 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 16)
;   movq    rsp(0 + virtual offset), %r11
;   movq    %r11, rbp(stack args max - 8)
;   load_ext_name %tail_callee_stack_args+0, %rax
;   movq    rsp(72 + virtual offset), %rcx
;   movq    rsp(80 + virtual offset), %rdx
;   movq    rsp(88 + virtual offset), %rsi
;   movq    rsp(96 + virtual offset), %rdi
;   movq    rsp(64 + virtual offset), %r8
;   movq    rsp(56 + virtual offset), %r9
;   return_call_unknown %rax (160) %rdi=%rdi %rsi=%rsi %rdx=%rdx %rcx=%rcx %r8=%r8 %r9=%r9
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
;   subq $0xa0, %rsp
;   movq %rsp, %rbp
;   movq 0xa0(%rsp), %r11
;   movq %r11, (%rsp)
;   movq 0xa8(%rsp), %r11
;   movq %r11, 8(%rsp)
;   subq $0xa0, %rsp
;   movq %rbx, 0x70(%rsp)
;   movq %r12, 0x78(%rsp)
;   movq %r13, 0x80(%rsp)
;   movq %r14, 0x88(%rsp)
;   movq %r15, 0x90(%rsp)
; block1: ; offset 0x50
;   movl $0xa, %edi
;   movq %rdi, 0x60(%rsp)
;   movl $0xf, %esi
;   movq %rsi, 0x58(%rsp)
;   movl $0x14, %edx
;   movq %rdx, 0x50(%rsp)
;   movl $0x19, %ecx
;   movq %rcx, 0x48(%rsp)
;   movl $0x1e, %r8d
;   movq %r8, 0x40(%rsp)
;   movl $0x23, %r9d
;   movq %r9, 0x38(%rsp)
;   movl $0x28, %esi
;   movq %rsi, 0x30(%rsp)
;   movl $0x2d, %eax
;   movl $0x32, %r10d
;   movl $0x37, %r12d
;   movl $0x3c, %r13d
;   movl $0x41, %r14d
;   movl $0x46, %r15d
;   movl $0x4b, %ebx
;   movl $0x50, %edi
;   movl $0x55, %esi
;   movl $0x5a, %edx
;   movl $0x5f, %ecx
;   movl $0x64, %r8d
;   movl $0x69, %r9d
;   movl $0x6e, %r11d
;   movq %r11, 0x28(%rsp)
;   movl $0x73, %r11d
;   movq %r11, 0x20(%rsp)
;   movl $0x78, %r11d
;   movq %r11, 0x18(%rsp)
;   movl $0x7d, %r11d
;   movq %r11, 0x10(%rsp)
;   movl $0x82, %r11d
;   movq %r11, 8(%rsp)
;   movl $0x87, %r11d
;   movq %r11, (%rsp)
;   movq 0x30(%rsp), %r11
;   movq %r11, 0x10(%rbp)
;   movq %rax, 0x18(%rbp)
;   movq %r10, 0x20(%rbp)
;   movq %r12, 0x28(%rbp)
;   movq %r13, 0x30(%rbp)
;   movq %r14, 0x38(%rbp)
;   movq %r15, 0x40(%rbp)
;   movq %rbx, 0x48(%rbp)
;   movq %rdi, 0x50(%rbp)
;   movq %rsi, 0x58(%rbp)
;   movq %rdx, 0x60(%rbp)
;   movq %rcx, 0x68(%rbp)
;   movq %r8, 0x70(%rbp)
;   movq %r9, 0x78(%rbp)
;   movq 0x28(%rsp), %r11
;   movq %r11, 0x80(%rbp)
;   movq 0x20(%rsp), %r11
;   movq %r11, 0x88(%rbp)
;   movq 0x18(%rsp), %r11
;   movq %r11, 0x90(%rbp)
;   movq 0x10(%rsp), %r11
;   movq %r11, 0x98(%rbp)
;   movq 8(%rsp), %r11
;   movq %r11, 0xa0(%rbp)
;   movq (%rsp), %r11
;   movq %r11, 0xa8(%rbp)
;   movabsq $0, %rax ; reloc_external Abs8 %tail_callee_stack_args 0
;   movq 0x48(%rsp), %rcx
;   movq 0x50(%rsp), %rdx
;   movq 0x58(%rsp), %rsi
;   movq 0x60(%rsp), %rdi
;   movq 0x40(%rsp), %r8
;   movq 0x38(%rsp), %r9
;   movq 0x70(%rsp), %rbx
;   movq 0x78(%rsp), %r12
;   movq 0x80(%rsp), %r13
;   movq 0x88(%rsp), %r14
;   movq 0x90(%rsp), %r15
;   addq $0xa0, %rsp
;   movq %rbp, %rsp
;   popq %rbp
;   jmpq *%rax

