test compile precise-output
set enable_llvm_abi_extensions=true
set unwind_info=true
target x86_64

function %f0(i64, i64, i64, i64) -> i64 windows_fastcall {
block0(v0: i64, v1: i64, v2: i64, v3: i64):
  return v0
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    %rcx, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f1(i64, i64, i64, i64) -> i64 windows_fastcall {
block0(v0: i64, v1: i64, v2: i64, v3: i64):
  return v1
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    %rdx, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f2(i64, i64, i64, i64) -> i64 windows_fastcall {
block0(v0: i64, v1: i64, v2: i64, v3: i64):
  return v2
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    %r8, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f3(i64, i64, i64, i64) -> i64 windows_fastcall {
block0(v0: i64, v1: i64, v2: i64, v3: i64):
  return v3
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    %r9, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f4(i64, i64, f64, i64) -> f64 windows_fastcall {
block0(v0: i64, v1: i64, v2: f64, v3: i64):
  return v2
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movdqa  %xmm2, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f5(i64, i64, f64, i64) -> i64 windows_fastcall {
block0(v0: i64, v1: i64, v2: f64, v3: i64):
  return v3
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    %r9, %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f6(i64, i64, i64, i64, i64, i64) -> i64 windows_fastcall {
block0(v0: i64, v1: i64, v2: i64, v3: i64, v4: i64, v5: i64):
  return v5

;; This is truly odd (because of the regalloc ordering), but it works. Note
;; that we're spilling and using rsi, which is a callee-save in fastcall, because
;; the regalloc order is optimized for SysV. Also note that because we copy args
;; out of their input locations to separate vregs, we have a spurious load
;; from [rbp+48]. Ordinarily these moves are coalesced because the dest vreg
;; is allocated as a caller-save (volatile), but here again we allocate rsi
;; first and so have to spill it (and consequently don't coalesce).
;;
;; TODO(#2704): fix regalloc's register priority ordering!
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    48(%rbp), %r10
;   movq    56(%rbp), %rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f7(i128, i64, i128, i128) -> i128 windows_fastcall {
block0(v0: i128, v1: i64, v2: i128, v3: i128):
  return v3
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   movq    48(%rbp), %r10
;   movq    56(%rbp), %rax
;   movq    64(%rbp), %rdx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f8(i64) -> i64 windows_fastcall {
  sig0 = (i64, i64, f64, f64, i64, i64) -> i64 windows_fastcall
  fn0 = %g sig0

block0(v0: i64):
  v1 = fcvt_from_sint.f64 v0
  v2 = call fn0(v0, v0, v1, v1, v0, v0)
  return v2
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 0 }
; block0:
;   cvtsi2sd %rcx, %xmm2
;   subq    %rsp, $48, %rsp
;   virtual_sp_offset_adjust 48
;   movq    %rcx, %rdx
;   movq    %rdx, %r8
;   movdqa  %xmm2, %xmm3
;   movq    %r8, 32(%rsp)
;   movq    %r8, 40(%rsp)
;   load_ext_name %g+0, %r8
;   call    *%r8
;   addq    %rsp, $48, %rsp
;   virtual_sp_offset_adjust -48
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %f9(i64) -> f64 windows_fastcall {
block0(v0: i64):
  v1 = load.f64 v0+0
  v2 = load.f64 v0+8
  v3 = load.f64 v0+16
  v4 = load.f64 v0+24
  v5 = load.f64 v0+32
  v6 = load.f64 v0+40
  v7 = load.f64 v0+48
  v8 = load.f64 v0+56
  v9 = load.f64 v0+64
  v10 = load.f64 v0+72
  v11 = load.f64 v0+80
  v12 = load.f64 v0+88
  v13 = load.f64 v0+96
  v14 = load.f64 v0+104
  v15 = load.f64 v0+112
  v16 = load.f64 v0+120
  v17 = load.f64 v0+128
  v18 = load.f64 v0+136
  v19 = load.f64 v0+144
  v20 = load.f64 v0+152

  v21 = fadd.f64 v1, v2
  v22 = fadd.f64 v3, v4
  v23 = fadd.f64 v5, v6
  v24 = fadd.f64 v7, v8
  v25 = fadd.f64 v9, v10
  v26 = fadd.f64 v11, v12
  v27 = fadd.f64 v13, v14
  v28 = fadd.f64 v15, v16
  v29 = fadd.f64 v17, v18
  v30 = fadd.f64 v19, v20

  v31 = fadd.f64 v21, v22
  v32 = fadd.f64 v23, v24
  v33 = fadd.f64 v25, v26
  v34 = fadd.f64 v27, v28
  v35 = fadd.f64 v29, v30

  v36 = fadd.f64 v31, v32
  v37 = fadd.f64 v33, v34

  v38 = fadd.f64 v36, v37

  v39 = fadd.f64 v38, v35

  return v39
}

;   pushq   %rbp
;   unwind PushFrameRegs { offset_upward_to_caller_sp: 16 }
;   movq    %rsp, %rbp
;   unwind DefineNewFrame { offset_upward_to_caller_sp: 16, offset_downward_to_clobbers: 160 }
;   subq    %rsp, $224, %rsp
;   movdqu  %xmm6, 64(%rsp)
;   unwind SaveReg { clobber_offset: 0, reg: p6f }
;   movdqu  %xmm7, 80(%rsp)
;   unwind SaveReg { clobber_offset: 16, reg: p7f }
;   movdqu  %xmm8, 96(%rsp)
;   unwind SaveReg { clobber_offset: 32, reg: p8f }
;   movdqu  %xmm9, 112(%rsp)
;   unwind SaveReg { clobber_offset: 48, reg: p9f }
;   movdqu  %xmm10, 128(%rsp)
;   unwind SaveReg { clobber_offset: 64, reg: p10f }
;   movdqu  %xmm11, 144(%rsp)
;   unwind SaveReg { clobber_offset: 80, reg: p11f }
;   movdqu  %xmm12, 160(%rsp)
;   unwind SaveReg { clobber_offset: 96, reg: p12f }
;   movdqu  %xmm13, 176(%rsp)
;   unwind SaveReg { clobber_offset: 112, reg: p13f }
;   movdqu  %xmm14, 192(%rsp)
;   unwind SaveReg { clobber_offset: 128, reg: p14f }
;   movdqu  %xmm15, 208(%rsp)
;   unwind SaveReg { clobber_offset: 144, reg: p15f }
; block0:
;   movsd   0(%rcx), %xmm0
;   movsd   8(%rcx), %xmm11
;   movdqu  %xmm11, rsp(48 + virtual offset)
;   movsd   16(%rcx), %xmm6
;   movsd   24(%rcx), %xmm15
;   movdqu  %xmm15, rsp(32 + virtual offset)
;   movsd   32(%rcx), %xmm14
;   movsd   40(%rcx), %xmm1
;   movdqu  %xmm1, rsp(16 + virtual offset)
;   movsd   48(%rcx), %xmm8
;   movsd   56(%rcx), %xmm9
;   movdqu  %xmm9, rsp(0 + virtual offset)
;   movsd   64(%rcx), %xmm13
;   movsd   72(%rcx), %xmm3
;   movsd   80(%rcx), %xmm10
;   movsd   88(%rcx), %xmm5
;   movsd   96(%rcx), %xmm4
;   movsd   104(%rcx), %xmm9
;   movsd   112(%rcx), %xmm12
;   movsd   120(%rcx), %xmm11
;   movsd   128(%rcx), %xmm7
;   movsd   136(%rcx), %xmm15
;   movsd   144(%rcx), %xmm2
;   movdqu  rsp(48 + virtual offset), %xmm1
;   addsd   %xmm0, %xmm1, %xmm0
;   movdqu  rsp(32 + virtual offset), %xmm1
;   addsd   %xmm6, %xmm1, %xmm6
;   movdqu  rsp(16 + virtual offset), %xmm1
;   addsd   %xmm14, %xmm1, %xmm14
;   movdqu  rsp(0 + virtual offset), %xmm1
;   addsd   %xmm8, %xmm1, %xmm8
;   addsd   %xmm13, %xmm3, %xmm13
;   addsd   %xmm10, %xmm5, %xmm10
;   addsd   %xmm4, %xmm9, %xmm4
;   addsd   %xmm12, %xmm11, %xmm12
;   addsd   %xmm7, %xmm15, %xmm7
;   addsd   %xmm2, 152(%rcx), %xmm2
;   addsd   %xmm0, %xmm6, %xmm0
;   addsd   %xmm14, %xmm8, %xmm14
;   addsd   %xmm13, %xmm10, %xmm13
;   addsd   %xmm4, %xmm12, %xmm4
;   addsd   %xmm7, %xmm2, %xmm7
;   addsd   %xmm0, %xmm14, %xmm0
;   addsd   %xmm13, %xmm4, %xmm13
;   addsd   %xmm0, %xmm13, %xmm0
;   addsd   %xmm0, %xmm7, %xmm0
;   movdqu  64(%rsp), %xmm6
;   movdqu  80(%rsp), %xmm7
;   movdqu  96(%rsp), %xmm8
;   movdqu  112(%rsp), %xmm9
;   movdqu  128(%rsp), %xmm10
;   movdqu  144(%rsp), %xmm11
;   movdqu  160(%rsp), %xmm12
;   movdqu  176(%rsp), %xmm13
;   movdqu  192(%rsp), %xmm14
;   movdqu  208(%rsp), %xmm15
;   addq    %rsp, $224, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

