test compile precise-output
target x86_64

function %f1(i8) -> f32 {
block0(v0: i8):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   movsbl  %dil, %eax
;   cvtsi2ss %eax, %xmm0
;   ret

function %f2(i16) -> f32 {
block0(v0: i16):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   movswl  %di, %eax
;   cvtsi2ss %eax, %xmm0
;   ret

function %f3(i32) -> f32 {
block0(v0: i32):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   cvtsi2ss %edi, %xmm0
;   ret

function %f4(i64) -> f32 {
block0(v0: i64):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   cvtsi2ss %rdi, %xmm0
;   ret

function %f5(i8) -> f64 {
block0(v0: i8):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   movsbl  %dil, %eax
;   cvtsi2sd %eax, %xmm0
;   ret

function %f6(i16) -> f64 {
block0(v0: i16):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   movswl  %di, %eax
;   cvtsi2sd %eax, %xmm0
;   ret

function %f7(i32) -> f64 {
block0(v0: i32):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   cvtsi2sd %edi, %xmm0
;   ret

function %f8(i64) -> f64 {
block0(v0: i64):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   cvtsi2sd %rdi, %xmm0
;   ret

function %f9(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = fcvt_low_from_sint.f64x2 v0
  return v1
}

; block0:
;   cvtdq2pd %xmm0, %xmm0
;   ret

function %f10(i8, i16, i32, i64) -> f32 {
block0(v0: i8, v1: i16, v2: i32, v3: i64):
  v4 = fcvt_from_uint.f32 v0
  v5 = fcvt_from_uint.f32 v1
  v6 = fcvt_from_uint.f32 v2
  v7 = fcvt_from_uint.f32 v3
  v8 = fadd.f32 v4, v5
  v9 = fadd.f32 v8, v6
  v10 = fadd.f32 v9, v7
  return v10
}

; block0:
;   movzbq  %dil, %r9
;   cvtsi2ss %r9, %xmm0
;   movzwq  %si, %r9
;   cvtsi2ss %r9, %xmm1
;   movl    %edx, %r9d
;   cvtsi2ss %r9, %xmm2
;   u64_to_f32_seq %rcx, %xmm14, %r9, %r10
;   addss   %xmm0, %xmm1, %xmm0
;   addss   %xmm0, %xmm2, %xmm0
;   addss   %xmm0, %xmm14, %xmm0
;   ret

function %f11(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = uwiden_low v0
  v2 = fcvt_from_uint.f64x2 v1
  return v2
}

; block0:
;   movdqu  const(0), %xmm2
;   unpcklps %xmm0, %xmm2, %xmm0
;   movdqu  const(1), %xmm6
;   subpd   %xmm0, %xmm6, %xmm0
;   ret

function %f12(i32x4) -> f32x4 {
block0(v0: i32x4):
  v1 = fcvt_from_uint.f32x4 v0
  return v1
}

; block0:
;   movdqa  %xmm0, %xmm3
;   pslld   %xmm3, $16, %xmm3
;   psrld   %xmm3, $16, %xmm3
;   movdqa  %xmm0, %xmm9
;   psubd   %xmm9, %xmm3, %xmm9
;   cvtdq2ps %xmm3, %xmm8
;   psrld   %xmm9, $1, %xmm9
;   cvtdq2ps %xmm9, %xmm0
;   addps   %xmm0, %xmm0, %xmm0
;   addps   %xmm0, %xmm8, %xmm0
;   ret

function %f13(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_uint.i32 v0
  return v1
}

; block0:
;   cvt_float32_to_uint32_seq %xmm0, %eax, %r8, %xmm3, %xmm4
;   ret

function %f14(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_uint.i64 v0
  return v1
}

; block0:
;   cvt_float32_to_uint64_seq %xmm0, %rax, %r8, %xmm3, %xmm4
;   ret

function %f15(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_uint.i32 v0
  return v1
}

; block0:
;   cvt_float64_to_uint32_seq %xmm0, %eax, %r8, %xmm3, %xmm4
;   ret

function %f16(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_uint.i64 v0
  return v1
}

; block0:
;   cvt_float64_to_uint64_seq %xmm0, %rax, %r8, %xmm3, %xmm4
;   ret

function %f17(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_uint_sat.i32 v0
  return v1
}

; block0:
;   cvt_float32_to_uint32_sat_seq %xmm0, %eax, %r8, %xmm3, %xmm4
;   ret

function %f18(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_uint_sat.i64 v0
  return v1
}

; block0:
;   cvt_float32_to_uint64_sat_seq %xmm0, %rax, %r8, %xmm3, %xmm4
;   ret

function %f19(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_uint_sat.i32 v0
  return v1
}

; block0:
;   cvt_float64_to_uint32_sat_seq %xmm0, %eax, %r8, %xmm3, %xmm4
;   ret

function %f20(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_uint_sat.i64 v0
  return v1
}

; block0:
;   cvt_float64_to_uint64_sat_seq %xmm0, %rax, %r8, %xmm3, %xmm4
;   ret

function %f21(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_sint.i32 v0
  return v1
}

; block0:
;   cvt_float32_to_sint32_seq %xmm0, %eax, %rdx, %xmm3
;   ret

function %f22(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_sint.i64 v0
  return v1
}

; block0:
;   cvt_float32_to_sint64_seq %xmm0, %rax, %rdx, %xmm3
;   ret

function %f23(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_sint.i32 v0
  return v1
}

; block0:
;   cvt_float64_to_sint32_seq %xmm0, %eax, %rdx, %xmm3
;   ret

function %f24(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_sint.i64 v0
  return v1
}

; block0:
;   cvt_float64_to_sint64_seq %xmm0, %rax, %rdx, %xmm3
;   ret

function %f25(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_sint_sat.i32 v0
  return v1
}

; block0:
;   cvt_float32_to_sint32_sat_seq %xmm0, %eax, %rdx, %xmm3
;   ret

function %f26(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_sint_sat.i64 v0
  return v1
}

; block0:
;   cvt_float32_to_sint64_sat_seq %xmm0, %rax, %rdx, %xmm3
;   ret

function %f27(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_sint_sat.i32 v0
  return v1
}

; block0:
;   cvt_float64_to_sint32_sat_seq %xmm0, %eax, %rdx, %xmm3
;   ret

function %f28(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_sint_sat.i64 v0
  return v1
}

; block0:
;   cvt_float64_to_sint64_sat_seq %xmm0, %rax, %rdx, %xmm3
;   ret

function %f29(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_uint_sat.i32x4 v0
  return v1
}

; block0:
;   pxor    %xmm2, %xmm2, %xmm2
;   movdqa  %xmm0, %xmm9
;   maxps   %xmm9, %xmm2, %xmm9
;   pcmpeqd %xmm7, %xmm7, %xmm7
;   psrld   %xmm7, $1, %xmm7
;   cvtdq2ps %xmm7, %xmm13
;   cvttps2dq %xmm9, %xmm12
;   subps   %xmm9, %xmm13, %xmm9
;   cmpps   $2, %xmm13, %xmm9, %xmm13
;   cvttps2dq %xmm9, %xmm0
;   pxor    %xmm0, %xmm13, %xmm0
;   pxor    %xmm6, %xmm6, %xmm6
;   pmaxsd  %xmm0, %xmm6, %xmm0
;   paddd   %xmm0, %xmm12, %xmm0
;   ret

function %f30(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_sint_sat.i32x4 v0
  return v1
}

; block0:
;   movdqa  %xmm0, %xmm4
;   cmpps   $0, %xmm4, %xmm0, %xmm4
;   movdqa  %xmm0, %xmm5
;   andps   %xmm5, %xmm4, %xmm5
;   pxor    %xmm4, %xmm5, %xmm4
;   cvttps2dq %xmm5, %xmm8
;   movdqa  %xmm8, %xmm0
;   pand    %xmm0, %xmm4, %xmm0
;   psrad   %xmm0, $31, %xmm0
;   pxor    %xmm0, %xmm8, %xmm0
;   ret

