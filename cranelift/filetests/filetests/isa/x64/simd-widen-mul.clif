test compile precise-output
target x86_64 sse41

function %imul_swiden_hi_i8x16(i8x16, i8x16) -> i16x8 {
block0(v0: i8x16, v1: i8x16):
    v2 = swiden_high v0
    v3 = swiden_high v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm0, %xmm6
;   palignr $8, %xmm6, %xmm0, %xmm6
;   pmovsxbw %xmm6, %xmm0
;   movdqa  %xmm1, %xmm6
;   palignr $8, %xmm6, %xmm1, %xmm6
;   pmovsxbw %xmm6, %xmm8
;   pmullw  %xmm0, %xmm8, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movdqa %xmm0, %xmm6
;   palignr $8, %xmm0, %xmm6
;   pmovsxbw %xmm6, %xmm0
;   movdqa %xmm1, %xmm6
;   palignr $8, %xmm1, %xmm6
;   pmovsxbw %xmm6, %xmm8
;   pmullw %xmm8, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_swiden_hi_i16x8(i16x8, i16x8) -> i32x4 {
block0(v0: i16x8, v1: i16x8):
    v2 = swiden_high v0
    v3 = swiden_high v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm0, %xmm5
;   pmullw  %xmm5, %xmm1, %xmm5
;   movdqa  %xmm5, %xmm6
;   movdqa  %xmm0, %xmm5
;   pmulhw  %xmm5, %xmm1, %xmm5
;   movdqa  %xmm6, %xmm0
;   punpckhwd %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movdqa %xmm0, %xmm5
;   pmullw %xmm1, %xmm5
;   movdqa %xmm5, %xmm6
;   movdqa %xmm0, %xmm5
;   pmulhw %xmm1, %xmm5
;   movdqa %xmm6, %xmm0
;   punpckhwd %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_swiden_hi_i32x4(i32x4, i32x4) -> i64x2 {
block0(v0: i32x4, v1: i32x4):
    v2 = swiden_high v0
    v3 = swiden_high v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pshufd  $250, %xmm0, %xmm0
;   pshufd  $250, %xmm1, %xmm5
;   pmuldq  %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pshufd $0xfa, %xmm0, %xmm0
;   pshufd $0xfa, %xmm1, %xmm5
;   pmuldq %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_swiden_low_i8x16(i8x16, i8x16) -> i16x8 {
block0(v0: i8x16, v1: i8x16):
    v2 = swiden_low v0
    v3 = swiden_low v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pmovsxbw %xmm0, %xmm0
;   pmovsxbw %xmm1, %xmm5
;   pmullw  %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pmovsxbw %xmm0, %xmm0
;   pmovsxbw %xmm1, %xmm5
;   pmullw %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_swiden_low_i16x8(i16x8, i16x8) -> i32x4 {
block0(v0: i16x8, v1: i16x8):
    v2 = swiden_low v0
    v3 = swiden_low v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm0, %xmm5
;   pmullw  %xmm5, %xmm1, %xmm5
;   movdqa  %xmm5, %xmm6
;   movdqa  %xmm0, %xmm5
;   pmulhw  %xmm5, %xmm1, %xmm5
;   movdqa  %xmm6, %xmm0
;   punpcklwd %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movdqa %xmm0, %xmm5
;   pmullw %xmm1, %xmm5
;   movdqa %xmm5, %xmm6
;   movdqa %xmm0, %xmm5
;   pmulhw %xmm1, %xmm5
;   movdqa %xmm6, %xmm0
;   punpcklwd %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_swiden_low_i32x4(i32x4, i32x4) -> i64x2 {
block0(v0: i32x4, v1: i32x4):
    v2 = swiden_low v0
    v3 = swiden_low v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pshufd  $80, %xmm0, %xmm0
;   pshufd  $80, %xmm1, %xmm5
;   pmuldq  %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pshufd $0x50, %xmm0, %xmm0
;   pshufd $0x50, %xmm1, %xmm5
;   pmuldq %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_uwiden_hi_i8x16(i8x16, i8x16) -> i16x8 {
block0(v0: i8x16, v1: i8x16):
    v2 = uwiden_high v0
    v3 = uwiden_high v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   uninit  %xmm8
;   pxor    %xmm8, %xmm8, %xmm8
;   punpckhbw %xmm0, %xmm8, %xmm0
;   uninit  %xmm8
;   pxor    %xmm8, %xmm8, %xmm8
;   movdqa  %xmm1, %xmm11
;   punpckhbw %xmm11, %xmm8, %xmm11
;   pmullw  %xmm0, %xmm11, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pxor %xmm8, %xmm8
;   punpckhbw %xmm8, %xmm0
;   pxor %xmm8, %xmm8
;   movdqa %xmm1, %xmm11
;   punpckhbw %xmm8, %xmm11
;   pmullw %xmm11, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_uwiden_hi_i16x8(i16x8, i16x8) -> i32x4 {
block0(v0: i16x8, v1: i16x8):
    v2 = uwiden_high v0
    v3 = uwiden_high v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm0, %xmm5
;   pmullw  %xmm5, %xmm1, %xmm5
;   movdqa  %xmm5, %xmm6
;   movdqa  %xmm0, %xmm5
;   pmulhuw %xmm5, %xmm1, %xmm5
;   movdqa  %xmm6, %xmm0
;   punpckhwd %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movdqa %xmm0, %xmm5
;   pmullw %xmm1, %xmm5
;   movdqa %xmm5, %xmm6
;   movdqa %xmm0, %xmm5
;   pmulhuw %xmm1, %xmm5
;   movdqa %xmm6, %xmm0
;   punpckhwd %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_uwiden_hi_i32x4(i32x4, i32x4) -> i64x2 {
block0(v0: i32x4, v1: i32x4):
    v2 = uwiden_high v0
    v3 = uwiden_high v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pshufd  $250, %xmm0, %xmm0
;   pshufd  $250, %xmm1, %xmm5
;   pmuludq %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pshufd $0xfa, %xmm0, %xmm0
;   pshufd $0xfa, %xmm1, %xmm5
;   pmuludq %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_uwiden_low_i8x16(i8x16, i8x16) -> i16x8 {
block0(v0: i8x16, v1: i8x16):
    v2 = uwiden_low v0
    v3 = uwiden_low v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pmovzxbw %xmm0, %xmm0
;   pmovzxbw %xmm1, %xmm5
;   pmullw  %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pmovzxbw %xmm0, %xmm0
;   pmovzxbw %xmm1, %xmm5
;   pmullw %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_uwiden_low_i16x8(i16x8, i16x8) -> i32x4 {
block0(v0: i16x8, v1: i16x8):
    v2 = uwiden_low v0
    v3 = uwiden_low v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm0, %xmm5
;   pmullw  %xmm5, %xmm1, %xmm5
;   movdqa  %xmm5, %xmm6
;   movdqa  %xmm0, %xmm5
;   pmulhuw %xmm5, %xmm1, %xmm5
;   movdqa  %xmm6, %xmm0
;   punpcklwd %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   movdqa %xmm0, %xmm5
;   pmullw %xmm1, %xmm5
;   movdqa %xmm5, %xmm6
;   movdqa %xmm0, %xmm5
;   pmulhuw %xmm1, %xmm5
;   movdqa %xmm6, %xmm0
;   punpcklwd %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %imul_uwiden_low_i32x4(i32x4, i32x4) -> i64x2 {
block0(v0: i32x4, v1: i32x4):
    v2 = uwiden_low v0
    v3 = uwiden_low v1
    v4 = imul v2, v3
    return v4
}

; VCode:
;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   pshufd  $80, %xmm0, %xmm0
;   pshufd  $80, %xmm1, %xmm5
;   pmuludq %xmm0, %xmm5, %xmm0
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   pshufd $0x50, %xmm0, %xmm0
;   pshufd $0x50, %xmm1, %xmm5
;   pmuludq %xmm5, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

