test compile precise-output
target x86_64 sse42 has_avx

function %sload8x8(i64) -> i16x8 {
block0(v0: i64):
  v1 = sload8x8 v0
  return v1
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vpmovsxbw (%rdi), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpmovsxbw (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %uload8x8(i64) -> i16x8 {
block0(v0: i64):
  v1 = uload8x8 v0
  return v1
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vpmovzxbw (%rdi), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpmovzxbw (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %sload16x4(i64) -> i32x4 {
block0(v0: i64):
  v1 = sload16x4 v0
  return v1
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vpmovsxwd (%rdi), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpmovsxwd (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %uload16x4(i64) -> i32x4 {
block0(v0: i64):
  v1 = uload16x4 v0
  return v1
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vpmovzxwd (%rdi), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpmovzxwd (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %sload32x2(i64) -> i64x2 {
block0(v0: i64):
  v1 = sload32x2 v0
  return v1
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vpmovsxdq (%rdi), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpmovsxdq (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %uload32x2(i64) -> i64x2 {
block0(v0: i64):
  v1 = uload32x2 v0
  return v1
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vpmovzxdq (%rdi), %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vpmovzxdq (%rdi), %xmm0 ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %load_store_i8x16(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.i8x16 v0
  store v2, v1
  return
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vmovdqu (%rdi), %xmm3
;   vmovdqu %xmm3, (%rsi)
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vmovdqu (%rdi), %xmm3 ; trap: heap_oob
;   vmovdqu %xmm3, (%rsi) ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %load_store_f32x4(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.f32x4 v0
  store v2, v1
  return
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vmovups (%rdi), %xmm3
;   vmovups %xmm3, (%rsi)
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vmovups (%rdi), %xmm3 ; trap: heap_oob
;   vmovups %xmm3, (%rsi) ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %load_store_f64x2(i64, i64) {
block0(v0: i64, v1: i64):
  v2 = load.f64x2 v0
  store v2, v1
  return
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   vmovupd (%rdi), %xmm3
;   vmovupd %xmm3, (%rsi)
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   vmovupd (%rdi), %xmm3 ; trap: heap_oob
;   vmovupd %xmm3, (%rsi) ; trap: heap_oob
;   movq %rbp, %rsp
;   popq %rbp
;   retq

