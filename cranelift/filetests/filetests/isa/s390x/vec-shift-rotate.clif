test compile precise-output
target s390x

function %rotr_i64x4_reg(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
  v2 = rotr.i64x2 v0, v1
  return v2
}

; block0:
;   lcr %r3, %r2
;   verllg %v24, %v24, 0(%r3)
;   br %r14

function %rotr_i64x4_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
  v1 = iconst.i32 17
  v2 = rotr.i64x2 v0, v1
  return v2
}

; block0:
;   verllg %v24, %v24, 47
;   br %r14

function %rotr_i32x4_reg(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = rotr.i32x4 v0, v1
  return v2
}

; block0:
;   lcr %r3, %r2
;   verllf %v24, %v24, 0(%r3)
;   br %r14

function %rotr_i32x4_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
  v1 = iconst.i32 17
  v2 = rotr.i32x4 v0, v1
  return v2
}

; block0:
;   verllf %v24, %v24, 15
;   br %r14

function %rotr_i16x8_reg(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
  v2 = rotr.i16x8 v0, v1
  return v2
}

; block0:
;   lcr %r3, %r2
;   verllh %v24, %v24, 0(%r3)
;   br %r14

function %rotr_i16x8_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
  v1 = iconst.i32 10
  v2 = rotr.i16x8 v0, v1
  return v2
}

; block0:
;   verllh %v24, %v24, 6
;   br %r14

function %rotr_i8x16_reg(i8x16, i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
  v2 = rotr.i8x16 v0, v1
  return v2
}

; block0:
;   lcr %r3, %r2
;   verllb %v24, %v24, 0(%r3)
;   br %r14

function %rotr_i8x16_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i32 3
  v2 = rotr.i8x16 v0, v1
  return v2
}

; block0:
;   verllb %v24, %v24, 5
;   br %r14

function %rotl_i64x2_reg(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
  v2 = rotl.i64x2 v0, v1
  return v2
}

; block0:
;   verllg %v24, %v24, 0(%r2)
;   br %r14

function %rotl_i64x2_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
  v1 = iconst.i32 17
  v2 = rotl.i64x2 v0, v1
  return v2
}

; block0:
;   verllg %v24, %v24, 17
;   br %r14

function %rotl_i32x4_reg(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = rotl.i32x4 v0, v1
  return v2
}

; block0:
;   verllf %v24, %v24, 0(%r2)
;   br %r14

function %rotl_i32x4_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
  v1 = iconst.i32 17
  v2 = rotl.i32x4 v0, v1
  return v2
}

; block0:
;   verllf %v24, %v24, 17
;   br %r14

function %rotl_i16x8_reg(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
  v2 = rotl.i16x8 v0, v1
  return v2
}

; block0:
;   verllh %v24, %v24, 0(%r2)
;   br %r14

function %rotl_i16x8_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
  v1 = iconst.i32 10
  v2 = rotl.i16x8 v0, v1
  return v2
}

; block0:
;   verllh %v24, %v24, 10
;   br %r14

function %rotl_i8x16_reg(i8x16, i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
  v2 = rotl.i8x16 v0, v1
  return v2
}

; block0:
;   verllb %v24, %v24, 0(%r2)
;   br %r14

function %rotr_i8x16_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i32 3
  v2 = rotl.i8x16 v0, v1
  return v2
}

; block0:
;   verllb %v24, %v24, 3
;   br %r14

function %ushr_i64x2_reg(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
  v2 = ushr.i64x2 v0, v1
  return v2
}

; block0:
;   vesrlg %v24, %v24, 0(%r2)
;   br %r14

function %ushr_i64x2_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
  v1 = iconst.i32 17
  v2 = ushr.i64x2 v0, v1
  return v2
}

; block0:
;   vesrlg %v24, %v24, 17
;   br %r14

function %ushr_i32x4_reg(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = ushr.i32x4 v0, v1
  return v2
}

; block0:
;   vesrlf %v24, %v24, 0(%r2)
;   br %r14

function %ushr_i32x4_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
  v1 = iconst.i32 17
  v2 = ushr.i32x4 v0, v1
  return v2
}

; block0:
;   vesrlf %v24, %v24, 17
;   br %r14

function %ushr_i16x8_reg(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
  v2 = ushr.i16x8 v0, v1
  return v2
}

; block0:
;   vesrlh %v24, %v24, 0(%r2)
;   br %r14

function %ushr_i16x8_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
  v1 = iconst.i32 10
  v2 = ushr.i16x8 v0, v1
  return v2
}

; block0:
;   vesrlh %v24, %v24, 10
;   br %r14

function %ushr_i8x16_reg(i8x16, i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
  v2 = ushr.i8x16 v0, v1
  return v2
}

; block0:
;   vesrlb %v24, %v24, 0(%r2)
;   br %r14

function %ushr_i8x16_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i32 3
  v2 = ushr.i8x16 v0, v1
  return v2
}

; block0:
;   vesrlb %v24, %v24, 3
;   br %r14

function %ishl_i64x2_reg(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
  v2 = ishl.i64x2 v0, v1
  return v2
}

; block0:
;   veslg %v24, %v24, 0(%r2)
;   br %r14

function %ishl_i64x2_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
  v1 = iconst.i32 17
  v2 = ishl.i64x2 v0, v1
  return v2
}

; block0:
;   veslg %v24, %v24, 17
;   br %r14

function %ishl_i32x4_reg(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = ishl.i32x4 v0, v1
  return v2
}

; block0:
;   veslf %v24, %v24, 0(%r2)
;   br %r14

function %ishl_i32x4_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
  v1 = iconst.i32 17
  v2 = ishl.i32x4 v0, v1
  return v2
}

; block0:
;   veslf %v24, %v24, 17
;   br %r14

function %ishl_i16x8_reg(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
  v2 = ishl.i16x8 v0, v1
  return v2
}

; block0:
;   veslh %v24, %v24, 0(%r2)
;   br %r14

function %ishl_i16x8_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
  v1 = iconst.i32 10
  v2 = ishl.i16x8 v0, v1
  return v2
}

; block0:
;   veslh %v24, %v24, 10
;   br %r14

function %ishl_i8x16_reg(i8x16, i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
  v2 = ishl.i8x16 v0, v1
  return v2
}

; block0:
;   veslb %v24, %v24, 0(%r2)
;   br %r14

function %ishl_i8x16_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i32 3
  v2 = ishl.i8x16 v0, v1
  return v2
}

; block0:
;   veslb %v24, %v24, 3
;   br %r14

function %sshr_i64x2_reg(i64x2, i64) -> i64x2 {
block0(v0: i64x2, v1: i64):
  v2 = sshr.i64x2 v0, v1
  return v2
}

; block0:
;   vesrag %v24, %v24, 0(%r2)
;   br %r14

function %sshr_i64x2_imm(i64x2) -> i64x2 {
block0(v0: i64x2):
  v1 = iconst.i32 17
  v2 = sshr.i64x2 v0, v1
  return v2
}

; block0:
;   vesrag %v24, %v24, 17
;   br %r14

function %sshr_i32x4_reg(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
  v2 = sshr.i32x4 v0, v1
  return v2
}

; block0:
;   vesraf %v24, %v24, 0(%r2)
;   br %r14

function %sshr_i32x4_imm(i32x4) -> i32x4 {
block0(v0: i32x4):
  v1 = iconst.i32 17
  v2 = sshr.i32x4 v0, v1
  return v2
}

; block0:
;   vesraf %v24, %v24, 17
;   br %r14

function %sshr_i16x8_reg(i16x8, i16) -> i16x8 {
block0(v0: i16x8, v1: i16):
  v2 = sshr.i16x8 v0, v1
  return v2
}

; block0:
;   vesrah %v24, %v24, 0(%r2)
;   br %r14

function %sshr_i16x8_imm(i16x8) -> i16x8 {
block0(v0: i16x8):
  v1 = iconst.i32 10
  v2 = sshr.i16x8 v0, v1
  return v2
}

; block0:
;   vesrah %v24, %v24, 10
;   br %r14

function %sshr_i8x16_reg(i8x16, i8) -> i8x16 {
block0(v0: i8x16, v1: i8):
  v2 = sshr.i8x16 v0, v1
  return v2
}

; block0:
;   vesrab %v24, %v24, 0(%r2)
;   br %r14

function %sshr_i8x16_imm(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i32 3
  v2 = sshr.i8x16 v0, v1
  return v2
}

; block0:
;   vesrab %v24, %v24, 3
;   br %r14

