test compile precise-output
target s390x

function %icmp_eq_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 eq v0, v1
  return v2
}

; block0:
;   vceqg %v24, %v24, %v25
;   br %r14

function %icmp_ne_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 ne v0, v1
  return v2
}

; block0:
;   vceqg %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sgt_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 sgt v0, v1
  return v2
}

; block0:
;   vchg %v24, %v24, %v25
;   br %r14

function %icmp_slt_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 slt v0, v1
  return v2
}

; block0:
;   vchg %v24, %v25, %v24
;   br %r14

function %icmp_sge_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 sge v0, v1
  return v2
}

; block0:
;   vchg %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sle_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 sle v0, v1
  return v2
}

; block0:
;   vchg %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ugt_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 ugt v0, v1
  return v2
}

; block0:
;   vchlg %v24, %v24, %v25
;   br %r14

function %icmp_ult_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 ult v0, v1
  return v2
}

; block0:
;   vchlg %v24, %v25, %v24
;   br %r14

function %icmp_uge_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 uge v0, v1
  return v2
}

; block0:
;   vchlg %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ule_i64x2(i64x2, i64x2) -> b64x2 {
block0(v0: i64x2, v1: i64x2):
  v2 = icmp.i64x2 ule v0, v1
  return v2
}

; block0:
;   vchlg %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_eq_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 eq v0, v1
  return v2
}

; block0:
;   vceqf %v24, %v24, %v25
;   br %r14

function %icmp_ne_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 ne v0, v1
  return v2
}

; block0:
;   vceqf %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sgt_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 sgt v0, v1
  return v2
}

; block0:
;   vchf %v24, %v24, %v25
;   br %r14

function %icmp_slt_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 slt v0, v1
  return v2
}

; block0:
;   vchf %v24, %v25, %v24
;   br %r14

function %icmp_sge_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 sge v0, v1
  return v2
}

; block0:
;   vchf %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sle_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 sle v0, v1
  return v2
}

; block0:
;   vchf %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ugt_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 ugt v0, v1
  return v2
}

; block0:
;   vchlf %v24, %v24, %v25
;   br %r14

function %icmp_ult_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 ult v0, v1
  return v2
}

; block0:
;   vchlf %v24, %v25, %v24
;   br %r14

function %icmp_uge_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 uge v0, v1
  return v2
}

; block0:
;   vchlf %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ule_i32x4(i32x4, i32x4) -> b32x4 {
block0(v0: i32x4, v1: i32x4):
  v2 = icmp.i32x4 ule v0, v1
  return v2
}

; block0:
;   vchlf %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_eq_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 eq v0, v1
  return v2
}

; block0:
;   vceqh %v24, %v24, %v25
;   br %r14

function %icmp_ne_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 ne v0, v1
  return v2
}

; block0:
;   vceqh %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sgt_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 sgt v0, v1
  return v2
}

; block0:
;   vchh %v24, %v24, %v25
;   br %r14

function %icmp_slt_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 slt v0, v1
  return v2
}

; block0:
;   vchh %v24, %v25, %v24
;   br %r14

function %icmp_sge_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 sge v0, v1
  return v2
}

; block0:
;   vchh %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sle_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 sle v0, v1
  return v2
}

; block0:
;   vchh %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ugt_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 ugt v0, v1
  return v2
}

; block0:
;   vchlh %v24, %v24, %v25
;   br %r14

function %icmp_ult_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 ult v0, v1
  return v2
}

; block0:
;   vchlh %v24, %v25, %v24
;   br %r14

function %icmp_uge_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 uge v0, v1
  return v2
}

; block0:
;   vchlh %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ule_i16x8(i16x8, i16x8) -> b16x8 {
block0(v0: i16x8, v1: i16x8):
  v2 = icmp.i16x8 ule v0, v1
  return v2
}

; block0:
;   vchlh %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_eq_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 eq v0, v1
  return v2
}

; block0:
;   vceqb %v24, %v24, %v25
;   br %r14

function %icmp_ne_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 ne v0, v1
  return v2
}

; block0:
;   vceqb %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sgt_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 sgt v0, v1
  return v2
}

; block0:
;   vchb %v24, %v24, %v25
;   br %r14

function %icmp_slt_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 slt v0, v1
  return v2
}

; block0:
;   vchb %v24, %v25, %v24
;   br %r14

function %icmp_sge_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 sge v0, v1
  return v2
}

; block0:
;   vchb %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_sle_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 sle v0, v1
  return v2
}

; block0:
;   vchb %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ugt_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 ugt v0, v1
  return v2
}

; block0:
;   vchlb %v24, %v24, %v25
;   br %r14

function %icmp_ult_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 ult v0, v1
  return v2
}

; block0:
;   vchlb %v24, %v25, %v24
;   br %r14

function %icmp_uge_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 uge v0, v1
  return v2
}

; block0:
;   vchlb %v5, %v25, %v24
;   vno %v24, %v5, %v5
;   br %r14

function %icmp_ule_i8x16(i8x16, i8x16) -> b8x16 {
block0(v0: i8x16, v1: i8x16):
  v2 = icmp.i8x16 ule v0, v1
  return v2
}

; block0:
;   vchlb %v5, %v24, %v25
;   vno %v24, %v5, %v5
;   br %r14

