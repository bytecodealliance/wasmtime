test compile precise-output
target s390x

function %vconst_f32x4_zero() -> f32x4 {
block0:
  v1 = vconst.f32x4 [0x0.0 0x0.0 0x0.0 0x0.0]
  return v1
}

; VCode:
; block0:
;   vgbm %v24, 0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vzero %v24
;   br %r14

function %vconst_f64x2_zero() -> f64x2 {
block0:
  v1 = vconst.f64x2 [0x0.0 0x0.0]
  return v1
}

; VCode:
; block0:
;   vgbm %v24, 0
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vzero %v24
;   br %r14

function %vconst_f32x4_mixed_be() -> f32x4 {
block0:
  v1 = vconst.f32x4 [0x1.0 0x2.0 0x3.0 0x4.0]
  return v1
}

; VCode:
; block0:
;   bras %r1, 20 ; data.u128 0x3f800000400000004040000040800000 ; vl %v24, 0(%r1)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   bras %r1, 0x14
;   sur %f8, %f0
;   .byte 0x00, 0x00
;   sth %r0, 0
;   sth %r4, 0
;   sth %r8, 0
;   vl %v24, 0(%r1)
;   br %r14

function %vconst_f32x4_mixed_le() -> f32x4 tail {
block0:
  v1 = vconst.f32x4 [0x1.0 0x2.0 0x3.0 0x4.0]
  return v1
}

; VCode:
; block0:
;   bras %r1, 20 ; data.u128 0x4080000040400000400000003f800000 ; vl %v24, 0(%r1)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   bras %r1, 0x14
;   sth %r8, 0
;   sth %r4, 0
;   sth %r0, 0
;   sur %f8, %f0
;   .byte 0x00, 0x00
;   vl %v24, 0(%r1)
;   br %r14

function %vconst_f64x2_mixed_be() -> f64x2 {
block0:
  v1 = vconst.f64x2 [0x1.0 0x2.0]
  return v1
}

; VCode:
; block0:
;   bras %r1, 20 ; data.u128 0x3ff00000000000004000000000000000 ; vl %v24, 0(%r1)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   bras %r1, 0x14
;   sur %f15, %f0
;   .byte 0x00, 0x00
;   .byte 0x00, 0x00
;   .byte 0x00, 0x00
;   sth %r0, 0
;   .byte 0x00, 0x00
;   .byte 0x00, 0x00
;   vl %v24, 0(%r1)
;   br %r14

function %vconst_f64x2_mixed_le() -> f64x2 tail {
block0:
  v1 = vconst.f64x2 [0x1.0 0x2.0]
  return v1
}

; VCode:
; block0:
;   bras %r1, 20 ; data.u128 0x40000000000000003ff0000000000000 ; vl %v24, 0(%r1)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   bras %r1, 0x14
;   sth %r0, 0
;   .byte 0x00, 0x00
;   .byte 0x00, 0x00
;   sur %f15, %f0
;   .byte 0x00, 0x00
;   .byte 0x00, 0x00
;   .byte 0x00, 0x00
;   vl %v24, 0(%r1)
;   br %r14

function %fadd_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fadd v0, v1
  return v2
}

; VCode:
; block0:
;   vfasb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfasb %v24, %v24, %v25
;   br %r14

function %fadd_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fadd v0, v1
  return v2
}

; VCode:
; block0:
;   vfadb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfadb %v24, %v24, %v25
;   br %r14

function %fsub_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fsub v0, v1
  return v2
}

; VCode:
; block0:
;   vfssb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfssb %v24, %v24, %v25
;   br %r14

function %fsub_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fsub v0, v1
  return v2
}

; VCode:
; block0:
;   vfsdb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfsdb %v24, %v24, %v25
;   br %r14

function %fmul_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fmul v0, v1
  return v2
}

; VCode:
; block0:
;   vfmsb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmsb %v24, %v24, %v25
;   br %r14

function %fmul_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fmul v0, v1
  return v2
}

; VCode:
; block0:
;   vfmdb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmdb %v24, %v24, %v25
;   br %r14

function %fdiv_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fdiv v0, v1
  return v2
}

; VCode:
; block0:
;   vfdsb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfdsb %v24, %v24, %v25
;   br %r14

function %fdiv_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fdiv v0, v1
  return v2
}

; VCode:
; block0:
;   vfddb %v24, %v24, %v25
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfddb %v24, %v24, %v25
;   br %r14

function %fmin_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fmin v0, v1
  return v2
}

; VCode:
; block0:
;   vfminsb %v24, %v24, %v25, 1
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfminsb %v24, %v24, %v25, 1
;   br %r14

function %fmin_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fmin v0, v1
  return v2
}

; VCode:
; block0:
;   vfmindb %v24, %v24, %v25, 1
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmindb %v24, %v24, %v25, 1
;   br %r14

function %fmax_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fmax v0, v1
  return v2
}

; VCode:
; block0:
;   vfmaxsb %v24, %v24, %v25, 1
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmaxsb %v24, %v24, %v25, 1
;   br %r14

function %fmax_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fmax v0, v1
  return v2
}

; VCode:
; block0:
;   vfmaxdb %v24, %v24, %v25, 1
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmaxdb %v24, %v24, %v25, 1
;   br %r14

function %fmin_pseudo_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp lt v1, v0
  v3 = bitcast.f32x4 v2
  v4 = bitselect v3, v1, v0
  return v4
}

; VCode:
; block0:
;   vfminsb %v24, %v24, %v25, 3
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfminsb %v24, %v24, %v25, 3
;   br %r14

function %fmin_pseudo_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp lt v1, v0
  v3 = bitcast.f64x2 v2
  v4 = bitselect v3, v1, v0
  return v4
}

; VCode:
; block0:
;   vfmindb %v24, %v24, %v25, 3
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmindb %v24, %v24, %v25, 3
;   br %r14

function %fmax_pseudo_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcmp lt v0, v1
  v3 = bitcast.f32x4 v2
  v4 = bitselect v3, v1, v0
  return v4
}

; VCode:
; block0:
;   vfmaxsb %v24, %v24, %v25, 3
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmaxsb %v24, %v24, %v25, 3
;   br %r14

function %fmax_pseudo_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcmp lt v0, v1
  v3 = bitcast.f64x2 v2
  v4 = bitselect v3, v1, v0
  return v4
}

; VCode:
; block0:
;   vfmaxdb %v24, %v24, %v25, 3
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmaxdb %v24, %v24, %v25, 3
;   br %r14

function %sqrt_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = sqrt v0
  return v1
}

; VCode:
; block0:
;   vfsqsb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfsqsb %v24, %v24
;   br %r14

function %sqrt_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = sqrt v0
  return v1
}

; VCode:
; block0:
;   vfsqdb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfsqdb %v24, %v24
;   br %r14

function %fabs_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = fabs v0
  return v1
}

; VCode:
; block0:
;   vflpsb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vflpsb %v24, %v24
;   br %r14

function %fabs_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = fabs v0
  return v1
}

; VCode:
; block0:
;   vflpdb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vflpdb %v24, %v24
;   br %r14

function %fneg_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = fneg v0
  return v1
}

; VCode:
; block0:
;   vflcsb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vflcsb %v24, %v24
;   br %r14

function %fneg_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = fneg v0
  return v1
}

; VCode:
; block0:
;   vflcdb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vflcdb %v24, %v24
;   br %r14

function %fvpromote_low_f32x4_be(f32x4) -> f64x2 {
block0(v0: f32x4):
  v1 = fvpromote_low v0
  return v1
}

; VCode:
; block0:
;   vmrhf %v2, %v24, %v24
;   vldeb %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vmrhf %v2, %v24, %v24
;   vldeb %v24, %v2
;   br %r14

function %fvpromote_low_f32x4_le(f32x4) -> f64x2 tail {
block0(v0: f32x4):
  v1 = fvpromote_low v0
  return v1
}

; VCode:
; block0:
;   vmrlf %v2, %v24, %v24
;   vldeb %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vmrlf %v2, %v24, %v24
;   vldeb %v24, %v2
;   br %r14

function %fvdemote_f64x2_be(f64x2) -> f32x4 {
block0(v0: f64x2):
  v1 = fvdemote v0
  return v1
}

; VCode:
; block0:
;   vledb %v2, %v24, 0, 0
;   vesrlg %v4, %v2, 32
;   vgbm %v6, 0
;   vpkg %v24, %v4, %v6
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vledb %v2, %v24, 0, 0
;   vesrlg %v4, %v2, 0x20
;   vzero %v6
;   vpkg %v24, %v4, %v6
;   br %r14

function %fvdemote_f64x2_le(f64x2) -> f32x4 tail {
block0(v0: f64x2):
  v1 = fvdemote v0
  return v1
}

; VCode:
; block0:
;   vledb %v2, %v24, 0, 0
;   vesrlg %v4, %v2, 32
;   vgbm %v6, 0
;   vpkg %v24, %v6, %v4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vledb %v2, %v24, 0, 0
;   vesrlg %v4, %v2, 0x20
;   vzero %v6
;   vpkg %v24, %v6, %v4
;   br %r14

function %ceil_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = ceil v0
  return v1
}

; VCode:
; block0:
;   vfisb %v24, %v24, 0, 6
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfisb %v24, %v24, 0, 6
;   br %r14

function %ceil_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = ceil v0
  return v1
}

; VCode:
; block0:
;   vfidb %v24, %v24, 0, 6
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfidb %v24, %v24, 0, 6
;   br %r14

function %floor_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = floor v0
  return v1
}

; VCode:
; block0:
;   vfisb %v24, %v24, 0, 7
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfisb %v24, %v24, 0, 7
;   br %r14

function %floor_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = floor v0
  return v1
}

; VCode:
; block0:
;   vfidb %v24, %v24, 0, 7
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfidb %v24, %v24, 0, 7
;   br %r14

function %trunc_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = trunc v0
  return v1
}

; VCode:
; block0:
;   vfisb %v24, %v24, 0, 5
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfisb %v24, %v24, 0, 5
;   br %r14

function %trunc_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = trunc v0
  return v1
}

; VCode:
; block0:
;   vfidb %v24, %v24, 0, 5
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfidb %v24, %v24, 0, 5
;   br %r14

function %nearest_f32x4(f32x4) -> f32x4 {
block0(v0: f32x4):
  v1 = nearest v0
  return v1
}

; VCode:
; block0:
;   vfisb %v24, %v24, 0, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfisb %v24, %v24, 0, 4
;   br %r14

function %nearest_f64x2(f64x2) -> f64x2 {
block0(v0: f64x2):
  v1 = nearest v0
  return v1
}

; VCode:
; block0:
;   vfidb %v24, %v24, 0, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfidb %v24, %v24, 0, 4
;   br %r14

function %fma_f32x4(f32x4, f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4, v2: f32x4):
  v3 = fma v0, v1, v2
  return v3
}

; VCode:
; block0:
;   vfmasb %v24, %v24, %v25, %v26
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmasb %v24, %v24, %v25, %v26
;   br %r14

function %fma_f64x2(f64x2, f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2, v2: f64x2):
  v3 = fma v0, v1, v2
  return v3
}

; VCode:
; block0:
;   vfmadb %v24, %v24, %v25, %v26
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vfmadb %v24, %v24, %v25, %v26
;   br %r14

function %fcopysign_f32x4(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
  v2 = fcopysign v0, v1
  return v2
}

; VCode:
; block0:
;   vgmf %v3, 1, 31
;   vsel %v24, %v24, %v25, %v3
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vgmf %v3, 1, 0x1f
;   vsel %v24, %v24, %v25, %v3
;   br %r14

function %fcopysign_f64x2(f64x2, f64x2) -> f64x2 {
block0(v0: f64x2, v1: f64x2):
  v2 = fcopysign v0, v1
  return v2
}

; VCode:
; block0:
;   vgmg %v3, 1, 63
;   vsel %v24, %v24, %v25, %v3
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vgmg %v3, 1, 0x3f
;   vsel %v24, %v24, %v25, %v3
;   br %r14

function %fcvt_from_uint_i32x4_f32x4(i32x4) -> f32x4 {
block0(v0: i32x4):
  v1 = fcvt_from_uint.f32x4 v0
  return v1
}

; VCode:
; block0:
;   vuplhf %v2, %v24
;   vcdlgb %v4, %v2, 0, 3
;   vledb %v6, %v4, 0, 4
;   vupllf %v16, %v24
;   vcdlgb %v18, %v16, 0, 3
;   vledb %v20, %v18, 0, 4
;   bras %r1, 20 ; data.u128 0x0001020308090a0b1011121318191a1b ; vl %v22, 0(%r1)
;   vperm %v24, %v6, %v20, %v22
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplhf %v2, %v24
;   vcdlgb %v4, %v2, 0, 3
;   vledb %v6, %v4, 0, 4
;   vupllf %v16, %v24
;   vcdlgb %v18, %v16, 0, 3
;   vledb %v20, %v18, 0, 4
;   bras %r1, 0x38
;   .byte 0x00, 0x01
;   .byte 0x02, 0x03
;   .byte 0x08, 0x09
;   svc 0xb
;   lpr %r1, %r1
;   ltr %r1, %r3
;   lr %r1, %r9
;   ar %r1, %r11
;   vl %v22, 0(%r1)
;   vperm %v24, %v6, %v20, %v22
;   br %r14

function %fcvt_from_sint_i32x4_f32x4(i32x4) -> f32x4 {
block0(v0: i32x4):
  v1 = fcvt_from_sint.f32x4 v0
  return v1
}

; VCode:
; block0:
;   vuphf %v2, %v24
;   vcdgb %v4, %v2, 0, 3
;   vledb %v6, %v4, 0, 4
;   vuplf %v16, %v24
;   vcdgb %v18, %v16, 0, 3
;   vledb %v20, %v18, 0, 4
;   bras %r1, 20 ; data.u128 0x0001020308090a0b1011121318191a1b ; vl %v22, 0(%r1)
;   vperm %v24, %v6, %v20, %v22
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuphf %v2, %v24
;   vcdgb %v4, %v2, 0, 3
;   vledb %v6, %v4, 0, 4
;   vuplf %v16, %v24
;   vcdgb %v18, %v16, 0, 3
;   vledb %v20, %v18, 0, 4
;   bras %r1, 0x38
;   .byte 0x00, 0x01
;   .byte 0x02, 0x03
;   .byte 0x08, 0x09
;   svc 0xb
;   lpr %r1, %r1
;   ltr %r1, %r3
;   lr %r1, %r9
;   ar %r1, %r11
;   vl %v22, 0(%r1)
;   vperm %v24, %v6, %v20, %v22
;   br %r14

function %fcvt_from_uint_i64x2_f64x2(i64x2) -> f64x2 {
block0(v0: i64x2):
  v1 = fcvt_from_uint.f64x2 v0
  return v1
}

; VCode:
; block0:
;   vcdlgb %v24, %v24, 0, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vcdlgb %v24, %v24, 0, 4
;   br %r14

function %fcvt_from_sint_i64x2_f64x2(i64x2) -> f64x2 {
block0(v0: i64x2):
  v1 = fcvt_from_sint.f64x2 v0
  return v1
}

; VCode:
; block0:
;   vcdgb %v24, %v24, 0, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vcdgb %v24, %v24, 0, 4
;   br %r14

function %fcvt_low_from_sint_i32x4_f64x2_be(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = swiden_low v0
  v2 = fcvt_from_sint.f64x2 v1
  return v2
}

; VCode:
; block0:
;   vuphf %v3, %v24
;   vcdgb %v24, %v3, 0, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuphf %v3, %v24
;   vcdgb %v24, %v3, 0, 4
;   br %r14

function %fcvt_low_from_sint_i32x4_f64x2_le(i32x4) -> f64x2 tail {
block0(v0: i32x4):
  v1 = swiden_low v0
  v2 = fcvt_from_sint.f64x2 v1
  return v2
}

; VCode:
; block0:
;   vuplf %v3, %v24
;   vcdgb %v24, %v3, 0, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplf %v3, %v24
;   vcdgb %v24, %v3, 0, 4
;   br %r14

function %fcvt_to_uint_sat_f32x4_i32x4(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_uint_sat.i32x4 v0
  return v1
}

; VCode:
; block0:
;   vmrhf %v2, %v24, %v24
;   vldeb %v4, %v2
;   vclgdb %v6, %v4, 0, 5
;   vmrlf %v16, %v24, %v24
;   vldeb %v18, %v16
;   vclgdb %v20, %v18, 0, 5
;   vpklsg %v24, %v6, %v20
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vmrhf %v2, %v24, %v24
;   vldeb %v4, %v2
;   vclgdb %v6, %v4, 0, 5
;   vmrlf %v16, %v24, %v24
;   vldeb %v18, %v16
;   vclgdb %v20, %v18, 0, 5
;   vpklsg %v24, %v6, %v20
;   br %r14

function %fcvt_to_sint_sat_f32x4_i32x4(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_sint_sat.i32x4 v0
  return v1
}

; VCode:
; block0:
;   vmrhf %v2, %v24, %v24
;   vldeb %v4, %v2
;   vcgdb %v6, %v4, 0, 5
;   vmrlf %v16, %v24, %v24
;   vldeb %v18, %v16
;   vcgdb %v20, %v18, 0, 5
;   vpksg %v22, %v6, %v20
;   vgbm %v25, 0
;   vfcesb %v26, %v24, %v24
;   vsel %v24, %v22, %v25, %v26
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vmrhf %v2, %v24, %v24
;   vldeb %v4, %v2
;   vcgdb %v6, %v4, 0, 5
;   vmrlf %v16, %v24, %v24
;   vldeb %v18, %v16
;   vcgdb %v20, %v18, 0, 5
;   vpksg %v22, %v6, %v20
;   vzero %v25
;   vfcesb %v26, %v24, %v24
;   vsel %v24, %v22, %v25, %v26
;   br %r14

function %fcvt_to_uint_sat_f64x2_i64x2(f64x2) -> i64x2 {
block0(v0: f64x2):
  v1 = fcvt_to_uint_sat.i64x2 v0
  return v1
}

; VCode:
; block0:
;   vclgdb %v24, %v24, 0, 5
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vclgdb %v24, %v24, 0, 5
;   br %r14

function %fcvt_to_sint_sat_f64x2_i64x2(f64x2) -> i64x2 {
block0(v0: f64x2):
  v1 = fcvt_to_sint_sat.i64x2 v0
  return v1
}

; VCode:
; block0:
;   vcgdb %v2, %v24, 0, 5
;   vgbm %v4, 0
;   vfcedb %v6, %v24, %v24
;   vsel %v24, %v2, %v4, %v6
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vcgdb %v2, %v24, 0, 5
;   vzero %v4
;   vfcedb %v6, %v24, %v24
;   vsel %v24, %v2, %v4, %v6
;   br %r14

