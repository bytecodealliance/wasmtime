test compile precise-output
target s390x

function %snarrow_i64x2_i32x4(i64x2, i64x2) -> i32x4 tail {
block0(v0: i64x2, v1: i64x2):
  v2 = snarrow.i64x2 v0, v1
  return v2
}

; VCode:
; block0:
;   vpksg %v24, %v25, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpksg %v24, %v25, %v24
;   br %r14

function %snarrow_i32x4_i16x8(i32x4, i32x4) -> i16x8 tail {
block0(v0: i32x4, v1: i32x4):
  v2 = snarrow.i32x4 v0, v1
  return v2
}

; VCode:
; block0:
;   vpksf %v24, %v25, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpksf %v24, %v25, %v24
;   br %r14

function %snarrow_i16x8_i8x16(i16x8, i16x8) -> i8x16 tail {
block0(v0: i16x8, v1: i16x8):
  v2 = snarrow.i16x8 v0, v1
  return v2
}

; VCode:
; block0:
;   vpksh %v24, %v25, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpksh %v24, %v25, %v24
;   br %r14

function %unarrow_i64x2_i32x4(i64x2, i64x2) -> i32x4 tail {
block0(v0: i64x2, v1: i64x2):
  v2 = unarrow.i64x2 v0, v1
  return v2
}

; VCode:
; block0:
;   vgbm %v3, 0
;   vmxg %v5, %v24, %v3
;   vmxg %v7, %v25, %v3
;   vpklsg %v24, %v7, %v5
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vzero %v3
;   vmxg %v5, %v24, %v3
;   vmxg %v7, %v25, %v3
;   vpklsg %v24, %v7, %v5
;   br %r14

function %unarrow_i32x4_i16x8(i32x4, i32x4) -> i16x8 tail {
block0(v0: i32x4, v1: i32x4):
  v2 = unarrow.i32x4 v0, v1
  return v2
}

; VCode:
; block0:
;   vgbm %v3, 0
;   vmxf %v5, %v24, %v3
;   vmxf %v7, %v25, %v3
;   vpklsf %v24, %v7, %v5
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vzero %v3
;   vmxf %v5, %v24, %v3
;   vmxf %v7, %v25, %v3
;   vpklsf %v24, %v7, %v5
;   br %r14

function %unarrow_i16x8_i8x16(i16x8, i16x8) -> i8x16 tail {
block0(v0: i16x8, v1: i16x8):
  v2 = unarrow.i16x8 v0, v1
  return v2
}

; VCode:
; block0:
;   vgbm %v3, 0
;   vmxh %v5, %v24, %v3
;   vmxh %v7, %v25, %v3
;   vpklsh %v24, %v7, %v5
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vzero %v3
;   vmxh %v5, %v24, %v3
;   vmxh %v7, %v25, %v3
;   vpklsh %v24, %v7, %v5
;   br %r14

function %uunarrow_i64x2_i32x4(i64x2, i64x2) -> i32x4 tail {
block0(v0: i64x2, v1: i64x2):
  v2 = uunarrow.i64x2 v0, v1
  return v2
}

; VCode:
; block0:
;   vpklsg %v24, %v25, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpklsg %v24, %v25, %v24
;   br %r14

function %uunarrow_i32x4_i16x8(i32x4, i32x4) -> i16x8 tail {
block0(v0: i32x4, v1: i32x4):
  v2 = uunarrow.i32x4 v0, v1
  return v2
}

; VCode:
; block0:
;   vpklsf %v24, %v25, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpklsf %v24, %v25, %v24
;   br %r14

function %uunarrow_i16x8_i8x16(i16x8, i16x8) -> i8x16 tail {
block0(v0: i16x8, v1: i16x8):
  v2 = uunarrow.i16x8 v0, v1
  return v2
}

; VCode:
; block0:
;   vpklsh %v24, %v25, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpklsh %v24, %v25, %v24
;   br %r14

function %swiden_low_i32x4_i64x2(i32x4) -> i64x2 tail {
block0(v0: i32x4):
  v1 = swiden_low.i32x4 v0
  return v1
}

; VCode:
; block0:
;   vuplf %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplf %v24, %v24
;   br %r14

function %swiden_low_i16x8_i32x4(i16x8) -> i32x4 tail {
block0(v0: i16x8):
  v1 = swiden_low.i16x8 v0
  return v1
}

; VCode:
; block0:
;   vuplh %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplhw %v24, %v24
;   br %r14

function %swiden_low_i8x16_i16x8(i8x16) -> i16x8 tail {
block0(v0: i8x16):
  v1 = swiden_low.i8x16 v0
  return v1
}

; VCode:
; block0:
;   vuplb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplb %v24, %v24
;   br %r14

function %swiden_high_i32x4_i64x2(i32x4) -> i64x2 tail {
block0(v0: i32x4):
  v1 = swiden_high.i32x4 v0
  return v1
}

; VCode:
; block0:
;   vuphf %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuphf %v24, %v24
;   br %r14

function %swiden_high_i16x8_i32x4(i16x8) -> i32x4 tail {
block0(v0: i16x8):
  v1 = swiden_high.i16x8 v0
  return v1
}

; VCode:
; block0:
;   vuphh %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuphh %v24, %v24
;   br %r14

function %swiden_high_i8x16_i16x8(i8x16) -> i16x8 tail {
block0(v0: i8x16):
  v1 = swiden_high.i8x16 v0
  return v1
}

; VCode:
; block0:
;   vuphb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuphb %v24, %v24
;   br %r14

function %uwiden_low_i32x4_i64x2(i32x4) -> i64x2 tail {
block0(v0: i32x4):
  v1 = uwiden_low.i32x4 v0
  return v1
}

; VCode:
; block0:
;   vupllf %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vupllf %v24, %v24
;   br %r14

function %uwiden_low_i16x8_i32x4(i16x8) -> i32x4 tail {
block0(v0: i16x8):
  v1 = uwiden_low.i16x8 v0
  return v1
}

; VCode:
; block0:
;   vupllh %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vupllh %v24, %v24
;   br %r14

function %uwiden_low_i8x16_i16x8(i8x16) -> i16x8 tail {
block0(v0: i8x16):
  v1 = uwiden_low.i8x16 v0
  return v1
}

; VCode:
; block0:
;   vupllb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vupllb %v24, %v24
;   br %r14

function %uwiden_high_i32x4_i64x2(i32x4) -> i64x2 tail {
block0(v0: i32x4):
  v1 = uwiden_high.i32x4 v0
  return v1
}

; VCode:
; block0:
;   vuplhf %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplhf %v24, %v24
;   br %r14

function %uwiden_high_i16x8_i32x4(i16x8) -> i32x4 tail {
block0(v0: i16x8):
  v1 = uwiden_high.i16x8 v0
  return v1
}

; VCode:
; block0:
;   vuplhh %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplhh %v24, %v24
;   br %r14

function %uwiden_high_i8x16_i16x8(i8x16) -> i16x8 tail {
block0(v0: i8x16):
  v1 = uwiden_high.i8x16 v0
  return v1
}

; VCode:
; block0:
;   vuplhb %v24, %v24
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vuplhb %v24, %v24
;   br %r14

