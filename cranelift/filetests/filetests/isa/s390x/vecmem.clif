test compile precise-output
set enable_multi_ret_implicit_sret
target s390x

function %uload8x8_big(i64) -> i16x8 {
block0(v0: i64):
  v1 = uload8x8 big v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuplhb %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuplhb %v24, %v2
;   br %r14

function %uload16x4_big(i64) -> i32x4 {
block0(v0: i64):
  v1 = uload16x4 big v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuplhh %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuplhh %v24, %v2
;   br %r14

function %uload32x2_big(i64) -> i64x2 {
block0(v0: i64):
  v1 = uload32x2 big v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuplhf %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuplhf %v24, %v2
;   br %r14

function %sload8x8_big(i64) -> i16x8 {
block0(v0: i64):
  v1 = sload8x8 big v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuphb %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuphb %v24, %v2
;   br %r14

function %sload16x4_big(i64) -> i32x4 {
block0(v0: i64):
  v1 = sload16x4 big v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuphh %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuphh %v24, %v2
;   br %r14

function %sload32x2_big(i64) -> i64x2 {
block0(v0: i64):
  v1 = sload32x2 big v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuphf %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuphf %v24, %v2
;   br %r14

function %load_i8x16_big(i64) -> i8x16 {
block0(v0: i64):
  v1 = load.i8x16 big v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %load_i16x8_big(i64) -> i16x8 {
block0(v0: i64):
  v1 = load.i16x8 big v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %load_i32x4_big(i64) -> i32x4 {
block0(v0: i64):
  v1 = load.i32x4 big v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %load_i64x2_big(i64) -> i64x2 {
block0(v0: i64):
  v1 = load.i64x2 big v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %load_i128_big(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 big v0
  return v1
}

; VCode:
; block0:
;   vl %v3, 0(%r3)
;   vst %v3, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v3, 0(%r3) ; trap: heap_oob
;   vst %v3, 0(%r2)
;   br %r14

function %load_f32x4_big(i64) -> f32x4 {
block0(v0: i64):
  v1 = load.f32x4 big v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %load_f64x2_big(i64) -> f64x2 {
block0(v0: i64):
  v1 = load.f64x2 big v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_i8x16_big(i8x16, i64) {
block0(v0: i8x16, v1: i64):
  store.i8x16 big v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_i16x8_big(i16x8, i64) {
block0(v0: i16x8, v1: i64):
  store.i16x8 big v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_i32x4_big(i32x4, i64) {
block0(v0: i32x4, v1: i64):
  store.i32x4 big v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_i64x2_big(i64x2, i64) {
block0(v0: i64x2, v1: i64):
  store.i64x2 big v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_i128_big(i128, i64) {
block0(v0: i128, v1: i64):
  store.i128 big v0, v1
  return
}

; VCode:
; block0:
;   vl %v1, 0(%r2)
;   vst %v1, 0(%r3)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v1, 0(%r2)
;   vst %v1, 0(%r3) ; trap: heap_oob
;   br %r14

function %store_f32x4_big(f32x4, i64) {
block0(v0: f32x4, v1: i64):
  store.f32x4 big v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_f64x2_big(f64x2, i64) {
block0(v0: f64x2, v1: i64):
  store.f64x2 big v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %uload8x8_little(i64) -> i16x8 {
block0(v0: i64):
  v1 = uload8x8 little v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuplhb %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuplhb %v24, %v2
;   br %r14

function %uload16x4_little(i64) -> i32x4 {
block0(v0: i64):
  v1 = uload16x4 little v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   verllh %v4, %v2, 8
;   vuplhh %v24, %v4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   verllh %v4, %v2, 8
;   vuplhh %v24, %v4
;   br %r14

function %uload32x2_little(i64) -> i64x2 {
block0(v0: i64):
  v1 = uload32x2 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   ldgr %f4, %r4
;   verllg %v6, %v4, 32
;   vuplhf %v24, %v6
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   ldgr %f4, %r4
;   verllg %v6, %v4, 0x20
;   vuplhf %v24, %v6
;   br %r14

function %sload8x8_little(i64) -> i16x8 {
block0(v0: i64):
  v1 = sload8x8 little v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   vuphb %v24, %v2
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   vuphb %v24, %v2
;   br %r14

function %sload16x4_little(i64) -> i32x4 {
block0(v0: i64):
  v1 = sload16x4 little v0
  return v1
}

; VCode:
; block0:
;   ld %f2, 0(%r2)
;   verllh %v4, %v2, 8
;   vuphh %v24, %v4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   ld %f2, 0(%r2) ; trap: heap_oob
;   verllh %v4, %v2, 8
;   vuphh %v24, %v4
;   br %r14

function %sload32x2_little(i64) -> i64x2 {
block0(v0: i64):
  v1 = sload32x2 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   ldgr %f4, %r4
;   verllg %v6, %v4, 32
;   vuphf %v24, %v6
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   ldgr %f4, %r4
;   verllg %v6, %v4, 0x20
;   vuphf %v24, %v6
;   br %r14

function %load_i8x16_little(i64) -> i8x16 {
block0(v0: i64):
  v1 = load.i8x16 little v0
  return v1
}

; VCode:
; block0:
;   vl %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %load_i16x8_little(i64) -> i16x8 {
block0(v0: i64):
  v1 = load.i16x8 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   lrvg %r2, 8(%r2)
;   vlvgp %v6, %r2, %r4
;   vpdi %v16, %v6, %v6, 4
;   verllg %v18, %v16, 32
;   verllf %v24, %v18, 16
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   lrvg %r2, 8(%r2) ; trap: heap_oob
;   vlvgp %v6, %r2, %r4
;   vpdi %v16, %v6, %v6, 4
;   verllg %v18, %v16, 0x20
;   verllf %v24, %v18, 0x10
;   br %r14

function %load_i32x4_little(i64) -> i32x4 {
block0(v0: i64):
  v1 = load.i32x4 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   lrvg %r2, 8(%r2)
;   vlvgp %v6, %r2, %r4
;   vpdi %v16, %v6, %v6, 4
;   verllg %v24, %v16, 32
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   lrvg %r2, 8(%r2) ; trap: heap_oob
;   vlvgp %v6, %r2, %r4
;   vpdi %v16, %v6, %v6, 4
;   verllg %v24, %v16, 0x20
;   br %r14

function %load_i64x2_little(i64) -> i64x2 {
block0(v0: i64):
  v1 = load.i64x2 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   lrvg %r2, 8(%r2)
;   vlvgp %v6, %r2, %r4
;   vpdi %v24, %v6, %v6, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   lrvg %r2, 8(%r2) ; trap: heap_oob
;   vlvgp %v6, %r2, %r4
;   vpdi %v24, %v6, %v6, 4
;   br %r14

function %load_i128_little(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r5, 0(%r3)
;   lrvg %r3, 8(%r3)
;   vlvgp %v7, %r3, %r5
;   vst %v7, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r5, 0(%r3) ; trap: heap_oob
;   lrvg %r3, 8(%r3) ; trap: heap_oob
;   vlvgp %v7, %r3, %r5
;   vst %v7, 0(%r2)
;   br %r14

function %load_f32x4_little(i64) -> f32x4 {
block0(v0: i64):
  v1 = load.f32x4 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   lrvg %r2, 8(%r2)
;   vlvgp %v6, %r2, %r4
;   vpdi %v16, %v6, %v6, 4
;   verllg %v24, %v16, 32
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   lrvg %r2, 8(%r2) ; trap: heap_oob
;   vlvgp %v6, %r2, %r4
;   vpdi %v16, %v6, %v6, 4
;   verllg %v24, %v16, 0x20
;   br %r14

function %load_f64x2_little(i64) -> f64x2 {
block0(v0: i64):
  v1 = load.f64x2 little v0
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 0(%r2)
;   lrvg %r2, 8(%r2)
;   vlvgp %v6, %r2, %r4
;   vpdi %v24, %v6, %v6, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0(%r2) ; trap: heap_oob
;   lrvg %r2, 8(%r2) ; trap: heap_oob
;   vlvgp %v6, %r2, %r4
;   vpdi %v24, %v6, %v6, 4
;   br %r14

function %load_f64x2_sum_little(i64, i64) -> f64x2 {
block0(v0: i64, v1: i64):
  v2 = iadd.i64 v0, v1
  v3 = load.f64x2 little v2
  return v3
}

; VCode:
; block0:
;   lrvg %r5, 0(%r3,%r2)
;   lrvg %r3, 8(%r3,%r2)
;   vlvgp %v7, %r3, %r5
;   vpdi %v24, %v7, %v7, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r5, 0(%r3, %r2) ; trap: heap_oob
;   lrvg %r3, 8(%r3, %r2) ; trap: heap_oob
;   vlvgp %v7, %r3, %r5
;   vpdi %v24, %v7, %v7, 4
;   br %r14

function %load_f64x2_off_little(i64) -> f64x2 {
block0(v0: i64):
  v1 = load.f64x2 little v0+128
  return v1
}

; VCode:
; block0:
;   lrvg %r4, 128(%r2)
;   lrvg %r2, 136(%r2)
;   vlvgp %v6, %r2, %r4
;   vpdi %v24, %v6, %v6, 4
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   lrvg %r4, 0x80(%r2) ; trap: heap_oob
;   lrvg %r2, 0x88(%r2) ; trap: heap_oob
;   vlvgp %v6, %r2, %r4
;   vpdi %v24, %v6, %v6, 4
;   br %r14

function %store_i8x16_little(i8x16, i64) {
block0(v0: i8x16, v1: i64):
  store.i8x16 little v0, v1
  return
}

; VCode:
; block0:
;   vst %v24, 0(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vst %v24, 0(%r2) ; trap: heap_oob
;   br %r14

function %store_i16x8_little(i16x8, i64) {
block0(v0: i16x8, v1: i64):
  store.i16x8 little v0, v1
  return
}

; VCode:
; block0:
;   vpdi %v3, %v24, %v24, 4
;   verllg %v5, %v3, 32
;   verllf %v7, %v5, 16
;   vlgvg %r3, %v7, 1
;   lgdr %r5, %f7
;   strvg %r3, 0(%r2)
;   strvg %r5, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v3, %v24, %v24, 4
;   verllg %v5, %v3, 0x20
;   verllf %v7, %v5, 0x10
;   vlgvg %r3, %v7, 1
;   lgdr %r5, %f7
;   strvg %r3, 0(%r2) ; trap: heap_oob
;   strvg %r5, 8(%r2) ; trap: heap_oob
;   br %r14

function %store_i32x4_little(i32x4, i64) {
block0(v0: i32x4, v1: i64):
  store.i32x4 little v0, v1
  return
}

; VCode:
; block0:
;   vpdi %v3, %v24, %v24, 4
;   verllg %v5, %v3, 32
;   vlgvg %r5, %v5, 1
;   lgdr %r3, %f5
;   strvg %r5, 0(%r2)
;   strvg %r3, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v3, %v24, %v24, 4
;   verllg %v5, %v3, 0x20
;   vlgvg %r5, %v5, 1
;   lgdr %r3, %f5
;   strvg %r5, 0(%r2) ; trap: heap_oob
;   strvg %r3, 8(%r2) ; trap: heap_oob
;   br %r14

function %store_i64x2_little(i64x2, i64) {
block0(v0: i64x2, v1: i64):
  store.i64x2 little v0, v1
  return
}

; VCode:
; block0:
;   vpdi %v3, %v24, %v24, 4
;   vlgvg %r3, %v3, 1
;   lgdr %r5, %f3
;   strvg %r3, 0(%r2)
;   strvg %r5, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v3, %v24, %v24, 4
;   vlgvg %r3, %v3, 1
;   lgdr %r5, %f3
;   strvg %r3, 0(%r2) ; trap: heap_oob
;   strvg %r5, 8(%r2) ; trap: heap_oob
;   br %r14

function %store_i128_little(i128, i64) {
block0(v0: i128, v1: i64):
  store.i128 little v0, v1
  return
}

; VCode:
; block0:
;   vl %v1, 0(%r2)
;   vlgvg %r2, %v1, 1
;   lgdr %r4, %f1
;   strvg %r2, 0(%r3)
;   strvg %r4, 8(%r3)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vl %v1, 0(%r2)
;   vlgvg %r2, %v1, 1
;   lgdr %r4, %f1
;   strvg %r2, 0(%r3) ; trap: heap_oob
;   strvg %r4, 8(%r3) ; trap: heap_oob
;   br %r14

function %store_f32x4_little(f32x4, i64) {
block0(v0: f32x4, v1: i64):
  store.f32x4 little v0, v1
  return
}

; VCode:
; block0:
;   vpdi %v3, %v24, %v24, 4
;   verllg %v5, %v3, 32
;   vlgvg %r5, %v5, 1
;   lgdr %r3, %f5
;   strvg %r5, 0(%r2)
;   strvg %r3, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v3, %v24, %v24, 4
;   verllg %v5, %v3, 0x20
;   vlgvg %r5, %v5, 1
;   lgdr %r3, %f5
;   strvg %r5, 0(%r2) ; trap: heap_oob
;   strvg %r3, 8(%r2) ; trap: heap_oob
;   br %r14

function %store_f64x2_little(f64x2, i64) {
block0(v0: f64x2, v1: i64):
  store.f64x2 little v0, v1
  return
}

; VCode:
; block0:
;   vpdi %v3, %v24, %v24, 4
;   vlgvg %r3, %v3, 1
;   lgdr %r5, %f3
;   strvg %r3, 0(%r2)
;   strvg %r5, 8(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v3, %v24, %v24, 4
;   vlgvg %r3, %v3, 1
;   lgdr %r5, %f3
;   strvg %r3, 0(%r2) ; trap: heap_oob
;   strvg %r5, 8(%r2) ; trap: heap_oob
;   br %r14

function %store_f64x2_sum_little(f64x2, i64, i64) {
block0(v0: f64x2, v1: i64, v2: i64):
  v3 = iadd.i64 v1, v2
  store.f64x2 little v0, v3
  return
}

; VCode:
; block0:
;   vpdi %v4, %v24, %v24, 4
;   vlgvg %r5, %v4, 1
;   lgdr %r4, %f4
;   strvg %r5, 0(%r3,%r2)
;   strvg %r4, 8(%r3,%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v4, %v24, %v24, 4
;   vlgvg %r5, %v4, 1
;   lgdr %r4, %f4
;   strvg %r5, 0(%r3, %r2) ; trap: heap_oob
;   strvg %r4, 8(%r3, %r2) ; trap: heap_oob
;   br %r14

function %store_f64x2_off_little(f64x2, i64) {
block0(v0: f64x2, v1: i64):
  store.f64x2 little v0, v1+128
  return
}

; VCode:
; block0:
;   vpdi %v3, %v24, %v24, 4
;   vlgvg %r3, %v3, 1
;   lgdr %r5, %f3
;   strvg %r3, 128(%r2)
;   strvg %r5, 136(%r2)
;   br %r14
;
; Disassembled:
; block0: ; offset 0x0
;   vpdi %v3, %v24, %v24, 4
;   vlgvg %r3, %v3, 1
;   lgdr %r5, %f3
;   strvg %r3, 0x80(%r2) ; trap: heap_oob
;   strvg %r5, 0x88(%r2) ; trap: heap_oob
;   br %r14

