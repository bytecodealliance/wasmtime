test compile precise-output
set unwind_info=false
target aarch64

function %shuffle_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [3 0 31 26 4 6 12 11 23 13 24 4 2 15 17 5]
    return v2
}

; VCode:
; block0:
;   ldr q3, [const(0)]
;   mov v30.16b, v0.16b
;   mov v31.16b, v1.16b
;   tbl v0.16b, { v30.16b, v31.16b }, v3.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr q3, #0x20
;   mov v30.16b, v0.16b
;   mov v31.16b, v1.16b
;   tbl v0.16b, {v30.16b, v31.16b}, v3.16b
;   ret
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   adc w3, w0, wzr
;   add w4, w16, w12, lsl #1
;   orr z23.b, p3/m, z23.b, z8.b
;   mov z2.b, p1/z, #0x78

function %aarch64_uzp1_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30]
    return v2
}

; VCode:
; block0:
;   uzp1 v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp1 v0.16b, v0.16b, v1.16b
;   ret

function %aarch64_uzp2_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31]
    return v2
}

; VCode:
; block0:
;   uzp2 v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp2 v0.16b, v0.16b, v1.16b
;   ret

function %aarch64_uzp1_i16x8(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 4 5 8 9 12 13 16 17 20 21 24 25 28 29]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   uzp1 v0.8h, v0.8h, v1.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp1 v0.8h, v0.8h, v1.8h
;   ret

function %aarch64_uzp2_i16x8(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [2 3 6 7 10 11 14 15 18 19 22 23 26 27 30 31]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   uzp2 v0.8h, v0.8h, v1.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp2 v0.8h, v0.8h, v1.8h
;   ret

function %aarch64_uzp1_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 8 9 10 11 16 17 18 19 24 25 26 27]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   uzp1 v0.4s, v0.4s, v1.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp1 v0.4s, v0.4s, v1.4s
;   ret

function %aarch64_uzp2_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [4 5 6 7 12 13 14 15 20 21 22 23 28 29 30 31]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   uzp2 v0.4s, v0.4s, v1.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp2 v0.4s, v0.4s, v1.4s
;   ret

function %aarch64_uzp1_i64x2(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   uzp1 v0.2d, v0.2d, v1.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp1 v0.2d, v0.2d, v1.2d
;   ret

function %aarch64_uzp2_i64x2(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   uzp2 v0.2d, v0.2d, v1.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp2 v0.2d, v0.2d, v1.2d
;   ret

function %punpcklbw(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [0 16 1 17 2 18 3 19 4 20 5 21 6 22 7 23]
    return v2
}

; VCode:
; block0:
;   zip1 v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   zip1 v0.16b, v0.16b, v1.16b
;   ret

function %punpckhbw(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [8 24 9 25 10 26 11 27 12 28 13 29 14 30 15 31]
    return v2
}

; VCode:
; block0:
;   zip2 v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   zip2 v0.16b, v0.16b, v1.16b
;   ret

function %punpcklwd(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 16 17 2 3 18 19 4 5 20 21 6 7 22 23]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   zip1 v0.8h, v0.8h, v1.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   zip1 v0.8h, v0.8h, v1.8h
;   ret

function %punpckhwd(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [8 9 24 25 10 11 26 27 12 13 28 29 14 15 30 31]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   zip2 v0.8h, v0.8h, v1.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   zip2 v0.8h, v0.8h, v1.8h
;   ret

function %punpckldq(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 16 17 18 19 4 5 6 7 20 21 22 23]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   zip1 v0.4s, v0.4s, v1.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   zip1 v0.4s, v0.4s, v1.4s
;   ret

function %punpckhdq(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [8 9 10 11 24 25 26 27 12 13 14 15 28 29 30 31]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   zip2 v0.4s, v0.4s, v1.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   zip2 v0.4s, v0.4s, v1.4s
;   ret

function %punpcklqdq(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   uzp1 v0.2d, v0.2d, v1.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp1 v0.2d, v0.2d, v1.2d
;   ret

function %punpckhqdq(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   uzp2 v0.2d, v0.2d, v1.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp2 v0.2d, v0.2d, v1.2d
;   ret

function %aarch64_trn1_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [0 16 2 18 4 20 6 22 8 24 10 26 12 28 14 30]
    return v2
}

; VCode:
; block0:
;   trn1 v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   trn1 v0.16b, v0.16b, v1.16b
;   ret

function %aarch64_trn2_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [1 17 3 19 5 21 7 23 9 25 11 27 13 29 15 31]
    return v2
}

; VCode:
; block0:
;   trn2 v0.16b, v0.16b, v1.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   trn2 v0.16b, v0.16b, v1.16b
;   ret

function %aarch64_trn1_i16x8(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 16 17 4 5 20 21 8 9 24 25 12 13 28 29]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   trn1 v0.8h, v0.8h, v1.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   trn1 v0.8h, v0.8h, v1.8h
;   ret

function %aarch64_trn2_i16x8(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [2 3 18 19 6 7 22 23 10 11 26 27 14 15 30 31]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   trn2 v0.8h, v0.8h, v1.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   trn2 v0.8h, v0.8h, v1.8h
;   ret

function %aarch64_trn1_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 16 17 18 19 8 9 10 11 24 25 26 27]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   trn1 v0.4s, v0.4s, v1.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   trn1 v0.4s, v0.4s, v1.4s
;   ret

function %aarch64_trn2_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [4 5 6 7 20 21 22 23 12 13 14 15 28 29 30 31]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   trn2 v0.4s, v0.4s, v1.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   trn2 v0.4s, v0.4s, v1.4s
;   ret

function %aarch64_trn1_i64x2(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   uzp1 v0.2d, v0.2d, v1.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp1 v0.2d, v0.2d, v1.2d
;   ret

function %aarch64_trn2_i64x2(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   uzp2 v0.2d, v0.2d, v1.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   uzp2 v0.2d, v0.2d, v1.2d
;   ret

function %aarch64_ext_0(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    return v2
}

; VCode:
; block0:
;   ext v0.16b, v0.16b, v1.16b, #0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ext v0.16b, v0.16b, v1.16b, #0
;   ret

function %aarch64_ext_1(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16]
    return v2
}

; VCode:
; block0:
;   ext v0.16b, v0.16b, v1.16b, #1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ext v0.16b, v0.16b, v1.16b, #1
;   ret

function %aarch64_ext_5(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20]
    return v2
}

; VCode:
; block0:
;   ext v0.16b, v0.16b, v1.16b, #5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ext v0.16b, v0.16b, v1.16b, #5
;   ret

function %aarch64_ext_11(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26]
    return v2
}

; VCode:
; block0:
;   ext v0.16b, v0.16b, v1.16b, #11
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ext v0.16b, v0.16b, v1.16b, #0xb
;   ret

function %aarch64_ext_16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31]
    return v2
}

; VCode:
; block0:
;   ldr q3, [const(0)]
;   mov v30.16b, v0.16b
;   mov v31.16b, v1.16b
;   tbl v0.16b, { v30.16b, v31.16b }, v3.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr q3, #0x20
;   mov v30.16b, v0.16b
;   mov v31.16b, v1.16b
;   tbl v0.16b, {v30.16b, v31.16b}, v3.16b
;   ret
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   .byte 0x00, 0x00, 0x00, 0x00
;   sbfiz w16, w8, #0xe, #5
;   b #0xfffffffffc585474
;   madd w24, w8, w26, w6
;   fmadd s28, s8, s30, s7

function %aarch64_dup_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = shuffle v0, v1, [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
    return v2
}

; VCode:
; block0:
;   dup v0.16b, v0.b[5]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v0.16b, v0.b[5]
;   ret

function %aarch64_dup_i16x8(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [12 13 12 13 12 13 12 13 12 13 12 13 12 13 12 13]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   dup v0.8h, v0.h[6]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v0.8h, v0.h[6]
;   ret

function %aarch64_dup_i32x4(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [12 13 14 15 12 13 14 15 12 13 14 15 12 13 14 15]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   dup v0.4s, v0.s[3]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v0.4s, v0.s[3]
;   ret

function %aarch64_dup_i64x2(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   dup v0.2d, v0.d[0]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v0.2d, v0.d[0]
;   ret

function %aarch64_rev16(i16x8, i16x8) -> i16x8 {
block0(v0: i16x8, v1: i16x8):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [1 0 3 2 5 4 7 6 9 8 11 10 13 12 15 14]
    v5 = bitcast.i16x8 little v4
    return v5
}

; VCode:
; block0:
;   rev16 v0.16b, v0.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   rev16 v0.16b, v0.16b
;   ret

function %aarch64_rev32_bytes(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [3 2 1 0 7 6 5 4 11 10 9 8 15 14 13 12]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   rev32 v0.16b, v0.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   rev32 v0.16b, v0.16b
;   ret

function %aarch64_rev32_words(i32x4, i32x4) -> i32x4 {
block0(v0: i32x4, v1: i32x4):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [2 3 0 1 6 7 4 5 10 11 8 9 14 15 12 13]
    v5 = bitcast.i32x4 little v4
    return v5
}

; VCode:
; block0:
;   rev32 v0.8h, v0.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   rev32 v0.8h, v0.8h
;   ret

function %aarch64_rev64_bytes(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [7 6 5 4 3 2 1 0 15 14 13 12 11 10 9 8]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   rev64 v0.16b, v0.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   rev64 v0.16b, v0.16b
;   ret

function %aarch64_rev64_words(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [6 7 4 5 2 3 0 1 14 15 12 13 10 11 8 9]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   rev64 v0.8h, v0.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   rev64 v0.8h, v0.8h
;   ret

function %aarch64_rev64_doublewords(i64x2, i64x2) -> i64x2 {
block0(v0: i64x2, v1: i64x2):
    v2 = bitcast.i8x16 little v0
    v3 = bitcast.i8x16 little v1
    v4 = shuffle v2, v3, [4 5 6 7 0 1 2 3 12 13 14 15 8 9 10 11]
    v5 = bitcast.i64x2 little v4
    return v5
}

; VCode:
; block0:
;   rev64 v0.4s, v0.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   rev64 v0.4s, v0.4s
;   ret

