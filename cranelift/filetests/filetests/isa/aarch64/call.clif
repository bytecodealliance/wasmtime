test compile
set unwind_info=false
set enable_probestack=false
target aarch64

function %f1(i64) -> i64 {
    fn0 = %g(i64) -> i64

block0(v0: i64):
    v1 = call fn0(v0)
    return v1
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  ldr x1, 8 ; b 12 ; data
; nextln:  blr x1
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f2(i32) -> i64 {
    fn0 = %g(i32 uext) -> i64 baldrdash_system_v

block0(v0: i32):
    v1 = call fn0(v0)
    return v1
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; check:  mov w0, w0
; nextln:  ldr x1, 8 ; b 12 ; data
; nextln:  blr x1
; check:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f3(i32) -> i32 uext baldrdash_system_v {
block0(v0: i32):
    return v0
}

; check:  mov w0, w0

function %f4(i32) -> i64 {
    fn0 = %g(i32 sext) -> i64 baldrdash_system_v

block0(v0: i32):
    v1 = call fn0(v0)
    return v1
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; check:  sxtw x0, w0
; nextln:  ldr x1, 8 ; b 12 ; data
; nextln:  blr x1
; check:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f5(i32) -> i32 sext baldrdash_system_v {
block0(v0: i32):
    return v0
}

; check:  sxtw x0, w0

function %f6(i8) -> i64 {
    fn0 = %g(i32, i32, i32, i32, i32, i32, i32, i32, i8 sext) -> i64

block0(v0: i8):
    v1 = iconst.i32 42
    v2 = call fn0(v1, v1, v1, v1, v1, v1, v1, v1, v0)
    return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  mov x8, x0
; nextln:  sub sp, sp, #16
; nextln:  virtual_sp_offset_adjust 16
; nextln:  movz x0, #42
; nextln:  movz x1, #42
; nextln:  movz x2, #42
; nextln:  movz x3, #42
; nextln:  movz x4, #42
; nextln:  movz x5, #42
; nextln:  movz x6, #42
; nextln:  movz x7, #42
; nextln:  sturb w8, [sp]
; nextln:  ldr x8, 8 ; b 12 ; data
; nextln:  blr x8
; nextln:  add sp, sp, #16
; nextln:  virtual_sp_offset_adjust -16
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f7(i8) -> i32, i32, i32, i32, i32, i32, i32, i32, i8 sext {
block0(v0: i8):
    v1 = iconst.i32 42
    return v1, v1, v1, v1, v1, v1, v1, v1, v0
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  mov x9, x0
; nextln:  mov x8, x1
; nextln:  movz x0, #42
; nextln:  movz x1, #42
; nextln:  movz x2, #42
; nextln:  movz x3, #42
; nextln:  movz x4, #42
; nextln:  movz x5, #42
; nextln:  movz x6, #42
; nextln:  movz x7, #42
; nextln:  sturb w9, [x8]
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f8() {
    fn0 = %g0() -> f32
    fn1 = %g1() -> f64
    fn2 = %g2()
    fn3 = %g3(f32)
    fn4 = %g4(f64)

block0:
    v0 = call fn0()
    v1 = call fn1()
    v2 = call fn1()
    call fn2()
    call fn3(v0)
    call fn4(v1)
    call fn4(v2)
    return
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sub sp, sp, #32
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str s0, [sp]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str d0, [sp, #8]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str d0, [sp, #16]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr s0, [sp]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr d0, [sp, #8]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr d0, [sp, #16]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  add sp, sp, #32
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f9() {
    fn0 = %g0() -> i8x16
    fn1 = %g1()
    fn2 = %g2(i8x16)

block0:
    v0 = call fn0()
    v1 = call fn0()
    v2 = call fn0()
    call fn1()
    call fn2(v0)
    call fn2(v1)
    call fn2(v2)
    return
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sub sp, sp, #48
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str q0, [sp]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str q0, [sp, #16]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str q0, [sp, #32]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr q0, [sp]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr q0, [sp, #16]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr q0, [sp, #32]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  add sp, sp, #48
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f10() {
    fn0 = %g0() -> f32
    fn1 = %g1() -> f64
    fn2 = %g2() -> i8x16
    fn3 = %g3()
    fn4 = %g4(f32)
    fn5 = %g5(f64)
    fn6 = %g6(i8x16)

block0:
    v0 = call fn0()
    v1 = call fn1()
    v2 = call fn2()
    call fn3()
    call fn4(v0)
    call fn5(v1)
    call fn6(v2)
    return
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sub sp, sp, #32
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str s0, [sp]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str d0, [sp, #8]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  str q0, [sp, #16]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr s0, [sp]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr d0, [sp, #8]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  ldr q0, [sp, #16]
; nextln:  ldr x0, 8 ; b 12 ; data
; nextln:  blr x0
; nextln:  add sp, sp, #32
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret


; i128 tests
function %f11(i128, i64) -> i64 {
block0(v0: i128, v1: i64):
    v2, v3 = isplit v0
    return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: mov x0, x1
; nextln: ldp fp, lr, [sp], #16
; nextln: ret


function %f11_call(i64) -> i64 {
    fn0 = %f11(i128, i64) -> i64

block0(v0: i64):
    v1 = iconst.i64 42
    v2 = iconcat v1, v0
    v3 = call fn0(v2, v1)
    return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: mov x1, x0
; nextln: movz x0, #42
; nextln: movz x2, #42
; nextln: ldr x3, 8 ; b 12 ; data
; nextln: blr x3
; nextln: ldp fp, lr, [sp], #16
; nextln: ret


; The aarch64 abi requires that the i128 argument be aligned
; and to be passed in x2 and x3
function %f12(i64, i128) -> i64 {
block0(v0: i64, v1: i128):
    v2, v3 = isplit v1
    return v2
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: mov x0, x2
; nextln: ldp fp, lr, [sp], #16
; nextln: ret



function %f12_call(i64) -> i64 {
    fn0 = %f12(i64, i128) -> i64

block0(v0: i64):
    v1 = iconst.i64 42
    v2 = iconcat v0, v1
    v3 = call fn0(v1, v2)
    return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: movz x3, #42
; nextln: mov x2, x0
; nextln: movz x0, #42
; nextln: ldr x1, 8 ; b 12 ; data
; nextln: blr x1
; nextln: ldp fp, lr, [sp], #16
; nextln: ret



; aarch64 allows the i128 argument to not be aligned
; and to be passed in x1 and x2
function %f13(i64, i128) -> i64 apple_aarch64 {
block0(v0: i64, v1: i128):
    v2, v3 = isplit v1
    return v2
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: mov x0, x1
; nextln: ldp fp, lr, [sp], #16
; nextln: ret


function %f13_call(i64) -> i64 apple_aarch64 {
    fn0 = %f13(i64, i128) -> i64 apple_aarch64

block0(v0: i64):
    v1 = iconst.i64 42
    v2 = iconcat v0, v1
    v3 = call fn0(v1, v2)
    return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: movz x2, #42
; nextln: mov x1, x0
; nextln: movz x0, #42
; nextln: ldr x3, 8 ; b 12 ; data
; nextln: blr x3
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

