test compile precise-output
target aarch64

function %i8x16_splat_add(i8, i8) -> i8x16 {
  gv0 = dyn_scale_target_const.i8x16
  dt0 = i8x16*gv0

block0(v0: i8, v1: i8):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = iadd v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.16b, w0
;   dup v6.16b, w1
;   add v0.16b, v5.16b, v6.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.16b, w0
;   dup v6.16b, w1
;   add v0.16b, v5.16b, v6.16b
;   ret

function %i16x8_splat_add(i16, i16) -> i16x8 {
  gv0 = dyn_scale_target_const.i16x8
  dt0 = i16x8*gv0

block0(v0: i16, v1: i16):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = iadd v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.8h, w0
;   dup v6.8h, w1
;   add v0.8h, v5.8h, v6.8h
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.8h, w0
;   dup v6.8h, w1
;   add v0.8h, v5.8h, v6.8h
;   ret

function %i32x4_splat_mul(i32, i32) -> i32x4 {
  gv0 = dyn_scale_target_const.i32x4
  dt0 = i32x4*gv0

block0(v0: i32, v1: i32):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = imul v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.4s, w0
;   dup v6.4s, w1
;   mul v0.4s, v5.4s, v6.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.4s, w0
;   dup v6.4s, w1
;   mul v0.4s, v5.4s, v6.4s
;   ret

function %i64x2_splat_sub(i64, i64) -> i64x2 {
  gv0 = dyn_scale_target_const.i64x2
  dt0 = i64x2*gv0

block0(v0: i64, v1: i64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = isub v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.2d, x0
;   dup v6.2d, x1
;   sub v0.2d, v5.2d, v6.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.2d, x0
;   dup v6.2d, x1
;   sub v0.2d, v5.2d, v6.2d
;   ret

function %f32x4_splat_add(f32, f32) -> f32x4 {
  gv0 = dyn_scale_target_const.f32x4
  dt0 = f32x4*gv0

block0(v0: f32, v1: f32):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fadd v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.4s, v0.s[0]
;   dup v6.4s, v1.s[0]
;   fadd v0.4s, v5.4s, v6.4s
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.4s, v0.s[0]
;   dup v6.4s, v1.s[0]
;   fadd v0.4s, v5.4s, v6.4s
;   ret

function %f64x2_splat_sub(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fsub v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fsub v0.2d, v5.2d, v6.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fsub v0.2d, v5.2d, v6.2d
;   ret

function %f64x2_splat_mul(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fmul v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fmul v0.2d, v5.2d, v6.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fmul v0.2d, v5.2d, v6.2d
;   ret

function %f64x2_splat_div(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fdiv v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fdiv v0.2d, v5.2d, v6.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fdiv v0.2d, v5.2d, v6.2d
;   ret

function %f64x2_splat_min(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fmin v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fmin v0.2d, v5.2d, v6.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fmin v0.2d, v5.2d, v6.2d
;   ret

function %f64x2_splat_max(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fmax v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fmax v0.2d, v5.2d, v6.2d
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v5.2d, v0.d[0]
;   dup v6.2d, v1.d[0]
;   fmax v0.2d, v5.2d, v6.2d
;   ret

function %f64x2_splat_min_pseudo(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fmin_pseudo v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v6.2d, v0.d[0]
;   dup v7.2d, v1.d[0]
;   fcmgt v0.2d, v6.2d, v7.2d
;   bsl v0.16b, v0.16b, v7.16b, v6.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v6.2d, v0.d[0]
;   dup v7.2d, v1.d[0]
;   fcmgt v0.2d, v6.2d, v7.2d
;   bsl v0.16b, v7.16b, v6.16b
;   ret

function %f64x2_splat_max_pseudo(f64, f64) -> f64x2 {
  gv0 = dyn_scale_target_const.f64x2
  dt0 = f64x2*gv0

block0(v0: f64, v1: f64):
  v2 = splat.dt0 v0
  v3 = splat.dt0 v1
  v4 = fmax_pseudo v2, v3
  v5 = extract_vector v4, 0
  return v5
}

; VCode:
; block0:
;   dup v6.2d, v0.d[0]
;   dup v7.2d, v1.d[0]
;   fcmgt v0.2d, v7.2d, v6.2d
;   bsl v0.16b, v0.16b, v7.16b, v6.16b
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   dup v6.2d, v0.d[0]
;   dup v7.2d, v1.d[0]
;   fcmgt v0.2d, v7.2d, v6.2d
;   bsl v0.16b, v7.16b, v6.16b
;   ret

