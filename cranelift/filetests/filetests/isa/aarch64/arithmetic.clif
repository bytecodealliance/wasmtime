test compile precise-output
set unwind_info=false
target aarch64

function %f1(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = iadd.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   add x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x0, x0, x1
;   ret

function %f2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = isub.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   sub x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sub x0, x0, x1
;   ret

function %f3(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = imul.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   madd x0, x0, x1, xzr
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mul x0, x0, x1
;   ret

function %umulhi_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = uextend.i128 v0
  v3 = uextend.i128 v1
  v4 = imul v2, v3
  v5, v6 = isplit v4
  return v6
}

; VCode:
; block0:
;   umulh x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   umulh x0, x0, x1
;   ret

function %smulhi_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = sextend.i128 v0
  v3 = sextend.i128 v1
  v4 = imul v2, v3
  v5, v6 = isplit v4
  return v6
}

; VCode:
; block0:
;   smulh x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   smulh x0, x0, x1
;   ret

function %f6(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = sdiv.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   cbz x1, #trap=int_divz
;   adds xzr, x1, #1
;   ccmp x0, #1, #nzcv, eq
;   b.vs #trap=int_ovf
;   sdiv x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   cbz x1, #0x18
;   cmn x1, #1
;   ccmp x0, #1, #0, eq
;   b.vs #0x1c
;   sdiv x0, x0, x1
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_ovf

function %f7(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 2
  v2 = sdiv.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   movz w2, #2
;   sdiv x0, x0, x2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, #2
;   sdiv x0, x0, x2
;   ret

function %f8(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = udiv.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   cbz x1, #trap=int_divz
;   udiv x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   cbz x1, #0xc
;   udiv x0, x0, x1
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz

function %f9(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 2
  v2 = udiv.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   movz x2, #2
;   udiv x0, x0, x2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x2, #2
;   udiv x0, x0, x2
;   ret

function %f10(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = srem.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   cbz x1, #trap=int_divz
;   sdiv x4, x0, x1
;   msub x0, x4, x1, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   cbz x1, #0x10
;   sdiv x4, x0, x1
;   msub x0, x4, x1, x0
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz

function %f11(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = urem.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   cbz x1, #trap=int_divz
;   udiv x4, x0, x1
;   msub x0, x4, x1, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   cbz x1, #0x10
;   udiv x4, x0, x1
;   msub x0, x4, x1, x0
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz

function %f12(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sdiv.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   sxtw x3, w0
;   sxtw x5, w1
;   cbz x5, #trap=int_divz
;   adds wzr, w5, #1
;   ccmp w3, #1, #nzcv, eq
;   b.vs #trap=int_ovf
;   sdiv x0, x3, x5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x3, w0
;   sxtw x5, w1
;   cbz x5, #0x20
;   cmn w5, #1
;   ccmp w3, #1, #0, eq
;   b.vs #0x24
;   sdiv x0, x3, x5
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_ovf

function %f13(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 2
  v2 = sdiv.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   sxtw x2, w0
;   movz w4, #2
;   sdiv x0, x2, x4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x2, w0
;   mov w4, #2
;   sdiv x0, x2, x4
;   ret

function %f14(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = udiv.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   mov w3, w0
;   mov w5, w1
;   cbz x5, #trap=int_divz
;   udiv x0, x3, x5
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w3, w0
;   mov w5, w1
;   cbz x5, #0x14
;   udiv x0, x3, x5
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz

function %f15(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 2
  v2 = udiv.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   mov w2, w0
;   movz w4, #2
;   udiv x0, x2, x4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, w0
;   mov w4, #2
;   udiv x0, x2, x4
;   ret

function %f16(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = srem.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   sxtw x3, w0
;   sxtw x5, w1
;   cbz x5, #trap=int_divz
;   sdiv x8, x3, x5
;   msub x0, x8, x5, x3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x3, w0
;   sxtw x5, w1
;   cbz x5, #0x18
;   sdiv x8, x3, x5
;   msub x0, x8, x5, x3
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz

function %f17(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = urem.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   mov w3, w0
;   mov w5, w1
;   cbz x5, #trap=int_divz
;   udiv x8, x3, x5
;   msub x0, x8, x5, x3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w3, w0
;   mov w5, w1
;   cbz x5, #0x18
;   udiv x8, x3, x5
;   msub x0, x8, x5, x3
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_divz

function %f18(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = band.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   and x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   and x0, x0, x1
;   ret

function %f19(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bor.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   orr x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   orr x0, x0, x1
;   ret

function %f20(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bxor.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   eor x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   eor x0, x0, x1
;   ret

function %f21(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = band_not.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   bic x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   bic x0, x0, x1
;   ret

function %f22(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bor_not.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   orn x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   orn x0, x0, x1
;   ret

function %f23(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bxor_not.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   eon x0, x0, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   eon x0, x0, x1
;   ret

function %f24(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bnot.i64 v0
  return v2
}

; VCode:
; block0:
;   orn x0, xzr, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mvn x0, x0
;   ret

function %f25(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = iconst.i32 53
  v3 = ishl.i32 v0, v2
  v4 = isub.i32 v1, v3
  return v4
}

; VCode:
; block0:
;   sub w0, w1, w0, LSL 21
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sub w0, w1, w0, lsl #21
;   ret

function %f26(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 -1
  v2 = iadd.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   sub w0, w0, #1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sub w0, w0, #1
;   ret

function %f27(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 -1
  v2 = isub.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   add w0, w0, #1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add w0, w0, #1
;   ret

function %f28(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 -1
  v2 = isub.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   add x0, x0, #1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x0, x0, #1
;   ret

function %f29(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 1
  v2 = ineg v1
  return v2
}

; VCode:
; block0:
;   movz x1, #1
;   sub x0, xzr, x1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x1, #1
;   neg x0, x1
;   ret

function %f30(i8x16) -> i8x16 {
block0(v0: i8x16):
  v1 = iconst.i64 1
  v2 = ushr.i8x16 v0, v1
  return v2
}

; VCode:
; block0:
;   ushr v0.16b, v0.16b, #1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ushr v0.16b, v0.16b, #1
;   ret

function %add_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = iadd v0, v1
    return v2
}

; VCode:
; block0:
;   adds x0, x0, x2
;   adc x1, x1, x3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   adds x0, x0, x2
;   adc x1, x1, x3
;   ret

function %sub_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = isub v0, v1
    return v2
}

; VCode:
; block0:
;   subs x0, x0, x2
;   sbc x1, x1, x3
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   subs x0, x0, x2
;   sbc x1, x1, x3
;   ret

function %mul_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = imul v0, v1
    return v2
}

; VCode:
; block0:
;   umulh x5, x0, x2
;   madd x7, x0, x3, x5
;   madd x1, x1, x2, x7
;   madd x0, x0, x2, xzr
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   umulh x5, x0, x2
;   madd x7, x0, x3, x5
;   madd x1, x1, x2, x7
;   mul x0, x0, x2
;   ret

function %add_mul_1(i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32):
    v3 = imul v1, v2
    v4 = iadd v0, v3
    return v4
}

; VCode:
; block0:
;   madd w0, w1, w2, w0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   madd w0, w1, w2, w0
;   ret

function %add_mul_2(i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32):
    v3 = imul v1, v2
    v4 = iadd v3, v0
    return v4
}

; VCode:
; block0:
;   madd w0, w1, w2, w0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   madd w0, w1, w2, w0
;   ret

function %msub_i32(i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32):
    v3 = imul v1, v2
    v4 = isub v0, v3
    return v4
}

; VCode:
; block0:
;   msub w0, w1, w2, w0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   msub w0, w1, w2, w0
;   ret

function %msub_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
    v3 = imul v1, v2
    v4 = isub v0, v3
    return v4
}

; VCode:
; block0:
;   msub x0, x1, x2, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   msub x0, x1, x2, x0
;   ret

function %imul_sub_i32(i32, i32, i32) -> i32 {
block0(v0: i32, v1: i32, v2: i32):
    v3 = imul v1, v2
    v4 = isub v3, v0
    return v4
}

; VCode:
; block0:
;   madd w5, w1, w2, wzr
;   sub w0, w5, w0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mul w5, w1, w2
;   sub w0, w5, w0
;   ret

function %imul_sub_i64(i64, i64, i64) -> i64 {
block0(v0: i64, v1: i64, v2: i64):
    v3 = imul v1, v2
    v4 = isub v3, v0
    return v4
}

; VCode:
; block0:
;   madd x5, x1, x2, xzr
;   sub x0, x5, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mul x5, x1, x2
;   sub x0, x5, x0
;   ret

function %srem_const (i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 2
  v2 = srem.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   movz w2, #2
;   sdiv x4, x0, x2
;   msub x0, x4, x2, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, #2
;   sdiv x4, x0, x2
;   msub x0, x4, x2, x0
;   ret

function %urem_const (i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 2
  v2 = urem.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   movz x2, #2
;   udiv x4, x0, x2
;   msub x0, x4, x2, x0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x2, #2
;   udiv x4, x0, x2
;   msub x0, x4, x2, x0
;   ret

function %sdiv_minus_one(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 -1
  v2 = sdiv.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   movn x2, #0
;   adds xzr, x2, #1
;   ccmp x0, #1, #nzcv, eq
;   b.vs #trap=int_ovf
;   sdiv x0, x0, x2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x2, #-1
;   cmn x2, #1
;   ccmp x0, #1, #0, eq
;   b.vs #0x18
;   sdiv x0, x0, x2
;   ret
;   .byte 0x1f, 0xc1, 0x00, 0x00 ; trap: int_ovf

