test compile precise-output
set unwind_info=false
target aarch64

function %f(f64) -> f64 {
block0(v0: f64):
    v1 = fadd.f64 v0, v0
    v2 = fadd.f64 v0, v0
    v3 = fadd.f64 v0, v0
    v4 = fadd.f64 v0, v0
    v5 = fadd.f64 v0, v0
    v6 = fadd.f64 v0, v0
    v7 = fadd.f64 v0, v0
    v8 = fadd.f64 v0, v0
    v9 = fadd.f64 v0, v0
    v10 = fadd.f64 v0, v0
    v11 = fadd.f64 v0, v0
    v12 = fadd.f64 v0, v0
    v13 = fadd.f64 v0, v0
    v14 = fadd.f64 v0, v0
    v15 = fadd.f64 v0, v0
    v16 = fadd.f64 v0, v0
    v17 = fadd.f64 v0, v0
    v18 = fadd.f64 v0, v0
    v19 = fadd.f64 v0, v0
    v20 = fadd.f64 v0, v0
    v21 = fadd.f64 v0, v0
    v22 = fadd.f64 v0, v0
    v23 = fadd.f64 v0, v0
    v24 = fadd.f64 v0, v0
    v25 = fadd.f64 v0, v0
    v26 = fadd.f64 v0, v0
    v27 = fadd.f64 v0, v0
    v28 = fadd.f64 v0, v0
    v29 = fadd.f64 v0, v0
    v30 = fadd.f64 v0, v0
    v31 = fadd.f64 v0, v0

    v32 = fadd.f64 v0, v1
    v33 = fadd.f64 v2, v3
    v34 = fadd.f64 v4, v5
    v35 = fadd.f64 v6, v7
    v36 = fadd.f64 v8, v9
    v37 = fadd.f64 v10, v11
    v38 = fadd.f64 v12, v13
    v39 = fadd.f64 v14, v15
    v40 = fadd.f64 v16, v17
    v41 = fadd.f64 v18, v19
    v42 = fadd.f64 v20, v21
    v43 = fadd.f64 v22, v23
    v44 = fadd.f64 v24, v25
    v45 = fadd.f64 v26, v27
    v46 = fadd.f64 v28, v29
    v47 = fadd.f64 v30, v31

    v48 = fadd.f64 v32, v33
    v49 = fadd.f64 v34, v35
    v50 = fadd.f64 v36, v37
    v51 = fadd.f64 v38, v39
    v52 = fadd.f64 v40, v41
    v53 = fadd.f64 v42, v43
    v54 = fadd.f64 v44, v45
    v55 = fadd.f64 v46, v47

    v56 = fadd.f64 v48, v49
    v57 = fadd.f64 v50, v51
    v58 = fadd.f64 v52, v53
    v59 = fadd.f64 v54, v55

    v60 = fadd.f64 v56, v57
    v61 = fadd.f64 v58, v59

    v62 = fadd.f64 v60, v61

    return v62
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   stp d14, d15, [sp, #-16]!
;   stp d12, d13, [sp, #-16]!
;   stp d10, d11, [sp, #-16]!
;   stp d8, d9, [sp, #-16]!
; block0:
;   fadd d2, d0, d0
;   fadd d4, d0, d0
;   fadd d6, d0, d0
;   fadd d8, d0, d0
;   fadd d10, d0, d0
;   fadd d12, d0, d0
;   fadd d14, d0, d0
;   fadd d1, d0, d0
;   fadd d3, d0, d0
;   fadd d5, d0, d0
;   fadd d7, d0, d0
;   fadd d9, d0, d0
;   fadd d11, d0, d0
;   fadd d13, d0, d0
;   fadd d30, d0, d0
;   fadd d15, d0, d0
;   fadd d18, d0, d0
;   fadd d20, d0, d0
;   fadd d22, d0, d0
;   fadd d24, d0, d0
;   fadd d26, d0, d0
;   fadd d28, d0, d0
;   fadd d31, d0, d0
;   fadd d16, d0, d0
;   fadd d19, d0, d0
;   fadd d21, d0, d0
;   fadd d23, d0, d0
;   fadd d25, d0, d0
;   fadd d27, d0, d0
;   fadd d29, d0, d0
;   fadd d17, d0, d0
;   fadd d0, d0, d2
;   fadd d2, d4, d6
;   fadd d4, d8, d10
;   fadd d6, d12, d14
;   fadd d8, d1, d3
;   fadd d10, d5, d7
;   fadd d12, d9, d11
;   fadd d14, d13, d30
;   fadd d1, d15, d18
;   fadd d3, d20, d22
;   fadd d5, d24, d26
;   fadd d7, d28, d31
;   fadd d9, d16, d19
;   fadd d11, d21, d23
;   fadd d13, d25, d27
;   fadd d15, d29, d17
;   fadd d0, d0, d2
;   fadd d2, d4, d6
;   fadd d4, d8, d10
;   fadd d6, d12, d14
;   fadd d8, d1, d3
;   fadd d10, d5, d7
;   fadd d12, d9, d11
;   fadd d14, d13, d15
;   fadd d0, d0, d2
;   fadd d2, d4, d6
;   fadd d4, d8, d10
;   fadd d6, d12, d14
;   fadd d8, d0, d2
;   fadd d10, d4, d6
;   fadd d0, d8, d10
;   ldp d8, d9, [sp], #16
;   ldp d10, d11, [sp], #16
;   ldp d12, d13, [sp], #16
;   ldp d14, d15, [sp], #16
;   ldp fp, lr, [sp], #16
;   ret

function %f2(i64) -> i64 {
block0(v0: i64):
    v1 = iadd.i64 v0, v0
    v2 = iadd.i64 v0, v1
    v3 = iadd.i64 v0, v2
    v4 = iadd.i64 v0, v3
    v5 = iadd.i64 v0, v4
    v6 = iadd.i64 v0, v5
    v7 = iadd.i64 v0, v6
    v8 = iadd.i64 v0, v7
    v9 = iadd.i64 v0, v8
    v10 = iadd.i64 v0, v9
    v11 = iadd.i64 v0, v10
    v12 = iadd.i64 v0, v11
    v13 = iadd.i64 v0, v12
    v14 = iadd.i64 v0, v13
    v15 = iadd.i64 v0, v14
    v16 = iadd.i64 v0, v15
    v17 = iadd.i64 v0, v16
    v18 = iadd.i64 v0, v17

    v19 = iadd.i64 v0, v1
    v20 = iadd.i64 v2, v3
    v21 = iadd.i64 v4, v5
    v22 = iadd.i64 v6, v7
    v23 = iadd.i64 v8, v9
    v24 = iadd.i64 v10, v11
    v25 = iadd.i64 v12, v13
    v26 = iadd.i64 v14, v15
    v27 = iadd.i64 v16, v17

    v28 = iadd.i64 v18, v19
    v29 = iadd.i64 v20, v21
    v30 = iadd.i64 v22, v23
    v31 = iadd.i64 v24, v25
    v32 = iadd.i64 v26, v27

    v33 = iadd.i64 v28, v29
    v34 = iadd.i64 v30, v31

    v35 = iadd.i64 v32, v33
    v36 = iadd.i64 v34, v35

    return v36
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   str x28, [sp, #-16]!
;   stp x19, x21, [sp, #-16]!
; block0:
;   add x6, x0, x0
;   add x7, x0, x6
;   add x8, x0, x7
;   add x9, x0, x8
;   add x10, x0, x9
;   add x11, x0, x10
;   add x12, x0, x11
;   add x13, x0, x12
;   add x14, x0, x13
;   add x15, x0, x14
;   add x1, x0, x15
;   add x2, x0, x1
;   add x3, x0, x2
;   add x4, x0, x3
;   add x5, x0, x4
;   add x28, x0, x5
;   add x21, x0, x28
;   add x19, x0, x21
;   add x6, x0, x6
;   add x7, x7, x8
;   add x8, x9, x10
;   add x9, x11, x12
;   add x10, x13, x14
;   add x11, x15, x1
;   add x12, x2, x3
;   add x13, x4, x5
;   add x14, x28, x21
;   add x6, x19, x6
;   add x7, x7, x8
;   add x8, x9, x10
;   add x9, x11, x12
;   add x10, x13, x14
;   add x6, x6, x7
;   add x7, x8, x9
;   add x6, x10, x6
;   add x0, x7, x6
;   ldp x19, x21, [sp], #16
;   ldr x28, [sp], #16
;   ldp fp, lr, [sp], #16
;   ret

