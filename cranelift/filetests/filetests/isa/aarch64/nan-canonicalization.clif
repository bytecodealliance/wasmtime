test compile precise-output
set enable_nan_canonicalization=true 
target x86_64 sse41

function %f0(f32x4, f32x4) -> f32x4 {
block0(v0: f32x4, v1: f32x4):
	v2 = fadd v0, v1
	return v2
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   addps %xmm1, %xmm0
;   movl $0x7fc00000, %r10d
;   movd %r10d, %xmm7
;   shufps $0x0, (%rip), %xmm7
;   movdqa %xmm0, %xmm1
;   cmpunordps %xmm0, %xmm1
;   movdqa %xmm0, %xmm2
;   movdqa %xmm1, %xmm0
;   movdqa %xmm2, %xmm1
;   pblendvb %xmm0, %xmm7, %xmm1
;   movdqa %xmm1, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   addps %xmm1, %xmm0
;   movl $0x7fc00000, %r10d
;   movd %r10d, %xmm7
;   shufps $0, 0x26(%rip), %xmm7
;   movdqa %xmm0, %xmm1
;   cmpunordps %xmm0, %xmm1
;   movdqa %xmm0, %xmm2
;   movdqa %xmm1, %xmm0
;   movdqa %xmm2, %xmm1
;   pblendvb %xmm0, %xmm7, %xmm1
;   movdqa %xmm1, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   sarb $0, (%rdi)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)
;   addb %al, (%rax)

function %f1(f64, f64) -> f64 {
block0(v0: f64, v1: f64):
	v2 = fadd v0, v1
	return v2
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   addsd %xmm1, %xmm0
;   movdqa %xmm0, %xmm7
;   movabsq $0x7ff8000000000000, %rcx
;   movq %rcx, %xmm6
;   uninit  %xmm5
;   xorpd %xmm5, %xmm5
;   movsd %xmm6, %xmm5
;   uninit  %xmm0
;   xorpd %xmm0, %xmm0
;   movdqa %xmm7, %xmm6
;   movsd %xmm6, %xmm0
;   movdqa %xmm0, %xmm6
;   cmpunordpd %xmm0, %xmm6
;   movdqa %xmm0, %xmm3
;   movdqa %xmm6, %xmm0
;   pblendvb %xmm0, %xmm5, %xmm3
;   movdqa %xmm3, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   addsd %xmm1, %xmm0
;   movdqa %xmm0, %xmm7
;   movabsq $0x7ff8000000000000, %rcx
;   movq %rcx, %xmm6
;   xorpd %xmm5, %xmm5
;   movsd %xmm6, %xmm5
;   xorpd %xmm0, %xmm0
;   movdqa %xmm7, %xmm6
;   movsd %xmm6, %xmm0
;   movdqa %xmm0, %xmm6
;   cmpunordpd %xmm0, %xmm6
;   movdqa %xmm0, %xmm3
;   movdqa %xmm6, %xmm0
;   pblendvb %xmm0, %xmm5, %xmm3
;   movdqa %xmm3, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

function %f1(f32, f32) -> f32 {
block0(v0: f32, v1: f32):
	v2 = fadd v0, v1
	return v2
}

; VCode:
;   pushq %rbp
;   movq %rsp, %rbp
; block0:
;   addss %xmm1, %xmm0
;   movdqa %xmm0, %xmm7
;   movl $0x7fc00000, %ecx
;   movd %ecx, %xmm6
;   uninit  %xmm5
;   xorps %xmm5, %xmm5
;   movss %xmm6, %xmm5
;   uninit  %xmm0
;   xorps %xmm0, %xmm0
;   movdqa %xmm7, %xmm6
;   movss %xmm6, %xmm0
;   movdqa %xmm0, %xmm6
;   cmpunordps %xmm0, %xmm6
;   movdqa %xmm0, %xmm3
;   movdqa %xmm6, %xmm0
;   pblendvb %xmm0, %xmm5, %xmm3
;   movdqa %xmm3, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq
;
; Disassembled:
; block0: ; offset 0x0
;   pushq %rbp
;   movq %rsp, %rbp
; block1: ; offset 0x4
;   addss %xmm1, %xmm0
;   movdqa %xmm0, %xmm7
;   movl $0x7fc00000, %ecx
;   movd %ecx, %xmm6
;   xorps %xmm5, %xmm5
;   movss %xmm6, %xmm5
;   xorps %xmm0, %xmm0
;   movdqa %xmm7, %xmm6
;   movss %xmm6, %xmm0
;   movdqa %xmm0, %xmm6
;   cmpunordps %xmm0, %xmm6
;   movdqa %xmm0, %xmm3
;   movdqa %xmm6, %xmm0
;   pblendvb %xmm0, %xmm5, %xmm3
;   movdqa %xmm3, %xmm0
;   movq %rbp, %rsp
;   popq %rbp
;   retq

