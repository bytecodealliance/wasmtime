test compile precise-output
target aarch64

function %store_scale() {
  gv0 = dyn_scale_target_const.i32x4
  ss0 = explicit_slot 8

block0:
  v0 = global_value.i64 gv0
  stack_store.i64 v0, ss0
  return
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   movz x1, #1
;   mov x2, sp
;   str x1, [x2]
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   mov x1, #1
;   mov x2, sp
;   str x1, [x2]
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %store_scale_lt_128() {
  gv0 = dyn_scale_target_const.i16x4
  ss0 = explicit_slot 8

block0:
  v0 = global_value.i64 gv0
  stack_store.i64 v0, ss0
  return
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   movz x1, #1
;   mov x2, sp
;   str x1, [x2]
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   mov x1, #1
;   mov x2, sp
;   str x1, [x2]
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %store_explicit(i32) {
  gv0 = dyn_scale_target_const.i32x4
  dt0 = i32x4*gv0
  dss0 = explicit_dynamic_slot dt0

block0(v0: i32):
  v1 = splat.dt0 v0
  dynamic_stack_store.dt0 v1, dss0
  return
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   dup v3.4s, w0
;   mov x3, sp
;   str q3, [x3]
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   dup v3.4s, w0
;   mov x3, sp
;   str q3, [x3]
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %load_explicit() -> i32x4 {
  gv0 = dyn_scale_target_const.i32x4
  dt0 = i32x4*gv0
  dss0 = explicit_dynamic_slot dt0

block0:
  v0 = dynamic_stack_load.dt0 dss0
  v1 = extract_vector.dt0 v0, 0
  return v1
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   mov x1, sp
;   ldr q0, [x1]
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   mov x1, sp
;   ldr q0, [x1]
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %store_implicit(i32) {
  gv0 = dyn_scale_target_const.i32x4
  dt0 = i32x4*gv0
  dss0 = explicit_dynamic_slot dt0

block0(v0: i32):
  v1 = splat.dt0 v0
  dynamic_stack_store v1, dss0
  return
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   dup v3.4s, w0
;   mov x3, sp
;   str q3, [x3]
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   dup v3.4s, w0
;   mov x3, sp
;   str q3, [x3]
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

function %addr() -> i64 {
  gv0 = dyn_scale_target_const.i32x4
  dt0 = i32x4*gv0
  dss0 = explicit_dynamic_slot dt0

block0:
  v0 = dynamic_stack_addr.i64 dss0
  return v0
}

; VCode:
;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   sub sp, sp, #16
; block0:
;   mov x0, sp
;   add sp, sp, #16
;   ldp fp, lr, [sp], #16
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   stp x29, x30, [sp, #-0x10]!
;   mov x29, sp
;   sub sp, sp, #0x10
; block1: ; offset 0xc
;   mov x0, sp
;   add sp, sp, #0x10
;   ldp x29, x30, [sp], #0x10
;   ret

