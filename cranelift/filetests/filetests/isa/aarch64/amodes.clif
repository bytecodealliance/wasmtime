test compile precise-output
set unwind_info=false
target aarch64

function %f0(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0, w1, UXTW]
;   Inst 1:   ret
; }}

function %f2(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v2+v0
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0, w1, UXTW]
;   Inst 1:   ret
; }}

function %f3(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0, w1, SXTW]
;   Inst 1:   ret
; }}

function %f4(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = load_complex.i32 v2+v0
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0, w1, SXTW]
;   Inst 1:   ret
; }}

function %f5(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = load.i32 v3
  return v4
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0, w1, SXTW]
;   Inst 1:   ret
; }}

function %f6(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v2, v0
  v4 = load.i32 v3
  return v4
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0, w1, SXTW]
;   Inst 1:   ret
; }}

function %f7(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = uextend.i64 v0
  v3 = uextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   mov w0, w0
;   Inst 1:   ldr w0, [x0, w1, UXTW]
;   Inst 2:   ret
; }}

function %f8(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iconst.i64 32
  v4 = iadd.i64 v2, v3
  v5 = iadd.i64 v4, v0
  v6 = iadd.i64 v5, v5
  v7 = load.i32 v6+4
  return v7
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   add x2, x0, #68
;   Inst 1:   add x0, x2, x0
;   Inst 2:   add x0, x0, x1, SXTW
;   Inst 3:   ldr w0, [x0, w1, SXTW]
;   Inst 4:   ret
; }}

function %f9(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 48
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 4)
;   Inst 0:   add x0, x0, x2
;   Inst 1:   add x0, x0, x1
;   Inst 2:   ldur w0, [x0, #48]
;   Inst 3:   ret
; }}

function %f10(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 4100
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   movz x3, #4100
;   Inst 1:   add x1, x3, x1
;   Inst 2:   add x1, x1, x2
;   Inst 3:   ldr w0, [x1, x0]
;   Inst 4:   ret
; }}

function %f10() -> i32 {
block0:
  v1 = iconst.i64 1234
  v2 = load.i32 v1
  return v2
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   movz x0, #1234
;   Inst 1:   ldr w0, [x0]
;   Inst 2:   ret
; }}

function %f11(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 8388608 ;; Imm12: 0x800 << 12
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   add x0, x0, #8388608
;   Inst 1:   ldr w0, [x0]
;   Inst 2:   ret
; }}

function %f12(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 -4
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   sub x0, x0, #4
;   Inst 1:   ldr w0, [x0]
;   Inst 2:   ret
; }}

function %f13(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 1000000000
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   movz w1, #51712
;   Inst 1:   movk w1, #15258, LSL #16
;   Inst 2:   add x0, x1, x0
;   Inst 3:   ldr w0, [x0]
;   Inst 4:   ret
; }}

function %f14(i32) -> i32 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i32 v1
  return v2
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   sxtw x0, w0
;   Inst 1:   ldr w0, [x0]
;   Inst 2:   ret
; }}

function %f15(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sextend.i64 v0
  v3 = sextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   sxtw x0, w0
;   Inst 1:   ldr w0, [x0, w1, SXTW]
;   Inst 2:   ret
; }}

function %f16(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i32 0
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldr w0, [x0]
;   Inst 1:   ret
; }}

function %f17(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i32 4
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 2)
;   Inst 0:   ldur w0, [x0, #4]
;   Inst 1:   ret
; }}

function %f18(i64, i32) -> i16x8 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = sload8x8_complex v2+v0
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   ldr d0, [x0, w1, UXTW]
;   Inst 1:   sxtl v0.8h, v0.8b
;   Inst 2:   ret
; }}

function %f19(i64, i64) -> i32x4 {
block0(v0: i64, v1: i64):
  v2 = uload16x4_complex v0+v1+8
  return v2
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 4)
;   Inst 0:   add x0, x0, x1
;   Inst 1:   ldr d0, [x0, #8]
;   Inst 2:   uxtl v0.4s, v0.4h
;   Inst 3:   ret
; }}

function %f20(i64, i32) -> i64x2 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = uload32x2_complex v2+v0
  return v3
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   ldr d0, [x0, w1, SXTW]
;   Inst 1:   uxtl v0.2d, v0.2s
;   Inst 2:   ret
; }}

function %f18(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 -4098
  v6 = uextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   movn w0, #4097
;   Inst 1:   ldrsh x0, [x0]
;   Inst 2:   ret
; }}

function %f19(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 4098
  v6 = uextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 3)
;   Inst 0:   movz x0, #4098
;   Inst 1:   ldrsh x0, [x0]
;   Inst 2:   ret
; }}

function %f20(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 -4098
  v6 = sextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 4)
;   Inst 0:   movn w0, #4097
;   Inst 1:   sxtw x0, w0
;   Inst 2:   ldrsh x0, [x0]
;   Inst 3:   ret
; }}

function %f21(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 4098
  v6 = sextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 4)
;   Inst 0:   movz x0, #4098
;   Inst 1:   sxtw x0, w0
;   Inst 2:   ldrsh x0, [x0]
;   Inst 3:   ret
; }}

function %i128(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0
  store.i128 v1, v0
  return v1
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   mov x1, x0
;   Inst 1:   ldp x2, x1, [x1]
;   Inst 2:   stp x2, x1, [x0]
;   Inst 3:   mov x0, x2
;   Inst 4:   ret
; }}

function %i128_imm_offset(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0+16
  store.i128 v1, v0+16
  return v1
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   mov x1, x0
;   Inst 1:   ldp x2, x1, [x1, #16]
;   Inst 2:   stp x2, x1, [x0, #16]
;   Inst 3:   mov x0, x2
;   Inst 4:   ret
; }}

function %i128_imm_offset_large(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0+504
  store.i128 v1, v0+504
  return v1
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   mov x1, x0
;   Inst 1:   ldp x2, x1, [x1, #504]
;   Inst 2:   stp x2, x1, [x0, #504]
;   Inst 3:   mov x0, x2
;   Inst 4:   ret
; }}

function %i128_imm_offset_negative_large(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0-512
  store.i128 v1, v0-512
  return v1
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   mov x1, x0
;   Inst 1:   ldp x2, x1, [x1, #-512]
;   Inst 2:   stp x2, x1, [x0, #-512]
;   Inst 3:   mov x0, x2
;   Inst 4:   ret
; }}

function %i128_add_offset(i64) -> i128 {
block0(v0: i64):
  v1 = iadd_imm v0, 32
  v2 = load.i128 v1
  store.i128 v2, v1
  return v2
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 5)
;   Inst 0:   mov x1, x0
;   Inst 1:   ldp x2, x1, [x1, #32]
;   Inst 2:   stp x2, x1, [x0, #32]
;   Inst 3:   mov x0, x2
;   Inst 4:   ret
; }}

function %i128_32bit_sextend_simple(i32) -> i128 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i128 v1
  store.i128 v2, v1
  return v2
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 6)
;   Inst 0:   sxtw x1, w0
;   Inst 1:   ldp x2, x1, [x1]
;   Inst 2:   sxtw x0, w0
;   Inst 3:   stp x2, x1, [x0]
;   Inst 4:   mov x0, x2
;   Inst 5:   ret
; }}

function %i128_32bit_sextend(i64, i32) -> i128 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = iadd_imm.i64 v3, 24
  v5 = load.i128 v4
  store.i128 v5, v4
  return v5
}

; VCode_ShowWithRRU {{
;   Entry block: 0
; Block 0:
;   (original IR block: block0)
;   (instruction range: 0 .. 8)
;   Inst 0:   mov x2, x0
;   Inst 1:   add x2, x2, x1, SXTW
;   Inst 2:   ldp x3, x2, [x2, #24]
;   Inst 3:   add x0, x0, x1, SXTW
;   Inst 4:   stp x3, x2, [x0, #24]
;   Inst 5:   mov x0, x3
;   Inst 6:   mov x1, x2
;   Inst 7:   ret
; }}

