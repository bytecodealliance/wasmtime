test compile precise-output
set unwind_info=false
target aarch64

function %f5(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = load.i32 v3
  return v4
}

; VCode:
; block0:
;   ldr w0, [x0, w1, SXTW]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr w0, [x0, w1, sxtw] ; trap: heap_oob
;   ret

function %f6(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v2, v0
  v4 = load.i32 v3
  return v4
}

; VCode:
; block0:
;   ldr w0, [x0, w1, SXTW]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr w0, [x0, w1, sxtw] ; trap: heap_oob
;   ret

function %f7(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = uextend.i64 v0
  v3 = uextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; VCode:
; block0:
;   mov w4, w1
;   ldr w0, [x4, w0, UXTW]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w4, w1
;   ldr w0, [x4, w0, uxtw] ; trap: heap_oob
;   ret

function %f8(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iconst.i64 32
  v4 = iadd.i64 v2, v3
  v5 = iadd.i64 v4, v0
  v6 = iadd.i64 v5, v5
  v7 = load.i32 v6+4
  return v7
}

; VCode:
; block0:
;   sxtw x7, w1
;   add x7, x7, #32
;   add x7, x7, x0
;   add x6, x7, #4
;   ldr w0, [x6, x7]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x7, w1
;   add x7, x7, #0x20
;   add x7, x7, x0
;   add x6, x7, #4
;   ldr w0, [x6, x7] ; trap: heap_oob
;   ret

function %f9(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 48
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; VCode:
; block0:
;   add x6, x0, x1
;   add x5, x6, #48
;   ldr w0, [x5, x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x6, x0, x1
;   add x5, x6, #0x30
;   ldr w0, [x5, x2] ; trap: heap_oob
;   ret

function %f10(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 4100
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; VCode:
; block0:
;   add x7, x0, x1
;   movz x5, #4100
;   add x7, x7, x5
;   ldr w0, [x7, x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x7, x0, x1
;   mov x5, #0x1004
;   add x7, x7, x5
;   ldr w0, [x7, x2] ; trap: heap_oob
;   ret

function %f10() -> i32 {
block0:
  v1 = iconst.i64 1234
  v2 = load.i32 v1
  return v2
}

; VCode:
; block0:
;   movz x1, #1234
;   ldr w0, [x1]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x1, #0x4d2
;   ldr w0, [x1] ; trap: heap_oob
;   ret

function %f11(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 8388608 ;; Imm12: 0x800 << 12
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   movz x2, #128, LSL #16
;   ldr w0, [x0, x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x2, #0x800000
;   ldr w0, [x0, x2] ; trap: heap_oob
;   ret

function %add_imm12_max_shifted(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 0xFFF000
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   orr x2, xzr, #16773120
;   ldr w0, [x0, x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   orr x2, xzr, #0xfff000
;   ldr w0, [x0, x2] ; trap: heap_oob
;   ret

function %f12(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 -4
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   ldur w0, [x0, #-4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldur w0, [x0, #-4] ; trap: heap_oob
;   ret

function %f13(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 1000000000
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   movz w3, #51712
;   movk w3, w3, #15258, LSL #16
;   ldr w0, [x0, x3]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w3, #0xca00
;   movk w3, #0x3b9a, lsl #16
;   ldr w0, [x0, x3] ; trap: heap_oob
;   ret

function %f14(i32) -> i32 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i32 v1
  return v2
}

; VCode:
; block0:
;   sxtw x3, w0
;   ldr w0, [x3]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x3, w0
;   ldr w0, [x3] ; trap: heap_oob
;   ret

function %f15(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sextend.i64 v0
  v3 = sextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; VCode:
; block0:
;   sxtw x4, w1
;   ldr w0, [x4, w0, SXTW]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x4, w1
;   ldr w0, [x4, w0, sxtw] ; trap: heap_oob
;   ret

function %f18(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 -4098
  v6 = uextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   movn w2, #4097
;   mov w2, w2
;   ldrsh x0, [x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, #-0x1002
;   mov w2, w2
;   ldrsh x0, [x2] ; trap: heap_oob
;   ret

function %f19(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 4098
  v6 = uextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   movz w2, #4098
;   mov w2, w2
;   ldrsh x0, [x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, #0x1002
;   mov w2, w2
;   ldrsh x0, [x2] ; trap: heap_oob
;   ret

function %f20(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 -4098
  v6 = sextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   movn w2, #4097
;   sxtw x2, w2
;   ldrsh x0, [x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, #-0x1002
;   sxtw x2, w2
;   ldrsh x0, [x2] ; trap: heap_oob
;   ret

function %f21(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 4098
  v6 = sextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   movz w2, #4098
;   sxtw x2, w2
;   ldrsh x0, [x2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov w2, #0x1002
;   sxtw x2, w2
;   ldrsh x0, [x2] ; trap: heap_oob
;   ret

function %i128(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0
  store.i128 v1, v0
  return v1
}

; VCode:
; block0:
;   mov x5, x0
;   ldp x0, x1, [x5]
;   stp x0, x1, [x5]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x5, x0
;   ldp x0, x1, [x5] ; trap: heap_oob
;   stp x0, x1, [x5] ; trap: heap_oob
;   ret

function %i128_imm_offset(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0+16
  store.i128 v1, v0+16
  return v1
}

; VCode:
; block0:
;   mov x5, x0
;   ldp x0, x1, [x5, #16]
;   stp x0, x1, [x5, #16]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x5, x0
;   ldp x0, x1, [x5, #0x10] ; trap: heap_oob
;   stp x0, x1, [x5, #0x10] ; trap: heap_oob
;   ret

function %i128_imm_offset_large(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0+504
  store.i128 v1, v0+504
  return v1
}

; VCode:
; block0:
;   mov x5, x0
;   ldp x0, x1, [x5, #504]
;   stp x0, x1, [x5, #504]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x5, x0
;   ldp x0, x1, [x5, #0x1f8] ; trap: heap_oob
;   stp x0, x1, [x5, #0x1f8] ; trap: heap_oob
;   ret

function %i128_imm_offset_negative_large(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0-512
  store.i128 v1, v0-512
  return v1
}

; VCode:
; block0:
;   mov x5, x0
;   ldp x0, x1, [x5, #-512]
;   stp x0, x1, [x5, #-512]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   mov x5, x0
;   ldp x0, x1, [x5, #-0x200] ; trap: heap_oob
;   stp x0, x1, [x5, #-0x200] ; trap: heap_oob
;   ret

function %i128_add_offset(i64) -> i128 {
block0(v0: i64):
  v1 = iadd_imm v0, 32
  v2 = load.i128 v1
  store.i128 v2, v1
  return v2
}

; VCode:
; block0:
;   add x4, x0, #32
;   ldp x0, x1, [x4]
;   stp x0, x1, [x4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x4, x0, #0x20
;   ldp x0, x1, [x4] ; trap: heap_oob
;   stp x0, x1, [x4] ; trap: heap_oob
;   ret

function %i128_32bit_sextend_simple(i32) -> i128 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i128 v1
  store.i128 v2, v1
  return v2
}

; VCode:
; block0:
;   sxtw x4, w0
;   ldp x0, x1, [x4]
;   stp x0, x1, [x4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sxtw x4, w0
;   ldp x0, x1, [x4] ; trap: heap_oob
;   stp x0, x1, [x4] ; trap: heap_oob
;   ret

function %i128_32bit_sextend(i64, i32) -> i128 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = iadd_imm.i64 v3, 24
  v5 = load.i128 v4
  store.i128 v5, v4
  return v5
}

; VCode:
; block0:
;   add x6, x0, x1, SXTW
;   add x6, x6, #24
;   ldp x0, x1, [x6]
;   stp x0, x1, [x6]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   add x6, x0, w1, sxtw
;   add x6, x6, #0x18
;   ldp x0, x1, [x6] ; trap: heap_oob
;   stp x0, x1, [x6] ; trap: heap_oob
;   ret

function %load_scaled16(i64, i64) -> i8 {
block0(v0: i64, v1: i64):
  v2 = ishl_imm v1, 0
  v3 = iadd v0, v2
  v4 = load.i8 v3
  return v4
}

; VCode:
; block0:
;   ldrb w0, [x0, x1, LSL #0]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldrb w0, [x0, x1, lsl #0] ; trap: heap_oob
;   ret

function %load_scaled16(i64, i64) -> i16 {
block0(v0: i64, v1: i64):
  v2 = ishl_imm v1, 1
  v3 = iadd v0, v2
  v4 = load.i16 v3
  return v4
}

; VCode:
; block0:
;   ldrh w0, [x0, x1, LSL #1]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldrh w0, [x0, x1, lsl #1] ; trap: heap_oob
;   ret

function %load_scaled32(i64, i64) -> i32 {
block0(v0: i64, v1: i64):
  v2 = ishl_imm v1, 2
  v3 = iadd v0, v2
  v4 = load.i32 v3
  return v4
}

; VCode:
; block0:
;   ldr w0, [x0, x1, LSL #2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr w0, [x0, x1, lsl #2] ; trap: heap_oob
;   ret

function %load_scaled64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = ishl_imm v1, 3
  v3 = iadd v0, v2
  v4 = load.i64 v3
  return v4
}

; VCode:
; block0:
;   ldr x0, [x0, x1, LSL #3]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr x0, [x0, x1, lsl #3] ; trap: heap_oob
;   ret

function %load_not_scaled64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = ishl_imm v1, 2
  v3 = iadd v0, v2
  v4 = load.i64 v3
  return v4
}

; VCode:
; block0:
;   lsl x4, x1, #2
;   ldr x0, [x0, x4]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   lsl x4, x1, #2
;   ldr x0, [x0, x4] ; trap: heap_oob
;   ret

function %load_uextend_scaled16(i64, i32) -> i8 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = ishl_imm v2, 0
  v4 = iadd v0, v3
  v5 = load.i8 v4
  return v5
}

; VCode:
; block0:
;   ldrb w0, [x0, w1, UXTW #0]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldrb w0, [x0, w1, uxtw #0] ; trap: heap_oob
;   ret

function %load_uextend_scaled16(i64, i32) -> i16 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = ishl_imm v2, 1
  v4 = iadd v0, v3
  v5 = load.i16 v4
  return v5
}

; VCode:
; block0:
;   ldrh w0, [x0, w1, UXTW #1]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldrh w0, [x0, w1, uxtw #1] ; trap: heap_oob
;   ret

function %load_uextend_scaled32(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = ishl_imm v2, 2
  v4 = iadd v0, v3
  v5 = load.i32 v4
  return v5
}

; VCode:
; block0:
;   ldr w0, [x0, w1, UXTW #2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr w0, [x0, w1, uxtw #2] ; trap: heap_oob
;   ret

function %load_uextend_scaled64(i64, i32) -> i64 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = ishl_imm v2, 3
  v4 = iadd v0, v3
  v5 = load.i64 v4
  return v5
}

; VCode:
; block0:
;   ldr x0, [x0, w1, UXTW #3]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr x0, [x0, w1, uxtw #3] ; trap: heap_oob
;   ret

function %load_not_extend_scaled64(i64, i32) -> i64 {
block0(v0: i64, v1: i32):
  v2 = ishl_imm v1, 3
  v3 = uextend.i64 v2
  v4 = iadd v0, v3
  v5 = load.i64 v4
  return v5
}

; VCode:
; block0:
;   lsl w4, w1, #3
;   ldr x0, [x0, w4, UXTW]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   lsl w4, w1, #3
;   ldr x0, [x0, w4, uxtw] ; trap: heap_oob
;   ret

function %load_sextend_scaled8(i64, i32) -> i8 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = ishl_imm v2, 0
  v4 = iadd v0, v3
  v5 = load.i8 v4
  return v5
}

; VCode:
; block0:
;   ldrb w0, [x0, w1, SXTW #0]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldrb w0, [x0, w1, sxtw #0] ; trap: heap_oob
;   ret

function %load_sextend_scaled16(i64, i32) -> i16 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = ishl_imm v2, 1
  v4 = iadd v0, v3
  v5 = load.i16 v4
  return v5
}

; VCode:
; block0:
;   ldrh w0, [x0, w1, SXTW #1]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldrh w0, [x0, w1, sxtw #1] ; trap: heap_oob
;   ret

function %load_sextend_scaled32(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = ishl_imm v2, 2
  v4 = iadd v0, v3
  v5 = load.i32 v4
  return v5
}

; VCode:
; block0:
;   ldr w0, [x0, w1, SXTW #2]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr w0, [x0, w1, sxtw #2] ; trap: heap_oob
;   ret

function %load_sextend_scaled64(i64, i32) -> i64 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = ishl_imm v2, 3
  v4 = iadd v0, v3
  v5 = load.i64 v4
  return v5
}

; VCode:
; block0:
;   ldr x0, [x0, w1, SXTW #3]
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ldr x0, [x0, w1, sxtw #3] ; trap: heap_oob
;   ret

function %no_panic(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v8 = ishl_imm v1, 100
  v9 = iadd v0, v8
  v10 = load.i64 v9

  v5 = ishl_imm v1, 100
  v6 = iadd v5, v0
  v7 = load.i64 v6
  return v10
}

; VCode:
; block0:
;   lsl x6, x1, #36
;   ldr x6, [x0, x6]
;   lsl x7, x1, #36
;   ldr x7, [x7, x0]
;   mov x0, x6
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   lsl x6, x1, #0x24
;   ldr x6, [x0, x6] ; trap: heap_oob
;   lsl x7, x1, #0x24
;   ldr x7, [x7, x0] ; trap: heap_oob
;   mov x0, x6
;   ret

