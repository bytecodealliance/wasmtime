test compile
target aarch64

function %f0(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0, w1, UXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f2(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v2+v0
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0, w1, UXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f3(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0, w1, SXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f4(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = load_complex.i32 v2+v0
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0, w1, SXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f5(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = load.i32 v3
  return v4
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0, w1, SXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f6(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v2, v0
  v4 = load.i32 v3
  return v4
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0, w1, SXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f7(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = uextend.i64 v0
  v3 = uextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: mov w0, w0
; nextln: ldr w0, [x0, w1, UXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f8(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iconst.i64 32
  v4 = iadd.i64 v2, v3
  v5 = iadd.i64 v4, v0
  v6 = iadd.i64 v5, v5
  v7 = load.i32 v6+4
  return v7
}

; v6+4 = 2*v5 = 2*v4 + 2*v0 + 4 = 2*v2 + 2*v3 + 2*v0 + 4
;      = 2*sextend($x1) + 2*$x0 + 68

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: add x2, x0, #68
; nextln: add x0, x2, x0
; nextln: add x0, x0, x1, SXTW
; nextln: ldr w0, [x0, w1, SXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f9(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 48
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; v6 = $x0 + $x1 + $x2 + 48

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: add x0, x0, x2
; nextln: add x0, x0, x1
; nextln: ldur w0, [x0, #48]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f10(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 4100
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; v6 = $x0 + $x1 + $x2 + 4100

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: movz x3, #4100
; nextln: add x1, x3, x1
; nextln: add x1, x1, x2
; nextln: ldr w0, [x1, x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f10() -> i32 {
block0:
  v1 = iconst.i64 1234
  v2 = load.i32 v1
  return v2
}

; v6 = $x0 + $x1 + $x2 + 48

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: movz x0, #1234
; nextln: ldr w0, [x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f11(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 8388608 ; Imm12: 0x800 << 12
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: add x0, x0, #8388608
; nextln: ldr w0, [x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f12(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 -4
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: sub x0, x0, #4
; nextln: ldr w0, [x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f13(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 1000000000
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: movz w1, #51712
; nextln: movk w1, #15258, LSL #16
; nextln: add x0, x1, x0
; nextln: ldr w0, [x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f14(i32) -> i32 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i32 v1
  return v2
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: sxtw x0, w0
; nextln: ldr w0, [x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f15(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sextend.i64 v0
  v3 = sextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: sxtw x0, w0
; nextln: ldr w0, [x0, w1, SXTW]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f16(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i32 0
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldr w0, [x0]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret

function %f17(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i32 4
  v2 = uextend.i64 v1
  v3 = load_complex.i32 v0+v2
  return v3
}

; check: stp fp, lr, [sp, #-16]!
; nextln: mov fp, sp
; nextln: ldur w0, [x0, #4]
; nextln: mov sp, fp
; nextln: ldp fp, lr, [sp], #16
; nextln: ret
