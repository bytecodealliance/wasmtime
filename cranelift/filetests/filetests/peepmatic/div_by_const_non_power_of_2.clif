test peepmatic
target i686 baseline

; -------- U32 --------

; complex case (mul, sub, shift, add, shift)
function %t_udiv32_p7(i32) -> i32 {
block0(v0: i32):
    v1 = udiv_imm v0, 7
    ; check: iconst.i32 0x2492_4925
    ; check: umulhi v0, v2
    ; check: isub v0, v3
    ; check: ushr_imm v4, 1
    ; check: iadd v5, v3
    ; check: v7 = ushr_imm v6, 2
    ; check: v1 -> v7
    return v1
}

; simple case (mul, shift)
function %t_udiv32_p125(i32) -> i32 {
block0(v0: i32):
    v1 = udiv_imm v0, 125
    ; check: iconst.i32 0x1062_4dd3
    ; check: umulhi v0, v2
    ; check: v4 = ushr_imm v3, 3
    ; check: v1 -> v4
    return v1
}

; simple case w/ shift by zero (mul)
function %t_udiv32_p641(i32) -> i32 {
block0(v0: i32):
    v1 = udiv_imm v0, 641
    ; check: iconst.i32 0x0066_3d81
    ; check: v3 = umulhi v0, v2
    ; check: v1 -> v3
    return v1
}


; -------- S32 --------

; simple case w/ shift by zero (mul, add-sign-bit)
function %t_sdiv32_n6(i32) -> i32 {
block0(v0: i32):
    v1 = sdiv_imm v0, -6
    ; check: iconst.i32 0xffff_ffff_d555_5555
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 31
    ; check: v5 = iadd v3, v4
    ; check: v1 -> v5
    return v1
}

; simple case (mul, shift, add-sign-bit)
function %t_sdiv32_n5(i32) -> i32 {
block0(v0: i32):
    v1 = sdiv_imm v0, -5
    ; check: iconst.i32 0xffff_ffff_9999_9999
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 1
    ; check: ushr_imm v4, 31
    ; check: v6 = iadd v4, v5
    ; check: v1 -> v6
    return v1
}

; case d < 0 && M > 0 (mul, sub, shift, add-sign-bit)
function %t_sdiv32_n3(i32) -> i32 {
block0(v0: i32):
    v1 = sdiv_imm v0, -3
    ; check: iconst.i32 0x5555_5555
    ; check: smulhi v0, v2
    ; check: isub v3, v0
    ; check: sshr_imm v4, 1
    ; check: ushr_imm v5, 31
    ; check: v7 = iadd v5, v6
    ; check: v1 -> v7
    return v1
}

; simple case w/ shift by zero (mul, add-sign-bit)
function %t_sdiv32_p6(i32) -> i32 {
block0(v0: i32):
    v1 = sdiv_imm v0, 6
    ; check: iconst.i32 0x2aaa_aaab
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 31
    ; check: v5 = iadd v3, v4
    ; check: v1 -> v5
    return v1
}

; case d > 0 && M < 0 (mull, add, shift, add-sign-bit)
function %t_sdiv32_p7(i32) -> i32 {
block0(v0: i32):
    v1 = sdiv_imm v0, 7
    ; check: iconst.i32 0xffff_ffff_9249_2493
    ; check: smulhi v0, v2
    ; check: iadd v3, v0
    ; check: sshr_imm v4, 2
    ; check: ushr_imm v5, 31
    ; check: v7 = iadd v5, v6
    ; check: v1 -> v7
    return v1
}

; simple case (mul, shift, add-sign-bit)
function %t_sdiv32_p625(i32) -> i32 {
block0(v0: i32):
    v1 = sdiv_imm v0, 625
    ; check: iconst.i32 0x68db_8bad
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 8
    ; check: ushr_imm v4, 31
    ; check: v6 = iadd v4, v5
    ; check: v1 -> v6
    return v1
}


; -------- U64 --------

; complex case (mul, sub, shift, add, shift)
function %t_udiv64_p7(i64) -> i64 {
block0(v0: i64):
    v1 = udiv_imm v0, 7
    ; check: iconst.i64 0x2492_4924_9249_2493
    ; check: umulhi v0, v2
    ; check: isub v0, v3
    ; check: ushr_imm v4, 1
    ; check: iadd v5, v3
    ; check: v7 = ushr_imm v6, 2
    ; check: v1 -> v7
    return v1
}

; simple case (mul, shift)
function %t_udiv64_p9(i64) -> i64 {
block0(v0: i64):
    v1 = udiv_imm v0, 9
    ; check: iconst.i64 0xe38e_38e3_8e38_e38f
    ; check: umulhi v0, v2
    ; check: v4 = ushr_imm v3, 3
    ; check: v1 -> v4
    return v1
}

; complex case (mul, sub, shift, add, shift)
function %t_udiv64_p125(i64) -> i64 {
block0(v0: i64):
    v1 = udiv_imm v0, 125
    ; check: iconst.i64 0x0624_dd2f_1a9f_be77
    ; check: umulhi v0, v2
    ; check: isub v0, v3
    ; check: ushr_imm v4, 1
    ; check: iadd v5, v3
    ; check: v7 = ushr_imm v6, 6
    ; check: v1 -> v7
    return v1
}

; simple case w/ shift by zero (mul)
function %t_udiv64_p274177(i64) -> i64 {
block0(v0: i64):
    v1 = udiv_imm v0, 274177
    ; check: iconst.i64 0x3d30_f19c_d101
    ; check: v3 = umulhi v0, v2
    ; check: v1 -> v3
    return v1
}


; -------- S64 --------

; simple case (mul, shift, add-sign-bit)
function %t_sdiv64_n625(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, -625
    ; check: iconst.i64 0xcb92_3a29_c779_a6b5
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 7
    ; check: ushr_imm v4, 63
    ; check: v6 = iadd v4, v5
    ; check: v1 -> v6
    return v1
}

; simple case w/ zero shift (mul, add-sign-bit)
function %t_sdiv64_n6(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, -6
    ; check: iconst.i64 0xd555_5555_5555_5555
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 63
    ; check: v5 = iadd v3, v4
    ; check: v1 -> v5
    return v1
}

; simple case w/ zero shift (mul, add-sign-bit)
function %t_sdiv64_n5(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, -5
    ; check: iconst.i64 0x9999_9999_9999_9999
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 1
    ; check: ushr_imm v4, 63
    ; check: v6 = iadd v4, v5
    ; check: v1 -> v6
    return v1
}

; case d < 0 && M > 0 (mul, sub, shift, add-sign-bit)
function %t_sdiv64_n3(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, -3
    ; check: iconst.i64 0x5555_5555_5555_5555
    ; check: smulhi v0, v2
    ; check: isub v3, v0
    ; check: sshr_imm v4, 1
    ; check: ushr_imm v5, 63
    ; check: v7 = iadd v5, v6
    ; check: v1 -> v7
    return v1
}

; simple case w/ zero shift (mul, add-sign-bit)
function %t_sdiv64_p6(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, 6
    ; check: iconst.i64 0x2aaa_aaaa_aaaa_aaab
    ; check: smulhi v0, v2
    ; check: ushr_imm v3, 63
    ; check: v5 = iadd v3, v4
    ; check: v1 -> v5
    return v1
}

; case d > 0 && M < 0 (mul, add, shift, add-sign-bit)
function %t_sdiv64_p15(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, 15
    ; check: iconst.i64 0x8888_8888_8888_8889
    ; check: smulhi v0, v2
    ; check: iadd v3, v0
    ; check: sshr_imm v4, 3
    ; check: ushr_imm v5, 63
    ; check: v7 = iadd v5, v6
    ; check: v1 -> v7
    return v1
}

; simple case (mul, shift, add-sign-bit)
function %t_sdiv64_p625(i64) -> i64 {
block0(v0: i64):
    v1 = sdiv_imm v0, 625
    ; check: iconst.i64 0x346d_c5d6_3886_594b
    ; check: smulhi v0, v2
    ; check: sshr_imm v3, 7
    ; check: ushr_imm v4, 63
    ; check: v6 = iadd v4, v5
    ; check: v1 -> v6
    return v1
}
